{
  
    
        "post0": {
            "title": "How Many Numbers Are Smaller Than the Current Number",
            "content": "Problem Statement . Given the array nums, for each nums[i] find out how many numbers in the array are smaller than it. That is, for each nums[i] you have to count the number of valid j&#39;s such that j != i and nums[j] &lt; nums[i]. Return the answer in an array.URL . Example 1: . Input: nums = [8,1,2,2,3] Output: [4,0,1,1,3] Explanation: For nums[0]=8 there exist four smaller numbers than it (1, 2, 2 and 3). For nums[1]=1 does not exist any smaller number than it. For nums[2]=2 there exist one smaller number than it (1). For nums[3]=2 there exist one smaller number than it (1). For nums[4]=3 there exist three smaller numbers than it (1, 2 and 2). . Example 2: . Input: nums = [6,5,4,8] Output: [2,1,0,3] . Example 3: . Input: nums = [7,7,7,7] Output: [0,0,0,0] . Constraints: . 2 &lt;= nums.length &lt;= 500 | 0 &lt;= nums[i] &lt;= 100 | . Approach 1: . Array traversal by count with Merge sort . #collapse-hide from typing import List class Solution: def smallerNumbersThanCurrent(self, nums: List[int]) -&gt; List[int]: input_list = nums.copy() if len(nums) == 0: return [] if len(nums) == 1: return [0] sorted_arr = self.merge_sort(nums) result = [] for i in range(len(input_list)): count = 0 for j in range(len(sorted_arr)): if input_list[i] == sorted_arr[j]: break else: count += 1 result.append(count) return result def merge_sort(self, arr): if(len(arr) &gt; 1): mid_val = len(arr)//2 left_list = arr[:mid_val] right_list = arr[mid_val:] self.merge_sort(left_list) self.merge_sort(right_list) i, j, k = 0, 0, 0 while(i &lt; len(left_list) and j &lt; len(right_list)): if left_list[i] &lt; right_list[j]: arr[k] = left_list[i] i += 1 else: arr[k] = right_list[j] j += 1 k += 1 while(i &lt; len(left_list)): arr[k] = left_list[i] i += 1 k += 1 while(j &lt; len(right_list)): arr[k] = right_list[j] j += 1 k += 1 return arr . . sol = Solution() sol.smallerNumbersThanCurrent([8,1,2,2,3]) . [4, 0, 1, 1, 3] . . Approach 2: . HashTable implementation with index and sorting . #collapse-hide class Solution: def smallerNumbersThanCurrent(self, nums): count = {} for i, v in enumerate(sorted(nums)): if v not in count: count[v] = i return [count[v] for v in nums] . . sol = Solution() sol.smallerNumbersThanCurrent([8,1,2,2,3]) . [4, 0, 1, 1, 3] . . Worst case performance in Time: $O(nlogn)$ . Approach 3: . considering the given constraints . 2 &lt;= nums.length &lt;= 500 | 0 &lt;= nums[i] &lt;= 100 | . . #collapse-hide class Solution: def smallerNumbersThanCurrent(self, nums): count = [0]*101 result = [0]*len(nums) for num in nums: count[num] += 1 for i in range(1, 100): count[i] += count[i-1] for ind, val in enumerate(nums): if val &gt; 0: result[ind] = count[val-1] return result . . sol = Solution() sol.smallerNumbersThanCurrent([5,0,10,0,10,6]) . [2, 0, 4, 0, 4, 3] . . Worst case performance in Time: $O(n)$ .",
            "url": "https://tejakummarikuntla.github.io/notes/problem%20solving/leetcode/2020/05/02/How-many-numbers-are-smaller-than-the-current-number.html",
            "relUrl": "/problem%20solving/leetcode/2020/05/02/How-many-numbers-are-smaller-than-the-current-number.html",
            "date": " • May 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Longest Common Prefix",
            "content": "Problem Statement . Write a function to find the longest common prefix string amongst an array of strings. . If there is no common prefix, return an empty string &quot;&quot;. [URL] . Example 1 . Input: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;] Output: &quot;fl&quot; . Example 2 . Input: [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;] Output: &quot;&quot; Explanation: There is no common prefix among the input strings. . . Note: All given inputs are in lowercase letters a-z. . Approach 1: . Word based comparison approach . . #collapse-hide from typing import List class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: if len(strs) == 0: return &#39;&#39; if len(strs) == 1: return strs[0] i = 0 j = i+1 while(j &lt; len(strs)): strs[i] = Solution.lcp_of_two(self, strs[i], strs[j]) j += 1 return strs[i] def lcp_of_two(self, ele1, ele2): i, j = 0, 0 while (i &lt; len(ele1) and j &lt; len(ele2)): if (ele1[i] == ele2[j]): i += 1 j += 1 else: break lcp = ele1[:i] return lcp sol = Solution() print(sol.longestCommonPrefix([&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;])) . . fl . print(sol.longestCommonPrefix([&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;])) . fl . print(sol.longestCommonPrefix([&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;])) . . . #collapse-hide class Solution(object): def commonPrefixUtil(self, str1, str2): len_str1 = len(str1) len_srt2 = len(str2) i = j = 0 while(i &lt; len_str1 and j &lt; len_srt2): if str1[i] != str2[j]: break i += 1 j += 1 return str1[:i] def longestCommonPrefix(self, strs): &quot;&quot;&quot; :type strs: List[str] :rtype: str &quot;&quot;&quot; if len(strs) &lt; 1: return &quot;&quot; prefix = strs[0] for i in range(1, len(strs)): prefix = self.commonPrefixUtil(prefix, strs[i]) return prefix . . sol = Solution() sol.longestCommonPrefix([&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]) . &#39;fl&#39; . . Approach 2 . Character based comparison approach . . #collapse-hide from typing import List class Solution: def longestCommonPrefix(self, strs): &quot;&quot;&quot; :type strs: List[str] :rtype: str &quot;&quot;&quot; if len(strs) == 0: return &#39;&#39; minLen = self.finMinimumLenString(strs) result = &quot;&quot; for i in range(minLen): current = strs[0][i] for j in range(1, len(strs)): if current != strs[j][i]: return result result += current return result def finMinimumLenString(self, strs): minLen = len(strs[0]) for i in range(1, len(strs)): if len(strs[i]) &lt; minLen: minLen = len(strs[i]) return minLen . . sol = Solution() sol.longestCommonPrefix([&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]) . &#39;fl&#39; . . Approach 3 . Character based comparison with Binary Tree implementation . 11%3 . 2 .",
            "url": "https://tejakummarikuntla.github.io/notes/problem%20solving/leetcode/2020/04/30/Longest-Common-Prefix.html",
            "relUrl": "/problem%20solving/leetcode/2020/04/30/Longest-Common-Prefix.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Comparison-based Lower Bounds for Sorting",
            "content": "The Big question . When we look back and figure out that, all the comprassion based sorting algorithms such as . Insertion Sort | Mergesort | Bubble sort | Quick sort | Selection sort etc., | . Best Case Perfomance Worst Case Perfomance . Insertion Sort | $O(n)$ comprisions, $O(1)$ swaps | $O(n^2)$ comprisions and $O(1)$ swaps | . Merge Sort | $O(n log n)$ | $O(n log n)$ | . Bubble Sort | $O(n)$ comparisons, ${ displaystyle O(1)}$ swaps | $O(n^{2})$ comparisons, ${ displaystyle O(n^{2})}$ swaps | . Quick Sort | $O(n log n)$ | $O(n^2)$ | . Selection Sort | $О(n^2)$ comparisons, $О(n)$ swaps | $O(n^2)$ comparisons, $О(n)$ swaps | . There is no comprasion based sorting algorithm with time complexity less than $O(nlogn)$ in worst case. So what would be the lower bound of the comparsion based sorting algorithms . Solving . Imagine we have three elements $a_1, a_2, a_3$ in which $a_i ne a_i$ and we are supposed to sort them in ascending order, The possible solutions would be . $a_1, a_2, a_3$ | $a_1, a_3, a_2$ | $a_2, a_1, a_3$ | $a_2, a_3, a_1$ | $a_3, a_1, a_2$ | $a_3, a_2, a_1$ | . The possible ascending order would be one among these 6. Which means the number of outcomes is $n!$.The Number of ways we can arrange n elements in $n!$ . . Note: 1. N elements give $N!$ arrangements . Let&#39;s assume an abstract approach of a comparison-based sorting algorithm using a Binary Decision tree, . . The possible arrangements of the three elements are the leaf nodes of the Binary Decision Tree. From this, we can say that the number of possible arrangements of n elements is equal to the number of leaf nodes in a binary decision tree. . Note: 2. # Leaf Nodes = # arrangements of n elements N! . Now, let&#39;s look into the hight of the tree. At each level of the tree we are making a decision, which means the height of the tree at a particular leaf will tell us the number of comparisons we made to reach that arrangement. . . Note: 3. hight of the tree = # comparisons we made . . From the Full binary tree above we can say that the number of leaf nodes of this tree is $2^{height}$ and there is no possibility that the leaf nodes can more than $2^{height}$ . Note: 3. Maximum Number of Leaf Nodes in a Binary tree = $2^{height}$ . Now, if we were given a binary decision tree for performing sorting which has $n!$[ref: Note2] leaf nodes, from the Note3, we can say that. . If h is the height of the Binary Decision tree that we are building for sorting n elements then, . $$2^{h} geq n! $$ . As the log is a monotonic function, we can apply log on both sides, . $$h ge log(n!)$$ . Now, we can calculate the h value which is the height, in return, we can figure out the comparison, from the asymptotic equations of growth of functions we know that, . $$log(n!) = theta(nlogn)$$ $$log(n!) = Omega(nlogn)$$ . Tip: Understand how $log(n!) = theta(nlogn)$ from this Video using the above equations, we can say . $$h = Omega(nlogn)$$ . Which means, the height of the binary decision tree for sorting n elements is lower bounded by $nlogn$, In which height is the number of comparisons we have to make. . From this we can say: . Note: #Comparision we have to make = $ Omega(nlogn)$ The Number of comparisions is lower bounded by $nlogn$. . . Important: From this, we can conclude that no matter of any comparison-based algorithms, we are bounded by the fact that the worst-case time complexity is lower bounded by $nlogn$. It means, we can not have a comparison-based algorithm which can perform better than $nlogn$ .",
            "url": "https://tejakummarikuntla.github.io/notes/algorithms/2020/04/28/Comparison-based-Lower-Bounds-for-Sorting.html",
            "relUrl": "/algorithms/2020/04/28/Comparison-based-Lower-Bounds-for-Sorting.html",
            "date": " • Apr 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Counting Sort",
            "content": "Introduction . If we stick to comparison-based sorting methods cannot do better than Ω(n log n), Comparison-based Lower Bounds for Sorting . It operates by counting the number of objects that have each distinct key values | Integer sorting algorithm: we assume the values to be integers | Using arithmetic on those counts to determine the positions of each key value in the output sequence. | It is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items | It can be used as a subroutine in radix sort | Because counting sort uses key values as indexes into an array, it is not a comparison based algorithm, so linearithmic running time can be reduced. | . Pseudo Code [CLRS] . CLRS has cooler implementation of counting sort with counters, no lists — but time bound is the same . . arr = [6, 3, 9, 10, 15, 6, 8, 12, 3, 6] . def return_max(arr): for i in range(len(arr)-1): if (arr[i] &gt; arr[i+1]): arr[i], arr[i+1] = arr[i+1], arr[i] return arr[-1] . return_max(arr) . 15 . Implementation of Counting Sort . #collapse-hide def counting_sort(arr): max_arr = return_max(arr) count_arr = [] res_arr = [] for i in range(max_arr+1): count_arr.append(0) for i in range(len(arr)): count_arr[arr[i]] += 1 i = 0 while(i &lt; len(count_arr)): if(count_arr[i] &gt; 0): res_arr.append(i) count_arr[i] -= 1 i = 0 i += 1 return res_arr . . counting_sort(arr) . [3, 3, 6, 6, 6, 8, 9, 10, 12, 15] . Implementation of Counting Sort without duplicates. . #collapse-hide def counting_sort_without_duplicates(arr): max_arr = return_max(arr) count_arr = [] res_arr = [] for i in range(max_arr+1): count_arr.append(0) for i in range(len(arr)): count_arr[arr[i]] += 1 for i in range(len(count_arr)): if count_arr[i] &gt; 0: res_arr.append(i) count_arr[i] -= 1 return res_arr . . counting_sort_without_duplicates(arr) . [3, 6, 8, 9, 10, 12, 15] . Algorithm Analysis . . . Time Complexity [Worst Case]: $O(n+k)$, where k is the range of the non-negative key values. . Space Complexcity [Worst Case]: $O(n+k)$ . In-Place: Counting sort is not an in-place algorithm as it makes use of external memory . Stable: Counting sort can be both Stable and non-stable, the above algorithm is stable . Crisp Summery: . Make assumptions about the data | Doesn&#39;t use comparisons | Counts the number of occurrences of each value | Only works with non-negative discrete values (can&#39;t work with floats, strings) | Values must be within a specific range. | O(n) can achieve this because we&#39;re making assumptions about the data we&#39;re sorting. | . Reference: . MIT 6.006 Lecture 7: Linear-Time Sorting PDF | MIT 6.006 Counting Sort PDF | MIT 6.006 Fall 2009 PDF | Learn Programming Academy WebPage | .",
            "url": "https://tejakummarikuntla.github.io/notes/algorithms/2020/04/27/Counting-Sort.html",
            "relUrl": "/algorithms/2020/04/27/Counting-Sort.html",
            "date": " • Apr 27, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "US Adult Income",
            "content": "Dataset of adult income &lt;/br&gt; DataSet Overveiw . Each row is labelled as either having a salary greater than &quot;&gt;50K&quot; or &quot;&lt;=50K&quot;. | This Data set is split into two CSV files, named adult-training.csv and adult-test.csv. &lt;/br&gt; To Build a binary classifier on the training dataset to predict the column income_bracket which has two possible values &quot;&gt;50K&quot; and &quot;&lt;=50K&quot; and evaluate the accuracy of the classifier with the test dataset. | categorical_columns = [workclass, education, marital_status, occupation, relationship, race, gender, native_country] | continuous_columns = [age, education_num, capital_gain, capital_loss, hours_per_week] | A set of reasonably clean records was extracted using the following conditions: ((AAGE&gt;16) &amp;&amp; (AGI&gt;100) &amp;&amp; (AFNLWGT&gt;1)&amp;&amp; (HRSWK&gt;0)) | . Prediction task is to determine whether a person makes over 50K a year. . Dataset Source: https://archive.ics.uci.edu/ml/datasets/census+income, http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/ | . import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline train_file = &quot;adult-training.csv&quot; columns = [&#39;Age&#39;,&#39;Workclass&#39;,&#39;fnlgwt&#39;,&#39;Education&#39;,&#39;Education_num&#39;,&#39;Marital_Status&#39;, &#39;Occupation&#39;,&#39;Relationship&#39;,&#39;Race&#39;,&#39;Sex&#39;,&#39;Capital_Gain&#39;,&#39;Capital_Loss&#39;, &#39;Hours/Week&#39;,&#39;Native_country&#39;,&#39;Income&#39;] . #collapse-hide train = pd.read_csv(train_file, names=columns) train.head() . . Age Workclass fnlgwt Education Education_num Marital_Status Occupation Relationship Race Sex Capital_Gain Capital_Loss Hours/Week Native_country Income . 0 39 | State-gov | 77516 | Bachelors | 13 | Never-married | Adm-clerical | Not-in-family | White | Male | 2174 | 0 | 40 | United-States | &lt;=50K | . 1 50 | Self-emp-not-inc | 83311 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 13 | United-States | &lt;=50K | . 2 38 | Private | 215646 | HS-grad | 9 | Divorced | Handlers-cleaners | Not-in-family | White | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 3 53 | Private | 234721 | 11th | 7 | Married-civ-spouse | Handlers-cleaners | Husband | Black | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 4 28 | Private | 338409 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Wife | Black | Female | 0 | 0 | 40 | Cuba | &lt;=50K | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 32561 entries, 0 to 32560 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 Age 32561 non-null int64 1 Workclass 32561 non-null object 2 fnlgwt 32561 non-null int64 3 Education 32561 non-null object 4 Education_num 32561 non-null int64 5 Marital_Status 32561 non-null object 6 Occupation 32561 non-null object 7 Relationship 32561 non-null object 8 Race 32561 non-null object 9 Sex 32561 non-null object 10 Capital_Gain 32561 non-null int64 11 Capital_Loss 32561 non-null int64 12 Hours/Week 32561 non-null int64 13 Native_country 32561 non-null object 14 Income 32561 non-null object dtypes: int64(6), object(9) memory usage: 3.7+ MB . train.shape . (32561, 15) . train.describe() . Age fnlgwt Education_num Capital_Gain Capital_Loss Hours/Week . count 32561.000000 | 3.256100e+04 | 32561.000000 | 32561.000000 | 32561.000000 | 32561.000000 | . mean 38.581647 | 1.897784e+05 | 10.080679 | 1077.648844 | 87.303830 | 40.437456 | . std 13.640433 | 1.055500e+05 | 2.572720 | 7385.292085 | 402.960219 | 12.347429 | . min 17.000000 | 1.228500e+04 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | . 25% 28.000000 | 1.178270e+05 | 9.000000 | 0.000000 | 0.000000 | 40.000000 | . 50% 37.000000 | 1.783560e+05 | 10.000000 | 0.000000 | 0.000000 | 40.000000 | . 75% 48.000000 | 2.370510e+05 | 12.000000 | 0.000000 | 0.000000 | 45.000000 | . max 90.000000 | 1.484705e+06 | 16.000000 | 99999.000000 | 4356.000000 | 99.000000 | . # Replacing &#39;?&#39; with nan train.replace(&#39; ?&#39;, np.nan, inplace=True) . train.isnull().sum() . Age 0 Workclass 1836 fnlgwt 0 Education 0 Education_num 0 Marital_Status 0 Occupation 1843 Relationship 0 Race 0 Sex 0 Capital_Gain 0 Capital_Loss 0 Hours/Week 0 Native_country 583 Income 0 dtype: int64 . Missing Data: . Workclass(1836), Occupation(1843), Native_country(583) &lt;/br&gt; . Important: All the missing data belongs to Categorical data . train[&#39;Income&#39;].value_counts() . &lt;=50K 24720 &gt;50K 7841 Name: Income, dtype: int64 . sns.countplot(train[&#39;Income&#39;]) plt.title(&quot;Count of Income Category&quot;) plt.show() . . Warning: Dataset is Imbalenced with Majority class label &lt;=50k. - 75.91% data points labeled &lt;=50k . 24.08% data points labeled &lt;50k | . # Gender distribution sns.countplot(train[&#39;Sex&#39;]) plt.title(&quot;Count of Sex Category&quot;) plt.show() . sns.stripplot(x=&#39;Sex&#39;, y=&#39;Hours/Week&#39;, data=train,hue=&#39;Income&#39;,marker=&#39;X&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19ac5a67ec8&gt; . # Workclass wclass_plot = sns.countplot(train[&#39;Workclass&#39;]) wclass_plot.set_xticklabels( wclass_plot.get_xticklabels(),rotation=50, ha=&quot;right&quot;) plt.title(&quot;Count Plot of Workclass&quot;) . Text(0.5, 1.0, &#39;Count Plot of Workclass&#39;) . Private class working people are overall High in count . train[&#39;Education&#39;].value_counts() . HS-grad 10501 Some-college 7291 Bachelors 5355 Masters 1723 Assoc-voc 1382 11th 1175 Assoc-acdm 1067 10th 933 7th-8th 646 Prof-school 576 9th 514 12th 433 Doctorate 413 5th-6th 333 1st-4th 168 Preschool 51 Name: Education, dtype: int64 . # Occupation occ_plot = sns.countplot(train[&#39;Occupation&#39;]) occ_plot.set_xticklabels(occ_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) plt.title(&quot;Count Plot of Occupation&quot;) . Text(0.5, 1.0, &#39;Count Plot of Occupation&#39;) . fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(20, 20)) plt.subplots_adjust(hspace=0.68) fig.delaxes(axs[3][1]) fig.suptitle(&#39;Subplot of Various Categorical Variables&#39;) # Workclass wc_plot = sns.countplot(train[&#39;Workclass&#39;], ax=axs[0][0]) wc_plot.set_xticklabels(wc_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[0][0].title.set_text(&#39;Count Plot of Workclass&#39;) # Native country nc_plot = sns.countplot(train[&#39;Native_country&#39;], ax=axs[0][1]) nc_plot.set_xticklabels(nc_plot.get_xticklabels(), rotation=72, ha=&quot;right&quot;) axs[0][1].title.set_text(&#39;Count plot of Native_country&#39;) # Education ed_plot = sns.countplot(train[&#39;Education&#39;], ax=axs[1][0]) ed_plot.set_xticklabels(ed_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[1][0].title.set_text(&#39;Count Plot of Education&#39;) # Marital status ms_plot = sns.countplot(train[&#39;Marital_Status&#39;], ax=axs[1][1]) ms_plot.set_xticklabels(ms_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[1][1].title.set_text(&#39;Count Plot of Martial Status&#39;) # Relationship rel_plot = sns.countplot(train[&#39;Relationship&#39;], ax=axs[2][0]) rel_plot.set_xticklabels(rel_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[2][0].title.set_text(&#39;Count Plot of Relationship&#39;) # Race race_plot = sns.countplot(train[&#39;Race&#39;], ax=axs[2][1]) race_plot.set_xticklabels(race_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[2][1].title.set_text(&#39;Count Plot of Race&#39;) # Occupation occ_plot = sns.countplot(train[&#39;Occupation&#39;], ax=axs[3][0]) occ_plot.set_xticklabels(occ_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[3][0].title.set_text(&#39;Count Plot of Occupation&#39;) . . Note: #### Majority count aggrigation in each column: - Workclass: . - Private : 22696 . Native_country: United-States : 29170 | . | Education: Hs-grad : 10501 | . | Marital_Status: Married-civ-spouse : 14976 | . | Relationship Husband : 13193 | . | Race White : 27816 | . | Occupation: Prof-specialty : 4140 | . | . . Note: #### Minority count aggrigation in each column: - Workclass: . - Never-worked : 7 . Native_country: Holand-Netherlands : 1 | . | Education: Preschool : 51 | . | Marital_Status: Married-AF-spouse : 23 | . | Relationship Other-relative : 981 | . | Race: other : 271 | . | Occupation: Armed-Forces : 9 | . | . plt.figure(figsize=(20, 6)) sns.countplot(train[&#39;Marital_Status&#39;], hue=train[&#39;Income&#39;]) plt.title(&quot;Count Plot of Maritial Status with Hue Income&quot;) . Text(0.5, 1.0, &#39;Count Plot of Maritial Status with Hue Income&#39;) . Most of the Never Married people are under Income of &lt;=50k . plt.figure(figsize=(20, 6)) sns.countplot(train[&#39;Relationship&#39;], hue=train[&#39;Income&#39;]) plt.title(&quot;Count Plot of Relationship with Hue Income&quot;) . Text(0.5, 1.0, &#39;Count Plot of Relationship with Hue Income&#39;) . plt.figure(figsize=(20, 6)) sns.countplot(train[&#39;Age&#39;], hue=train[&#39;Income&#39;]) plt.title(&quot;Count Plot of Age with Hue Income&quot;) . Text(0.5, 1.0, &#39;Count Plot of Age with Hue Income&#39;) . sns.set_style(&quot;whitegrid&quot;) sns.pairplot(train, hue=&quot;Income&quot;, size=3) plt.show() . #collapse-hide # Age with Income sns.FacetGrid(train, hue=&quot;Income&quot;, size=6) .map(sns.distplot, &quot;Age&quot;) .add_legend(); plt.show(); # Education_num with Education_num sns.FacetGrid(train, hue=&quot;Income&quot;, size=6) .map(sns.distplot, &quot;Education_num&quot;) .add_legend(); plt.show(); # Education_num with Capital_Gain sns.FacetGrid(train, hue=&quot;Income&quot;, size=7) .map(sns.distplot, &quot;Capital_Gain&quot;) .add_legend(); plt.show(); # Education_num with Capital_Loss sns.FacetGrid(train, hue=&quot;Income&quot;, size=7) .map(sns.distplot, &quot;Capital_Loss&quot;) .add_legend(); plt.show(); # Education_num with Hours/Week sns.FacetGrid(train, hue=&quot;Income&quot;, size=7) .map(sns.distplot, &quot;Hours/Week&quot;) .add_legend(); plt.show(); . . [Report]Univariate Analysis . Dataset is Imbalenced with Majority class label &lt;=50k. . 75.91% data points labeled &lt;=50k | 24.08% data points labeled &lt;50k | . Missing Data: . Workclass(1836), Occupation(1843), Native_country(583) &lt;/br&gt; All belongs to Categorical data . Workclass Majority: Private Class, 22696 | . | Minority: Never-worked, 7 | Without-pay, 14 | Federal-gov, 960 | . | . | Native Country Majority: United-States, 29170 | . | Minority: Holand-Netherlands, 1 | Scotland, 12 | . | Missing Data: ?, 583 | . | . | Education Majority: HS-grad, 10501 | Some-college, 7291 | Bachelors, 5355 | . | Minority: Preschool, 51 | 1st-4th, 168 | 5th-6th, 333 | . | . | Martial Status Majority: Married-civ-spouse, 14976 | Never-married, 10683 | Divorced, 4443 | . | Minority: Married-AF-spouse, 23 | Married-spouse-absent, 418 | . | . | Relationship Majority: Husband, 13193 | Not-in-family, 8305 | . | Minority: Other-relative, 981 | Wife, 1568 | . | . | Race Majority: White, 27816 | Black, 3124 | . | Minority: Other, 271 | Amer-Indian-Eskimo, 311 | . | . | Occupation Majority: Prof-specialty, 4140 | Craft-repair, 4099 | Exec-managerial, 4066 | . | Minority: Armed-Forces, 9 | Priv-house-serv, 149 | Protective-serv, 649 | . | Missing Data: ?, 1843 | . | . | Majority count aggrigation in each column: . Workclass: Private : 22696 | . | Native_country: United-States : 29170 | . | Education: Hs-grad : 10501 | . | Marital_Status: Married-civ-spouse : 14976 | . | Relationship Husband : 13193 | . | Race White : 27816 | . | Occupation: Prof-specialty : 4140 | . | . Minority count aggrigation in each column: . Workclass: Never-worked : 7 | . | Native_country: Holand-Netherlands : 1 | . | Education: Preschool : 51 | . | Marital_Status: Married-AF-spouse : 23 | . | Relationship Other-relative : 981 | . | Race: other : 271 | . | Occupation: Armed-Forces : 9 | . | . train_df = pd.read_csv(&quot;adult-training.csv&quot;, names=columns) # Repalcing &#39;?&#39; to nan #train_df.replace(&#39; ?&#39;, np.nan, inplace=True) . Bivariate Analysis . Questions: . Which workclass people are earning the most? | Which level of educated people are earning the most? | Which martial category people are earning the most? | people form which occupation category are earning the most? | People form wich relation category are earning the most? | Which gender people are earning the most? | Which Race of people are earning the most? | People belongs to which Native country are earning the most? | . Income . changing Income into 0&#39;s and 1&#39;s . train_df[&#39;Income&#39;] = train[&#39;Income&#39;].apply(lambda x: 1 if x==&#39; &gt;50K&#39; else 0) . Workclass . Replaceing NaNs with 0s . train_df[&#39;Workclass&#39;].fillna(&#39; 0&#39;, inplace=True) . sns.factorplot(x=&quot;Workclass&quot;, y=&quot;Income&quot;, data=train_df, kind=&quot;bar&quot;, size = 6, palette = &quot;muted&quot;) plt.xticks(rotation=45); plt.title(&quot;Bar plot of Work Class VS Income&quot;) . Text(0.5, 1, &#39;Bar plot of Work Class VS Income&#39;) . People from Self-emp-inc are earning the most . Education . sns.factorplot(x=&quot;Education&quot;,y=&quot;Income&quot;,data=train_df,kind=&quot;bar&quot;, size = 7, palette = &quot;muted&quot;) plt.xticks(rotation=60); plt.title(&quot;Bar plot of Education VS Income&quot;) . Text(0.5, 1, &#39;Bar plot of Education VS Income&#39;) . All the Grade Education can be combined in to Primary as a single feature &lt;/br&gt; ref: https://www.kaggle.com/kost13/us-income-logistic-regression/comments . def primary(x): if x in [&#39; 1st-4th&#39;, &#39; 5th-6th&#39;, &#39; 7th-8th&#39;, &#39; 9th&#39;, &#39; 10th&#39;, &#39; 11th&#39;, &#39; 12th&#39;]: return &#39; Primary&#39; else: return x . train_df[&#39;Education&#39;] = train_df[&#39;Education&#39;].apply(primary) . sns.factorplot(x=&quot;Education&quot;, y=&quot;Income&quot;, data=train_df, kind=&quot;bar&quot;, size=7, palette=&quot;muted&quot;) plt.xticks(rotation=60); . Combinded [&#39; 1st-4th&#39;, &#39; 5th-6th&#39;, &#39; 7th-8th&#39;, &#39; 9th&#39;, &#39; 10th&#39;, &#39; 11th&#39;, &#39; 12th&#39;] to single feature Primary . Doctorates and Prof-school people has Hihger Income &gt;50k | . Education num . sns.factorplot(x=&quot;Education_num&quot;,y=&quot;Income&quot;,data=train_df,kind=&quot;bar&quot;, size = 6, palette = &quot;muted&quot;) plt.xticks(rotation=60); plt.title(&quot;Factorplot of Education VS Income&quot;) . Text(0.5, 1, &#39;Factorplot of Education VS Income&#39;) . Relation Higher the Education_num give better Income . Martial Status . sns.factorplot(x=&quot;Marital_Status&quot;,y=&quot;Income&quot;,data=train_df,kind=&quot;bar&quot;, size = 5, palette = &quot;muted&quot;) plt.xticks(rotation=60); print(train_df[&#39;Marital_Status&#39;].value_counts()) plt.title(&quot;Factor plot of Martial Status VS Income&quot;) . Married-civ-spouse 14976 Never-married 10683 Divorced 4443 Separated 1025 Widowed 993 Married-spouse-absent 418 Married-AF-spouse 23 Name: Marital_Status, dtype: int64 . Text(0.5, 1, &#39;Factor plot of Martial Status VS Income&#39;) . People belonging to Married-civ-spouse are earning the most. . Occupation . #filing NaNs in Occupation with 0 train_df[&#39;Occupation&#39;].replace(&#39; ?&#39;, &#39; 0&#39;, inplace=True) . train_df[&#39;Occupation&#39;].value_counts() . Prof-specialty 4140 Craft-repair 4099 Exec-managerial 4066 Adm-clerical 3770 Sales 3650 Other-service 3295 Machine-op-inspct 2002 0 1843 Transport-moving 1597 Handlers-cleaners 1370 Farming-fishing 994 Tech-support 928 Protective-serv 649 Priv-house-serv 149 Armed-Forces 9 Name: Occupation, dtype: int64 . sns.factorplot(x=&quot;Occupation&quot;,y=&quot;Income&quot;,data=train_df,kind=&quot;bar&quot;, size = 8, palette = &quot;muted&quot;) plt.xticks(rotation=60); plt.title(&quot;Factor plot of Occupation VS Income&quot;) . Text(0.5, 1, &#39;Factor plot of Occupation VS Income&#39;) . people belonging to Exec-managerial occupation are earning the most . Relationship . sns.factorplot(x=&quot;Relationship&quot;, y=&quot;Income&quot;, data=train_df, size=5, kind=&quot;bar&quot;, palette=&quot;muted&quot;) plt.xticks(rotation=60) plt.title(&quot;Factorplot of Relationship vs Income&quot;) print(train_df[&#39;Relationship&#39;].value_counts()) . Husband 13193 Not-in-family 8305 Own-child 5068 Unmarried 3446 Wife 1568 Other-relative 981 Name: Relationship, dtype: int64 . People belonging to wife category of relationship are earning the most . Race . sns.factorplot(x=&quot;Race&quot;, y=&quot;Income&quot;, data=train_df, size=5, kind=&quot;bar&quot;, palette=&quot;muted&quot;) plt.xticks(rotation=60) plt.title(&quot;Factorplot of Race VS Income&quot;) print(train_df[&#39;Race&#39;].value_counts()) . White 27816 Black 3124 Asian-Pac-Islander 1039 Amer-Indian-Eskimo 311 Other 271 Name: Race, dtype: int64 . People belonging to Asian-Pac-Islander are earning the most in Race . sex . sns.factorplot(x=&quot;Sex&quot;, y=&quot;Income&quot;, data=train_df,size=5,kind=&quot;bar&quot;, palette=&quot;muted&quot;) plt.xticks(rotation=60) plt.title(&quot;Factorplot of Sex VS Income&quot;) print(train_df[&#39;Sex&#39;].value_counts()) . Male 21790 Female 10771 Name: Sex, dtype: int64 . Male gender are earning the most . Native country . There Exist 583 Unknown values replacing with 0 . train_df[&#39;Native_country&#39;].replace(&#39; ?&#39;, &#39; 0&#39;, inplace=True) . #collapse-hide sns.factorplot(x=&quot;Native_country&quot;, y=&quot;Income&quot;, data=train_df,size=13,kind=&quot;bar&quot;, palette=&quot;muted&quot;) plt.xticks(rotation=80) print(train_df[&#39;Native_country&#39;].value_counts()) . . United-States 29170 Mexico 643 0 583 Philippines 198 Germany 137 Canada 121 Puerto-Rico 114 El-Salvador 106 India 100 Cuba 95 England 90 Jamaica 81 South 80 China 75 Italy 73 Dominican-Republic 70 Vietnam 67 Guatemala 64 Japan 62 Poland 60 Columbia 59 Taiwan 51 Haiti 44 Iran 43 Portugal 37 Nicaragua 34 Peru 31 France 29 Greece 29 Ecuador 28 Ireland 24 Hong 20 Cambodia 19 Trinadad&amp;Tobago 19 Thailand 18 Laos 18 Yugoslavia 16 Outlying-US(Guam-USVI-etc) 14 Hungary 13 Honduras 13 Scotland 12 Holand-Netherlands 1 Name: Native_country, dtype: int64 . train_df.columns . Index([&#39;Age&#39;, &#39;Workclass&#39;, &#39;fnlgwt&#39;, &#39;Education&#39;, &#39;Education_num&#39;, &#39;Marital_Status&#39;, &#39;Occupation&#39;, &#39;Relationship&#39;, &#39;Race&#39;, &#39;Sex&#39;, &#39;Capital_Gain&#39;, &#39;Capital_Loss&#39;, &#39;Hours/Week&#39;, &#39;Native_country&#39;, &#39;Income&#39;], dtype=&#39;object&#39;) . train_df[&#39;Native_country&#39;].value_counts() . United-States 29170 Mexico 643 0 583 Philippines 198 Germany 137 Canada 121 Puerto-Rico 114 El-Salvador 106 India 100 Cuba 95 England 90 Jamaica 81 South 80 China 75 Italy 73 Dominican-Republic 70 Vietnam 67 Guatemala 64 Japan 62 Poland 60 Columbia 59 Taiwan 51 Haiti 44 Iran 43 Portugal 37 Nicaragua 34 Peru 31 France 29 Greece 29 Ecuador 28 Ireland 24 Hong 20 Cambodia 19 Trinadad&amp;Tobago 19 Thailand 18 Laos 18 Yugoslavia 16 Outlying-US(Guam-USVI-etc) 14 Hungary 13 Honduras 13 Scotland 12 Holand-Netherlands 1 Name: Native_country, dtype: int64 . People from Iran are earning the most . colormap = plt.cm.magma plt.figure(figsize=(16,16)) plt.title(&#39;Pearson Correlation of Features&#39;, y=1.05, size=15) sns.heatmap(train_df.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor=&#39;white&#39;, annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbe9e5fe080&gt; . [Bivariate][Report] Answers with perfomed bivariate analysis . Which workclass people are earning the most? Self-emp-inc | . | Which level of educated people are earning the most? Doctorates and Prof-school | . | Which martial category people are earning the most? Married-civ-spouse | . | people form which occupation category are earning the most? Exec-managerial | . | People form wich relation category are earning the most? Wife | . | Which gender people are earning the most? Men | . | Which Race of people are earning the most? Asian-Pac-Islander | . | People belongs to which Native country are earning the most? Iran | . | . Mulitvariate Analysis, pivoting . Questions: . Specific Counts of each in different workclass belongs to various education on Income basis | Specific Counts of each in different workclass belongs to various education on Gender basis | . train_mult_index = train_df.set_index(keys = [&#39;Income&#39;,&#39;Education&#39;,&#39;Native_country&#39;]).sort_index() . train_mult_index.tail() . Age Workclass fnlgwt Education_num Marital_Status Occupation Relationship Race Sex Capital_Gain Capital_Loss Hours/Week . Income Education Native_country . 1 Some-college United-States 30 | Self-emp-not-inc | 176185 | 10 | Married-spouse-absent | Craft-repair | Own-child | White | Male | 0 | 0 | 60 | . United-States 53 | Private | 304504 | 10 | Married-civ-spouse | Transport-moving | Husband | White | Male | 0 | 1887 | 45 | . United-States 46 | Private | 42251 | 10 | Married-civ-spouse | Sales | Husband | White | Male | 0 | 0 | 45 | . United-States 46 | Private | 364548 | 10 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 48 | . Yugoslavia 36 | Self-emp-inc | 337778 | 10 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 60 | . train_mult_index.loc[(1, &quot; Primary&quot;, &quot; United-States&quot;),].count()[0] . 202 . # People having Income &gt;50K with Primary Education in United-Sates: 202 . train_mult_index.stack().to_frame() . 0 . Income Education Native_country . 0 Assoc-acdm ? Age 42 | . Workclass Self-emp-not-inc | . fnlgwt 183765 | . Education_num 12 | . Marital_Status Married-civ-spouse | . ... ... ... ... ... | . 1 Some-college Yugoslavia Race White | . Sex Male | . Capital_Gain 0 | . Capital_Loss 0 | . Hours/Week 60 | . 390732 rows × 1 columns . #collapse-hide train_df . . Age Workclass fnlgwt Education Education_num Marital_Status Occupation Relationship Race Sex Capital_Gain Capital_Loss Hours/Week Native_country Income . 0 39 | State-gov | 77516 | Bachelors | 13 | Never-married | Adm-clerical | Not-in-family | White | Male | 2174 | 0 | 40 | United-States | 0 | . 1 50 | Self-emp-not-inc | 83311 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 13 | United-States | 0 | . 2 38 | Private | 215646 | HS-grad | 9 | Divorced | Handlers-cleaners | Not-in-family | White | Male | 0 | 0 | 40 | United-States | 0 | . 3 53 | Private | 234721 | Primary | 7 | Married-civ-spouse | Handlers-cleaners | Husband | Black | Male | 0 | 0 | 40 | United-States | 0 | . 4 28 | Private | 338409 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Wife | Black | Female | 0 | 0 | 40 | Cuba | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 32556 27 | Private | 257302 | Assoc-acdm | 12 | Married-civ-spouse | Tech-support | Wife | White | Female | 0 | 0 | 38 | United-States | 0 | . 32557 40 | Private | 154374 | HS-grad | 9 | Married-civ-spouse | Machine-op-inspct | Husband | White | Male | 0 | 0 | 40 | United-States | 1 | . 32558 58 | Private | 151910 | HS-grad | 9 | Widowed | Adm-clerical | Unmarried | White | Female | 0 | 0 | 40 | United-States | 0 | . 32559 22 | Private | 201490 | HS-grad | 9 | Never-married | Adm-clerical | Own-child | White | Male | 0 | 0 | 20 | United-States | 0 | . 32560 52 | Self-emp-inc | 287927 | HS-grad | 9 | Married-civ-spouse | Exec-managerial | Wife | White | Female | 15024 | 0 | 40 | United-States | 1 | . 32561 rows × 15 columns . iec_data = train_df.loc[:,(&quot;Income&quot;, &quot;Education&quot;, &quot;Workclass&quot;)] . iec_data . Income Education Workclass . 0 0 | Bachelors | State-gov | . 1 0 | Bachelors | Self-emp-not-inc | . 2 0 | HS-grad | Private | . 3 0 | Primary | Private | . 4 0 | Bachelors | Private | . ... ... | ... | ... | . 32556 0 | Assoc-acdm | Private | . 32557 1 | HS-grad | Private | . 32558 0 | HS-grad | Private | . 32559 0 | HS-grad | Private | . 32560 1 | HS-grad | Self-emp-inc | . 32561 rows × 3 columns . iec_data.pivot_table(values=&#39;Income&#39;, index=&#39;Education&#39;, aggfunc=&#39;count&#39;, margins_name=&#39;Income&#39;) . Income . Education . Assoc-acdm 1067 | . Assoc-voc 1382 | . Bachelors 5355 | . Doctorate 413 | . HS-grad 10501 | . Masters 1723 | . Preschool 51 | . Primary 4202 | . Prof-school 576 | . Some-college 7291 | . iec_data[iec_data.Income == 1].pivot_table(values=&#39;Income&#39;, index=&#39;Education&#39;, aggfunc=&#39;count&#39;, margins_name=&#39;Income&#39;) . Income . Education . Assoc-acdm 265 | . Assoc-voc 361 | . Bachelors 2221 | . Doctorate 306 | . HS-grad 1675 | . Masters 959 | . Primary 244 | . Prof-school 423 | . Some-college 1387 | . #collapse-hide iec_data_pivot = iec_data[iec_data.Income == 1].pivot_table(values=&#39;Income&#39;, index=&#39;Education&#39;, aggfunc=&#39;count&#39;, margins_name=&#39;Income&#39;) plt.figure(figsize=(16, 8)) sns.heatmap(iec_data_pivot, annot=True, fmt=&#39;.1f&#39;, cbar_kws= {&#39;label&#39;:&#39;Income range in categories&#39;}, cmap=&#39;coolwarm&#39;) plt.title(&#39;Incomes of various educated categories in Income wise&#39;) . . Text(0.5, 1, &#39;Incomes of various educated categories in Income wise&#39;) . #collapse-hide train_df[train_df.Income == 1].pivot_table(values=&#39;Income&#39;, index=[&#39;Native_country&#39;, &#39;Education&#39;], aggfunc=&#39;count&#39;) . . Income . Native_country Education . 0 Assoc-acdm 3 | . Assoc-voc 4 | . Bachelors 52 | . Doctorate 15 | . HS-grad 13 | . Masters 23 | . Primary 10 | . Prof-school 9 | . Some-college 17 | . Cambodia Bachelors 2 | . HS-grad 2 | . Primary 1 | . Some-college 2 | . Canada Assoc-acdm 1 | . Assoc-voc 3 | . Bachelors 9 | . Doctorate 4 | . HS-grad 8 | . Masters 3 | . Primary 1 | . Prof-school 1 | . Some-college 9 | . China Bachelors 8 | . Doctorate 5 | . HS-grad 3 | . Masters 4 | . Columbia Doctorate 1 | . Prof-school 1 | . Cuba Bachelors 4 | . Doctorate 1 | . ... ... ... | . South Prof-school 1 | . Some-college 3 | . Taiwan Bachelors 3 | . Doctorate 7 | . HS-grad 1 | . Masters 6 | . Prof-school 3 | . Thailand Assoc-acdm 1 | . Doctorate 1 | . Some-college 1 | . Trinadad&amp;Tobago HS-grad 1 | . Primary 1 | . United-States Assoc-acdm 247 | . Assoc-voc 336 | . Bachelors 2016 | . Doctorate 249 | . HS-grad 1583 | . Masters 866 | . Primary 202 | . Prof-school 374 | . Some-college 1298 | . Vietnam Bachelors 1 | . Doctorate 1 | . HS-grad 1 | . Primary 2 | . Yugoslavia Assoc-acdm 1 | . Bachelors 2 | . HS-grad 1 | . Primary 1 | . Some-college 1 | . 191 rows × 1 columns . gen_in_df = train_df.where(train_df.Income == 1).pivot_table(values=[&#39;Income&#39;], index=&#39;Education&#39;, columns=&#39;Workclass&#39;, aggfunc=&#39;count&#39;) . #collapse-hide gen_in_df.sort_index() . . Income . Workclass ? Federal-gov Local-gov Private Self-emp-inc Self-emp-not-inc State-gov . Education . Assoc-acdm 6 | 19 | 28 | 170 | 18 | 18 | 6 | . Assoc-voc 13 | 15 | 25 | 256 | 19 | 21 | 12 | . Bachelors 45 | 95 | 162 | 1495 | 171 | 163 | 90 | . Doctorate 11 | 15 | 17 | 132 | 29 | 31 | 71 | . HS-grad 46 | 73 | 90 | 1119 | 119 | 179 | 49 | . Masters 18 | 47 | 173 | 534 | 57 | 59 | 71 | . Primary 9 | 2 | 10 | 163 | 15 | 40 | 5 | . Prof-school 8 | 23 | 19 | 171 | 78 | 106 | 18 | . Some-college 35 | 82 | 93 | 923 | 116 | 107 | 31 | . plt.figure(figsize=(16, 8)) sns.heatmap(gen_in_df.sort_index(), annot=True, fmt=&#39;.1f&#39;, cbar_kws= {&#39;label&#39;:&#39;Income range in categories&#39;}, cmap=&#39;coolwarm&#39;) plt.title(&#39;Incomes of various educated categories in Income wise&#39;) . Text(0.5, 1, &#39;Incomes of various educated categories in Income wise&#39;) . Bachelors of Education field in Private Worclass are 1495.0 Income count . gen_sex_df = train_df.where(train_df.Income == 1).pivot_table(values=[&#39;Sex&#39;], index=&#39;Education&#39;, columns=&#39;Workclass&#39;, aggfunc=&#39;count&#39;) . gen_sex_df . Sex . Workclass ? Federal-gov Local-gov Private Self-emp-inc Self-emp-not-inc State-gov . Education . Assoc-acdm 6 | 19 | 28 | 170 | 18 | 18 | 6 | . Assoc-voc 13 | 15 | 25 | 256 | 19 | 21 | 12 | . Bachelors 45 | 95 | 162 | 1495 | 171 | 163 | 90 | . Doctorate 11 | 15 | 17 | 132 | 29 | 31 | 71 | . HS-grad 46 | 73 | 90 | 1119 | 119 | 179 | 49 | . Masters 18 | 47 | 173 | 534 | 57 | 59 | 71 | . Primary 9 | 2 | 10 | 163 | 15 | 40 | 5 | . Prof-school 8 | 23 | 19 | 171 | 78 | 106 | 18 | . Some-college 35 | 82 | 93 | 923 | 116 | 107 | 31 | . #collapse-hide plt.figure(figsize=(16, 8)) sns.heatmap(gen_sex_df, annot=True, fmt=&#39;.1f&#39;, cbar_kws= {&#39;label&#39;:&#39;Income range in categories&#39;}, cmap=&#39;coolwarm&#39;) plt.title(&#39;Incomes of various educated categories in Gender wise&#39;) . . Text(0.5, 1, &#39;Incomes of various educated categories in Gender wise&#39;) . Bachelors of Education field in Private Worclass are in marjority of Gender count basis . train_df.Sex.value_counts() . Male 21790 Female 10771 Name: Sex, dtype: int64 . gen_in_df.index.names . FrozenList([&#39;Education&#39;]) . gen_in_df.loc[:,&#39;Sex&#39;] . Workclass ? Federal-gov Local-gov Private Self-emp-inc Self-emp-not-inc State-gov . Education . Assoc-acdm 6 | 19 | 28 | 170 | 18 | 18 | 6 | . Assoc-voc 13 | 15 | 25 | 256 | 19 | 21 | 12 | . Bachelors 45 | 95 | 162 | 1495 | 171 | 163 | 90 | . Doctorate 11 | 15 | 17 | 132 | 29 | 31 | 71 | . HS-grad 46 | 73 | 90 | 1119 | 119 | 179 | 49 | . Masters 18 | 47 | 173 | 534 | 57 | 59 | 71 | . Primary 9 | 2 | 10 | 163 | 15 | 40 | 5 | . Prof-school 8 | 23 | 19 | 171 | 78 | 106 | 18 | . Some-college 35 | 82 | 93 | 923 | 116 | 107 | 31 | . [Report] Multivaiate Analysis . Specific Counts of each in different workclass belongs to various education on Income basis Bachelors of Education field in Private Worclass are 1495.0 Income count | . | Specific Counts of each in different workclass belongs to various education on Gender basis Bachelors of Education field in Private Worclass are in marjority of Gender count basis | . | .",
            "url": "https://tejakummarikuntla.github.io/notes/eda/2020/04/23/EDA-US-Salary-Dataset.html",
            "relUrl": "/eda/2020/04/23/EDA-US-Salary-Dataset.html",
            "date": " • Apr 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Monte Carlo Simulation",
            "content": "Monte Carlo Simulation . ref: https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-0002-introduction-to-computational-thinking-and-data-science-fall-2016/ . A method of estimating the value of an unknown quantity using the principles of inferential statistics. . Inferentila Statistics . Population: a set of examples | Sample: s proper subset of a population | key fact: a random Sample tends to exhibit the same properties as the population from which it is drawn | . | We make an inference about the population based upon some set of statistics we do on the sample. . | if we start drawing and the sample is not random then there is no reason to expect the same properties as that of the population. | . Example: . Given a single coin, estimate fraction of heads you would get if you flipped the coin an infinite number of times. . Consier one flip and you got Heads, Now how about saying the probability of getting heads for the next time is 1.0. | Let&#39;s flip the coin again, you the Heads again. | Flip the coin for 100 times, still you get the heads, now do you think that the next flip will come up heads? | Next, flipping the coin 100 Times, you noticed . 50 Heads | 48 Tails | 02 Heads in a series. . Do you think the probability of the next flip coming up heads is $ frac{52}{100}$? . | . | . Given the data, it&#39;s your best to estimate. But confidence should be low. . Why the difference in Confidence? . Confidence in our estimate depends upon two things Size of sample(e.g 100 versus 2) | variance of sample(e.g, all heads versus 52 heads) | As the variance grows, we need larger sample to have the same degree of confidence. | . | . looking into Roulette. . As the win or loose is very quick and the possiblites are more often in the game. We don&#39;t need to simulate, since answers obbious . It allow us to compare simulaiton results to actual probabilities . import random class FairRoulette(): def __init__(self): self.pockets = [] # One to 36 Pockets for i in range(1,37): self.pockets.append(i) self.ball = None # if you bet on a pocket and you win, you get len(pockets) - 1 # You bet a dollar if you win, you get $36, which means # your dollar + $35 back. If you loose you loose all. self.pocketOdds = len(self.pockets) - 1 def spin(self): self.ball = random.choice(self.pockets) def betPocket(self, pocket, amt): if str(pocket) == str(self.ball): return amt*self.pocketOdds else: return -amt def __str__(self): return &#39;Fair Roulette&#39; . def playRoulette(game, numSpins, pocket, bet, toPrint): totPocket = 0 for i in range(numSpins): game.spin() totPocket += game.betPocket(pocket, bet) if toPrint: print(numSpins, &#39;spins of&#39;, game) print(&#39;Expected return betting&#39;, pocket, &#39;=&#39;, str(100*totPocket/numSpins) + &#39;% n&#39;) return (totPocket/numSpins) . # Let&#39;s play the game game = FairRoulette() for numSpins in (100,1000000): # We are gonna see what happens when we spin it 100 times # and what happens when we spin it 1000000 times. for i in range(3): playRoulette(game, numSpins, 2, 1, True) . 100 spins of Fair Roulette Expected return betting 2 = 80.0% 100 spins of Fair Roulette Expected return betting 2 = -28.0% 100 spins of Fair Roulette Expected return betting 2 = 8.0% 1000000 spins of Fair Roulette Expected return betting 2 = -0.3124% 1000000 spins of Fair Roulette Expected return betting 2 = 0.4796% 1000000 spins of Fair Roulette Expected return betting 2 = 0.314% . What&#39;s gooing on here called the law of large numbers, or sometimes Bernoulli&#39;s law. . Law of Large Numbers . In repeated independnet tests with the same actual probability p of a aprticular outocme in each test, the chance that the fraction of times that outcome occurs differs from p converges to zero as the number of trails goes to infinity | . This says, If I were to spin this roulette wheel infinite number of times, the expected return would be 0. The real true probability from the mathematics. . And what this says is the close I get to inifnite, the close it will be to the true probaility. . The law of large numbers is a theorem from probability and statistics that suggests that the average result from repeating an experiment multiple times will better approximate the true or expected underlying result. ref:https://machinelearningmastery.com/a-gentle-introduction-to-the-law-of-large-numbers-in-machine-learning/ | . Independent and Identically Distributed . It is important to be clear that the observations in the sample must be independent. . Gambler&#39;s fallacy . Also known as the Monte Carlo Fallacy, the Gambler&#39;s Fallacy occurs when an individual erroneously believes that a certain random event is less likely or more likely, given a previous event or a series of events. This line of thinking is incorrect, since past events do not change the probability that certain events will occur in the future. . . &quot;On August 18, 1913, at casino in Monte Carlo, black came up a record twenty-six times in succession[in roulette]...[Threre] was a nerar-panicky rush to bet on red, beginning about the time black had come up a phenomenal fifteen times.&quot; --HUff and Geis, How to Take a Chace. . The Probability of 26 consecutive reds: $ frac{1}{67,108,865}$ | The Probability of 26 consecutive reds when previous 25 rolls were red: $ frac{1}{2}$ | . There&#39;s something that it&#39;s gambler&#39;s fallacy that&#39;s often confused with it, and that&#39;s called Regression to the Mean. This term was coined in 1885by Francis Galton. . The paper by him say: . if somebody&#39;s parents are both taller than average, it&#39;s likely that the child will be smaller thatn the parents. Conversely, if the parents are shorter than average, it&#39;s likely that the child will be taller than average. Then he came up with this Regression to the Mean. . Regression to the Mean . ref:https://www.youtube.com/watch?v=1tSqSMOyNFE We expect that a single result or the mean result from a small sample is likely. That is close to the central tendency, the mean of the population distribution. It may not be; in fact, it may be very strange or unlikely. . As we increase the sample size, the finding or mean of the sample will move back toward the population mean, back toward the true underlying expected value. This is called regression to the mean or sometimes reversion to the mean. . . Following an exreeme random event, the next random event is likely to be less extreme. Let&#39;s understand in roulette. #### Example: If you spin a fair roulette wheel 10 times and get 100% red[10 reds], that is an extreme event (probability = $ frac{1}{1024}$) | Now the Gambler&#39;s fallacy says, If I were to spin it another 10 times, it would need to even out. As in I should get more blacks than you would usually get to make up for these excess reds. | Now, What Regression to the Mean says, is differnt. It says, It&#39;s likely that in the next 10 spins, you will get fewer than 10 reds. . | So, If you look at the average of the 20 spins, it will be closer to the expected mean of 50% reds than to the 100% of the first 10 spins. So that&#39;s why it&#39;s called regression to the mean The more samples you take, the more likely you&#39;ll get to the mean. . | . class EuRoulette(FairRoulette): def __init__(self): FairRoulette.__init__(self) self.pockets.append(&#39;0&#39;) def __str__(self): return &#39;European Roulette&#39; class AmRoulette(EuRoulette): def __init__(self): EuRoulette.__init__(self) self.pockets.append(&#39;00&#39;) def __str__(self): return &#39;American Roulette&#39; . def findPocketReturn(game, numTrials, trialSize, toPrint): pocketReturns = [] for t in range(numTrials): trialVals = playRoulette(game, trialSize, 2, 1, toPrint) pocketReturns.append(trialVals) return pocketReturns random.seed(0) numTrials = 20 resultDict = {} games = (FairRoulette, EuRoulette, AmRoulette) for G in games: resultDict[G().__str__()] = [] for numSpins in (1000, 10000, 100000, 1000000): print(&#39; nSimulate&#39;, numTrials, &#39;trials of&#39;, numSpins, &#39;spins each&#39;) for G in games: pocketReturns = findPocketReturn(G(), numTrials, numSpins, False) expReturn = 100*sum(pocketReturns)/len(pocketReturns) print(&#39;Exp. return for&#39;, G(), &#39;=&#39;, str(round(expReturn, 4)) + &#39;%&#39;) . Simulate 20 trials of 1000 spins each Exp. return for Fair Roulette = 6.56% Exp. return for European Roulette = -2.26% Exp. return for American Roulette = -8.92% Simulate 20 trials of 10000 spins each Exp. return for Fair Roulette = -1.234% Exp. return for European Roulette = -4.168% Exp. return for American Roulette = -5.752% Simulate 20 trials of 100000 spins each Exp. return for Fair Roulette = 0.8144% Exp. return for European Roulette = -2.6506% Exp. return for American Roulette = -5.113% Simulate 20 trials of 1000000 spins each Exp. return for Fair Roulette = -0.0723% Exp. return for European Roulette = -2.7329% Exp. return for American Roulette = -5.212% . We are Sampling, that&#39;s why the results will change. If we ran a different simulation with a different seed we&#39;d get different numbers. . When ever you are sampling you can&#39;t be guaranteed to get prefect accuracy.It&#39;s always possible we get a weird sample and that&#39;s not the say that you won&#39;t get exactly the right answer. . This gets us to what&#39;s in some sense the fundamental question of all computational statistics, is How many samples do we need to look at before we can have real, justifiable confidence? . As we&#39;ve seen in coins example in the top. Our intuituion tells us that it depends upon the variability in the underlying possibilities. . We have to look at the variability in the data so let&#39;s look at first something called variance. $$ sigma^{2}(X) = frac{ sum{(x- mu)}^2}{|X|}$$ . Why we are squaring here? Squaring it has one virtue, which is that it means I don&#39;t care wheather the difference is positive or negative. And I shouldn&#39;t, right? I don&#39;t care which side of the mean it&#39;s on, But if that&#39;s all I wanted to do I could take the absolute value. | The other thing with squaring is it gives the outliers extra emphasis, because I&#39;m I am sqaring that distance. But you can think it&#39;s good or bad. but it&#39;s worth knowing it&#39;s a fact. | . | . Standard Deviation: . The more important thing to think about is standard deviation all by itself is a meaningless number. You always have to think about it in the context of the mean. If I tell you the standard deviation is 100. and I ask you wheather it&#39;s big or small, you have no idea. | If the mean is 100 and the standard deviation is 100, it&#39;s pretty big. If the mean is billion and standard deviaiton is 100, it&#39;s pretty small. | So you should never want to look at just the standard deviation. | . | . def getMeanAndStd(X): mean = sum(X)/float(len(X)) tot = 0.0 for x in X: tot += (x - mean)**2 std = (tot/len(X))**0.5 return mean, std . Confidence Levels and Intervals . Insted of estimating an unknown paramater by a single value (e.g., the mean of a set of trials), a confidence interval provides a range that is likely to conatin the unknown value and a confidence that the unknown value lays within that range. | The retun on betting a pocket 10k times in European roulette is -3.3%. The margian eroor is +/-3.5% with a 95% level of confidence.&quot; | . How do we compute Confidence Interval? Most of the time we compute them using Empirical rule. . The empirical rule says that if i take the data, find the mean, compute the standard deviaion. | Under Some Assumptions[Normal Distribution] ~68% of data within one standard deviation of mean | ~95% of data within 1.96 standard deviation of mean | ~99.7% of data within 3 standard deviation of mean. | . | . resultDict = {} games = (FairRoulette, EuRoulette, AmRoulette) for G in games: resultDict[G().__str__()] = [] for numSpins in (100, 1000, 10000): print(&#39; nSimulate betting a pocket for&#39;, numTrials, &#39;trials of&#39;, numSpins, &#39;spins each&#39;) for G in games: pocketReturns = findPocketReturn(G(), 20, numSpins, False) mean, std = getMeanAndStd(pocketReturns) resultDict[G().__str__()].append((numSpins, 100*mean, 100*std)) print(&#39;Exp. return for&#39;, G(), &#39;=&#39;, str(round(100*mean, 3)) + &#39;%,&#39;, &#39;+/- &#39; + str(round(100*1.96*std, 3)) + &#39;% with 95% confidence&#39;) . Simulate betting a pocket for 20 trials of 100 spins each Exp. return for Fair Roulette = -2.8%, +/- 116.156% with 95% confidence Exp. return for European Roulette = -28.0%, +/- 77.295% with 95% confidence Exp. return for American Roulette = -11.8%, +/- 147.122% with 95% confidence Simulate betting a pocket for 20 trials of 1000 spins each Exp. return for Fair Roulette = -4.24%, +/- 34.234% with 95% confidence Exp. return for European Roulette = -2.8%, +/- 28.136% with 95% confidence Exp. return for American Roulette = 4.22%, +/- 35.03% with 95% confidence Simulate betting a pocket for 20 trials of 10000 spins each Exp. return for Fair Roulette = 1.358%, +/- 11.674% with 95% confidence Exp. return for European Roulette = -1.414%, +/- 14.178% with 95% confidence Exp. return for American Roulette = -6.742%, +/- 6.269% with 95% confidence . Assumptions Underlying Empirical Rule . The mean estimation error is zero | The distribution of the error in the estimates is normal | .",
            "url": "https://tejakummarikuntla.github.io/notes/math/2020/01/28/Monte-Carlo.html",
            "relUrl": "/math/2020/01/28/Monte-Carlo.html",
            "date": " • Jan 28, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Introduction to Probability Distributions",
            "content": "Introduction to Distributions . - ref: https://www.coursera.org/lecture/data-analytics-accountancy-1/introduction-to-descriptive-statistics-WDaaH . Given a data set, an individual feature, or groups of features, can be converted into an empirical probability density function. An empirical density function can provide important insight into a data set, such as measures of a typical value, the variability around this typical value, and the shape of the data distribution. Often, however, we wish to understand some parent data set, from which our empirical data are sampled. In these cases, we often resort to theoretical distributions, which can provide invaluable guidance based on either theoretical expectations or previous applications. Theoretical distributions also can be used as generative models to create new data samples for subsequent analysis. In addition, theoretical distributions often are based on physical insights that can provide deeper insight into either our data or the process by which the data are acquired. . Theoretical Distributions . If you wish to delve deeply into probability theory, a good place to start is the Wikipedia discussion on probability theory. For this course, however, we can assume, without loss of generality, that theoretical distributions come in two varieties: . discrete distributions, which are restricted to integer values, and | continuous distributions, which are restricted to real values. | While we could roll our own Python code for these distributions, there are several standard implementations for the standard theoretical distributions. We will use the best supported version that is developed and maintained by the SciPy community within the [scipy.stats][ss] module. To use any of the available distributions, we will import them directly. For example, to use the uniform distribution (talked about in the next section), we simply import the distribution directly. . from scipy.stats import uniform . Given a theoretical distribution, we can create and use samples of random variables or sample the probability density function. Once imported, there are two ways to work with a distribution: . Create the distribution once, by creating the distribution with the desired parameters. Afterwards, this frozen distribution’s (the name refers to the fact the distribution&#39;s parameters are fixed, or frozen solid) functions are called as needed on the already created distribution. | Create the distribution each time it is needed (for instance to create random samples) by specifying the distribution’s parameters every time. | We will demonstrate both approaches in this notebook. However, of these two techniques, the easiest is the first technique where the distribution is created once from fixed parameters. This also provides a potential performance boost since the distribution is effectively only created once, and subsequently reused as needed. Before we proceed with a discussion on how to use theoretical distribution functions in Python, we perform our standard imports. . . %matplotlib inline # Standard imports import numpy as np import scipy as sp import pandas as pd import scipy.stats as stats import matplotlib.pyplot as plt import seaborn as sns # We do this to ignore several specific Pandas warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) sns.set(style=&quot;white&quot;) # Global color definition g_colors = [sns.xkcd_rgb[&#39;pale red&#39;], sns.xkcd_rgb[&#39;denim blue&#39;], sns.xkcd_rgb[&#39;bluegrey&#39;], sns.xkcd_rgb[&#39;faded green&#39;], sns.xkcd_rgb[&#39;dusty purple&#39;]] . . Uniform . The simplest theoretical distribution is the Uniform distribution, which has probability uniformly distributed between two end points. A Uniform distribution can be either discrete, being defined only for integer values, or continuous, being defined over all real values. In the latter case, we have a probability density function $f(x)$, which for $a leq x leq b$ is defined: . $$f(x) = frac{1}{(b - a)} $$ . with probability zero everywhere else. . To create a discrete uniform distribution, we use the randint function from the SciPy module. We first create our distribution by passing in the two end points, after which we can apply different functions to perform specific operations. The result of this operation is the creation of a frozen distribution, which allows different statistical functions to be easily applied to the created distribution without specifying the distribution parameters repeatedly (e.g., the end points of the Uniform distribution). . The primary function we apply to a frozen distribution is the rvs function, which generates a sample of random values from a distribution. For example, to generate a set of ten integers from a discrete Uniform distribution between five and ten: . udrv = randint(5, 10) ddata = udrv.rvs(1000) . Note that since this is a random process, a different result will be produced each time a set of random variables is drawn from this distribution. Thus, one possible output from this code would be for the discrete data variable ddata to be array([5, 6, 9, 7, 5, 8, 5, 9, 6, 9]), which demonstrates that the first value, 5, is included in the output, but the second value, 10, is not included. . For a continuous Uniform distribution, the process is similar, but we instead use the uniform function from the SciPy module. For example, to draw ten, real valued random numbers from a Uniform distribution, we create a uniform object before sampling from this frozen distribution to obtain random data: . ucrv = uniform(5, 10) cdata = ucrv.rvs(10) . Once again, the continuous data cdata is randomly drawn, but is now composed of real values, for example: . array([ 13.36813407, 8.40738799, 10.52660647, 9.83945343, 10.5271254 , 13.54307454, 11.77493877, 11.3713698 , 8.36527195, 7.58982897]) . The SciPy distribution objects provide a rich interface that can be used to obtain statistics, including the mean, standard deviation, or median, as well as summary distributions. One of the most important is the probability mass (or PMF for discrete distributions) or density (or PDF for continuous distributions) function. Once a distribution object has been created, we can sample the PMF (or PDF) by simply calling the pmf (or pdf) function with different values for the random variable of interest. For example, if we wish to sample the PDF of the uniform distribution between zero and one, we can sample the pdf function by using a linearly spaced set of values for x. . # Create distribution ucrv = uniform(0, 1) # Create sample points x = np.linspace(0, 1) # Construct PDF probs = ucrv.pdf(x) . The following code cell demonstrates this approach for both the discrete and continuous uniform distributions, where the probability is sampled over the range from zero to ten. . . from scipy.stats import randint, uniform # Set region of interest low = 1 ; high = 10 # Define frozen distributions # For discrete the format is a distribution [low, high) udrv = randint(low, high) # For continuous, the format is distribution [low, (high -low)) ucrv = uniform(low, high - low) # Function to plot discrete distribution def plot_pmf(axs, x, low, high, dist, froz): # Plot distribution PMF axs.plot(x, dist.pmf(x, low, high), &#39;o&#39;, ms=10, color=g_colors[0], label=f&#39;Discrete PMF&#39;) # Plot frozen PMF axs.vlines(x, 0, froz.pmf(x), lw=2, alpha=0.5, colors=g_colors[1], label=&#39;Frozen PMF&#39;) # Function to plot continuous distribution def plot_pdf(axs, x, low, high, dist, froz): # Plot distribution PMF axs.plot(x, dist.pdf(x, low, high - low), lw=7, alpha = 0.5, color=g_colors[0], label=f&#39;Contiuous PMF&#39;) # Plot frozen PMF axs.plot(x, froz.pdf(x), lw=2, alpha=0.75, color=g_colors[1], label=&#39;Frozen PMF&#39;) # Plot binned counts data = froz.rvs(1000) axs.hist(data, normed=True, histtype=&#39;step&#39;, lw=4, alpha=0.25, color=g_colors[2], label=&#39;Binned Counts&#39;) # Function to make overall plot, leaves distribution plotting # to one of the two helper plot functions def make_plot(axs, low, high, dist, froz, kind): # Decide whch helper function to call if kind == &#39;Discrete&#39;: # Sampling range x = np.arange(low, high) plot_pmf(axs, x, low, high, dist, froz) elif kind ==&#39;Continuous&#39;: # Sampling range x = np.linspace(low, high) plot_pdf(axs, x, low, high, dist, froz) # We scale the y-axis to allow room for the legend axs.set_ylim(0, 1.75 * 1.0/(high - low)) # Decorate plot axs.set_xticks(np.arange(low, high + 1)) axs.set_xlabel(&#39;X&#39;, fontsize=16) axs.set_ylabel(r&#39;P(X)&#39;, fontsize=16) axs.set_title(f&#39;{kind} Uniform Distribution: [{low}, {high})&#39;, fontsize=16) axs.legend(loc=1, fontsize=14) sns.despine(ax=axs, offset = 5, trim=True) # Now plot the binomial distribution for different # numbers of heads in 5 coin flips fig, axs = plt.subplots(figsize=(15, 6), nrows=1, ncols=2, sharey=True) # Call our plotting routine for the different distributions make_plot(axs[0], low, high, randint, udrv, &#39;Discrete&#39;) make_plot(axs[1], low, high, uniform, ucrv, &#39;Continuous&#39;) . . Poisson Distribution . One of the most useful discrete distribution functions is the Poisson distribution, which expresses the probability that a given number of events will occur in a fixed interval of time or space. For example, the Poisson distribution is used to model the number goals that might be scored in a soccer match, the number of floods that occur on a particular river, or the number of geographically diverse group of people that visit an online discussion forum. Formally, the following requirements must be met for the Poisson distribution to be valid: . If $X$ represents the number of events that can occur in the defined interval, then $X$ can only be either zero, or a positive integer. | The events are independent, thus one event occurring does not impact any of the other events. | The rate at which events occurs is constant. | Two events can not occur simultaneously. | The probability an event occurs within a small interval is proportional to the length of the interval or the actual probability is determined by a Binomial distribution where the number of trials is much larger than the number of successes. | When these conditions are true, given a positive, real-valued rate parameter $ mu$, we have the probability of observing $X$ events in a Poisson process: . $$ P(X) = frac{e^{- mu} mu^X}{X!} $$ . The Poisson distribution has mean and variance equal to the rate parameter, $ mu$. The Poisson distribution can be derived as a special case of the Binomial distribution where the number of trials goes to infinity, while the number of successes remains fixed, a result known as the Law of Rare Events. . To work with this distribution in Python, we will create the poisson distribution by specifying the rate parameter to create a frozen distribution: . # Create frozen Poisson Discrete Distribution (pdd) mu = 2.5 pdd = poisson(mu) . The following code cell demonstrates how to use the Poisson distribution within a Python script, where we create and visually compare three different distributions, for three different rate parameters. For simplicity, we do not create frozen distributions in this example, and instead create and sample the probability mass function for each distribution as needed. . . from scipy.stats import poisson # Number of points to sample for each distribution num_pts = 100 # Poisson distribution parameter definition mu = [1., 3., 5.] # Local label definition labels = [&#39;$ mu = 1$&#39;, &#39;$ mu = 3$&#39;, &#39;$ mu = 5$&#39;] # Sampling points x = np.arange(2.5 * mu[-1]) fig, ax = plt.subplots(figsize=(10, 6)) # Plot each distribution PMF, offset them by delta to give clarity delta = 0.25 plt.bar(x - 1.25 * delta, poisson.pmf(x, mu[0]), width=delta, color=g_colors[2], label = labels[0]) plt.bar(x, poisson.pmf(x, mu[1]), width=delta, color=g_colors[3], label = labels[1]) plt.bar(x + 1.25 * delta, poisson.pmf(x, mu[2]), width=delta, color=g_colors[4], label = labels[2]) # Decorate plot plt.xticks(x) plt.xlabel(r&quot;$X$&quot;, fontsize=18) plt.ylabel(r&quot;$P(X| mu)$&quot;, fontsize=18) plt.title(&quot;Poisson PMF&quot;, fontsize=20) plt.legend(fontsize=16) sns.despine(trim=True, offset = 5) . . Other Discrete Distributions . In addition to the Binomial, Uniform, and Poisson distributions, there are a number of other discrete theoretical distributions that prove useful on occasion, including the . Boltzmann, | Geometric, | Hypergeometric, and | Zipf distributions. | . While we will not review these (or other discrete) distributions in this notebook, you should be aware of their existence in case they prove useful in a future analytics task. . . Normal Distribution . Perhaps the most common distribution is the Normal (or Gaussian) distribution, which occurs frequently in nature and is also frequently seen as a result of the Central Limit Theorem, which will be discussed later in this notebook. The Normal distribution is often referred to as the bell curve since it resembles a bell. This distribution can be used to represent the heights of a group of people errors in measurements, health measurements such as blood pressure, grades on a test, or the measurements of items manufactured by the same machine. . Given a mean value, $ mu$, and standard deviation, $ sigma$, the probability density function for a Normal distribution is given by . $$ P(X) = frac{1}{ sqrt{2 pi sigma}} exp^{ frac{-(X - mu)^2}{2 sigma^2}} $$ . Note that the median, mode, and mean values all coincide for a Normal distribution, while the standard deviation measures how wide the distribution is around the central value. Given the ubiquity of the Normal distribution, you likely should memorize the percent of all data in a Normal distribution enclosed by $X$ standard deviations (i.e., $ mu pm X sigma$): . $X sigma$ Percent of data enclosed Interpretation . 1 | 68% | Data likely to be contained with this interval | . 2 | 95% | Data very likely to be contained with this interval | . 3 | 99.7% | Data almost certainly to be contained with this interval | . An alternative form, known as the standard normal distribution, has zero mean and unit standard deviation, thus the standard normal distribution has the following probability density function: . $$ P(X) = frac{1}{ sqrt{2 pi}} exp^{ frac{-X^2}{2}} $$ . A general Normal distribution can be transformed into a standard normal distribution by . subtracting the mean value, and | dividing by the standard deviation. | This process is sometimes encoded by the standard score or z-score, . $$ z = frac{x - mu}{ sigma} $$ . which has the benefit of (1) allowing different Normal distributions to be easily compared since they are all on the same scale, and (2) simplifying the interpretation of the result since the z-score implicitly is a measure of the number of standard deviations a value is from the mean of the distribution. . In Python, we create a Normal distribution by using the norm object in the SciPy module, which takes a mean value and a standard deviation. For example, to create a frozen Normal distribution with $ mu = 2.5$ and $ sigma = 1.2$: . from scipy.stats import norm ncd = norm(2.5, 1.2) . In the following code cell, we create and contrast three different Normal distributions. In this case, we do not use frozen distributions since we create and use the different distributions in one line of code. . . from scipy.stats import norm # Number of points to sample for each distribution num_pts = 100 # Gaussian distribution parameter definition mu = [1., 2., 3.] sig = [0.25, 0.5, 1.0] # Local label definition labels = [r&#39;$ mu = 1.0$, $ sigma = 0.25$&#39;, r&#39;$ mu = 2.0$, $ sigma = 0.5$&#39;, r&#39;$ mu = 3.0$, $ sigma = 1.0$&#39;] # Sampling points x = np.linspace(-0.5, 6.5, 250) fig, ax = plt.subplots(figsize=(10, 6)) # Create and Plot Normal PDFs for idx in range(len(mu)): ax.plot(x, norm.pdf(x, mu[idx], sig[idx]), color=g_colors[2 + idx], lw=3, label = labels[idx]) # Decorate plot ax.set_xlabel(r&quot;$X$&quot;, fontsize=18) ax.set_ylabel(r&quot;$P(X| mu, sigma)$&quot;, fontsize=18) ax.set_title(&quot;Gaussian PMF&quot;, fontsize=20) ax.legend(fontsize=16) sns.despine(trim=True, offset = 5) . . Other Continuous Distributions . While the Normal distribution is the most popular continuous distribution, there are many other continuous distributions that arise frequently in data analytics. Some of these have long histories in classical statistical analysis, such as the t-distribution and the F-distribution, which are typically used with small samples. Others arise from the analysis of diverse populations, such as the Exponential or Pareto distributions. In the next section, we explore eight specific distributions in more detail. The SciPy reference documentation provides a complete list of the distributions supported by the SciPy module. . Student&#39;s t . The [t]-distribution is used in many statistical tests, perhaps most prominently in assessing the statistical significance of the difference between two sample means. This distribution, which is similar to the Normal distribution but with heavier tails, was popularized by testing the quality of ingredients for making Guinness beer. The formal probability density function is rather complex, but in Python we can create and use a t-distribution by using the t object in the SciPy module. When creating this object, we specify the degrees of freedom via the df parameter. . from scipy.stats import t td = t(df=3) . In SciPy, this distribution is only defined for $ df &gt; 0$. To shift the distribution, the loc and scale parameters can be specified. . $ chi^2$ . A $ chi^2$ distribution is a widely used distribution in inferential statistics and when determining the quality of a model fit. Formally, the sum of $k$ independent, standard normal variables ($Z_i$) is distributed according to the $ chi^2$ distribution with $k$ degrees of freedom. The probability density function for a $ chi^2$ distribution is given by . $$ P(x) = frac{x^{(k/2 - 1)}e^{-x/2}}{2^{k/2} Gamma left( frac{k}{2} right)} $$for $x &gt; 0$, and $P(x) = 0$ otherwise. $ Gamma left( frac{k}{2} right)$ is the Gamma function. . In Python, we generate and use a $ chi^2$ distribution by using the chi2 object in the SciPy module. The $k$ parameter is specified by using the df, or degrees of freedom, parameter when creating the object. . from scipy.stats import chi2 cd = chi2(df=3) . F . The F-distribution arises in inferential statistics, particularly in the analysis of variance (or ANOVA). The F-distribution is generated by the ratio of two random variables that are described by $ chi^2$ distributions with degrees of freedom $d_1$ and $d_2$, respectively. Formally, the probability density function is rather complex, but in Python we can create and use an F-distribution by using the f object from the SciPy module. This object takes the two degrees of freedom as input parameters, dfn and dfd, for the degree of freedom in the numerator or denominator, respectively. . from scipy.stats import f fd = f(dfn=2, dfd=3) . In SciPy, this distribution is only defined for $ x &gt; 0$. To shift the distribution, the loc and scale parameters can be specified. . LogNormal Distribution . A LogNormal distribution is used to model random variables whose logarithm is distributed normally. This distribution can be used to model the size of living tissues, the length of comments posted on Internet forums, extreme values of rainfall, repair times for many systems, the allocation of wealth for approximately the lower 99% of the population, and the LogNormal distribution underlies the financial Black-Scholes model. . Formally, the probability density function for a LogNormal distribution is given by $$ P(X) = frac{1}{X sqrt{2 pi sigma^2}} exp left[- frac{1}{2} left( frac{ log(X/ mu)}{ sigma} right)^2 right] $$ . In Python, we use the lognorm object within the SciPy module to generate and use LogNormal distributions. To specify the mean ($ mu$) and standard deviation ($ sigma$) of this distribution, you must pass the standard deviation in using the s parameter and the exponential of the mean in via the scale parameter. . from scipy.stats import lognorm lnd = lognorm(scale = np.exp(1.25), s = 0.75) . In SciPy, this distribution is only defined for $ x &gt; 0$ and $s &gt; 0$. To shift the distribution, the loc and scale parameters can be specified. . . Note: We use a special trick in the following code cells called tuple unpacking. Simply put, we create arrays of parameters for the different distributions, and pass the entire tuple into the function as shown in the first code block below. The function will unpack the elements in the tuple and in order assign them to required (or optional) parameters. The only reason we do this is to shorten the code, if it is easier for you to follow or understand, simply unpack these operations as demonstrated in the second code block. . ln_params = [(0.5, 0.0, np.exp(0.75)), (.75, 0.0, np.exp(0.5)), (1.0, 0.0, np.exp(0.25))] flnl = [lognorm(*params) for params in ln_params] . flnl = [] flnl.append(lognorm(0.5, 0.0, np.exp(0.75))) flnl.append(lognorm(.75, 0.0, np.exp(0.5))) flnl.append(lognorm(1.0, 0.0, np.exp(0.25))) . . from scipy.stats import t, f, chi2, lognorm def make_plots(axs, x, fd_list, lbl, ylbl, title): for idx in range(len(fd_list)): axs.plot(x, fd_list[idx].pdf(x), color=g_colors[2 + idx], label=lbl[idx]) # Decorate plot axs.set_xticks(np.arange(np.min(x), np.max(x))) axs.set_xlabel(r&quot;$X$&quot;, fontsize=18) axs.set_ylabel(ylbl, fontsize=18) axs.set_title(title, fontsize=20) axs.legend(fontsize=16) sns.despine(offset = 5, trim=True) # Now we create our figure and axes for the plot we will make. fig, axs = plt.subplots(figsize=(10, 20), nrows=4, ncols=1) fig.subplots_adjust(hspace=.75) # Compute and plot Student&#39;s t-Distribution t_params = [(2, 0.5, 0.5), (3, 1.5, 0.75), (4, 2.5, 1.0)] ftl = [t(*params) for params in t_params] tlbl = [r&#39;$df = {0}$, $ mu = {1}$, $ sigma = {2}$&#39;.format(*p) for p in t_params] make_plots(axs[0], np.linspace(-3, 8, 250), ftl, tlbl, r&#39;$P(X | df, mu, sigma)$&#39;, &quot;Student&#39;s t-Distribution&quot;) # Compute and plot f-Distribution f_params = [(2, 2), (5, 1), (10, 12)] ffl = [f(*params) for params in f_params] flbl = [r&#39;$df1 =$ {0}, $df2 =$ {1}&#39;.format(*p) for p in f_params] make_plots(axs[1], np.linspace(0, 8, 250), ffl, flbl, r&#39;$P(X | df1, df2)$&#39;, &quot;f-Distribution&quot;) # Compute and plot ChiSqaured Distribution c_params = [2, 3, 5] fcl = [chi2(params) for params in c_params] clbl = [r&#39;$k = {0}$&#39;.format(p) for p in c_params] make_plots(axs[2], np.linspace(0, 13, 250), fcl, clbl, r&#39;$P(X | k)$&#39;, r&quot;$ chi^2$-Distribution&quot;) # Compute and plot LogNormal Distribution ln_params = [(0.5, 0.0, np.exp(0.75)), (.75, 0.0, np.exp(0.5)), (1.0, 0.0, np.exp(0.25))] flnl = [lognorm(*params) for params in ln_params] lnlbl = [f&#39;$ mu = {np.log(p[2]):4.2f}$, $ sigma = {p[0]:4.2f}$&#39; for p in ln_params] make_plots(axs[3], np.linspace(-0.5, 6.5, 250), flnl, lnlbl, r&#39;$P(X | mu, sigma)$&#39;, &quot;LogNormal Distribution&quot;) . . Power-Law . The Power-law distribution is seen frequently in situations where there are many small items and fewer large items, or where physical conditions are determined by polynomial relations (like mass and height). A classic example is incomes for a large group of individuals, as there are many people with small incomes and very few with large incomes. Other examples include populations in cities, masses of different species, or frequencies of word counts. In addition, certain critical events, such as landslides, can be modeled by using a Power-law distribution. Finally, other distributions (including the Pareto or Zipf distributions) can be considered special cases of the general Power-law distribution. . As a general form, we can consider a minimum cutoff ($x_{min}$), and a scaling exponent ($ alpha$). . $$ P(x) = frac{ alpha - 1}{x_{min}} left( frac{x}{x_{min}} right)^{- alpha} $$ . To use a Power-law distribution in Python, we use the powerlaw object within the SciPy module. The SciPy powerlaw distribution takes a single, real valued argument, a, which must be greater than zero. . from scipy.stats import powerlaw pld = powerlaw(a=1.25) . By default, in SciPy this distribution is defined over the range $0 leq x leq 1$. Additional parameters (loc and scale) can be specified to extend the range over which the distribution is defined. . Exponential . An Exponential distribution describes the time between events that occur continuously and at a constant rate (i.e., a Poisson process). The Exponential distribution can be used to model decay of radioactive particles, the maximum rainfall in a set period of time, the time between telephone calls, and the time until a default in certain credit risk modeling. . Note, even if a process generates events at a constant rate, the Exponential distribution can be applied over a time that the rate is nearly constant, for example, the time during which calls are generally made. One interesting result of the Exponential distribution is that the time to the next event (such as a phone call) is independent of the time since the last call. This property is formally known as being memoryless. . Given a rate parameter: $ lambda$, we have the following probability density function for the Exponential distribution: . $$ P(x) = lambda e^{- lambda x} $$if $x geq 0$ and zero otherwise. For this PDF, an Exponential distribution has mean value $ mu = lambda^{-1}$ and variance $ sigma^2 = lambda^{-2}$. . In Python, we use the expon object within the SciPy module to generate and use Exponential distributions. To set the rate parameter for the expon object, you set the scale parameter to the inverse of the rate parameter: . from scipy.stats import expon ed = expon(scale=1.25) . In SciPy, this distribution is defined only for $x geq 0$. . Pareto . A Pareto distribution is a special case of the Power-law distribution, originally developed to describe the allocation of wealth, where 20% of the population was inferred to control 80% of the wealth. Thus, the Pareto distribution informally refers to distributions that follow the 80-20 rule (or a similar pre-defined allocation) with scale parameter $ alpha$. This distribution can be used to model the sizes of human settlements, values in oil fields, sizes of sand particles, or the standardized price returns on individual stocks. . Formally, for any $x &gt; x_m$, the probability density function for a Pareto distribution is given by $$ P(X &gt; x) = frac{ alpha x_m^{ alpha}}{x^{ alpha + 1}} $$ . with mean value . $$ mu = frac{ alpha x_m}{ alpha - 1}$$ . and variance . $$ sigma^2 = frac{ alpha x_m^2}{( alpha - 1)^2( alpha - 2)}$$ . In Python, we use the pareto object within the SciPy module to generate and use Pareto distributions. To specify a scale parameter when creating this distribution, set the b parameter to the desired scale value. . from scipy.stats import pareto bd = pareto(b=1.25) . In SciPy, this distribution is only defined for $ x geq 1$. To shift the distribution, the loc and scale parameters can be specified. . Cauchy . The Cauchy distribution appears when solving different physical conditions or mathematical expressions, and also results from the ratio of two independent normally distributed random variables. The Cauchy distribution is the canonical pathological distribution in that both the mean and variance for a Cauchy distribution are undefined. Formally, the probability density function for a Cauchy distribution is given by . $$ P(x) = frac{1}{ pi gamma} left[ frac{ gamma^2}{(x - x_0)^2 + gamma^2} right] $$In Python, we generate and use a Cauchy distribution by using the cauchy object in the SciPy module. By default, $ gamma$ and $x_0$ are assumed to be one. . from scipy.stats import cauchy cd = cauchy(scale=1.25) . The distribution can be shifted by specifying the loc and scale parameters. . . from scipy.stats import powerlaw, expon, pareto, cauchy # Now we create our figure and axes for the plot we will make. fig, axs = plt.subplots(figsize=(10, 20), nrows=4, ncols=1) fig.subplots_adjust(hspace=.75) # Compute and plot Scipy PowerLaw Distribution pl_params = [0.001, 0.01, 2] fpll = [powerlaw(p, 0.5, 5.5) for p in pl_params] pllbl = [r&#39;$ alpha = {0}$&#39;.format(p) for p in pl_params] # Scale y-axis to be logarithmic for clarity axs[0].set_yscale(&#39;log&#39;) make_plots(axs[0], np.linspace(-0.5, 8.5, 250), fpll, pllbl, r&#39;$P(X | alpha)$&#39;, &quot;PowerLaw Distribution&quot;) # Compute and plot Exponential Distribution e_params = [1, 2, 3] fel = [expon(scale=p) for p in e_params] elbl = [r&#39;$ lambda =$ {0}&#39;.format(p) for p in e_params] make_plots(axs[1], np.linspace(-0.5, 7.5, 250), fel, elbl, r&#39;$P(X | lambda)$&#39;, &quot;Exponential Distribution&quot;) # Compute and plot Pareto Distribution p_params = [3, 2, 1] fpl = [pareto(p) for p in p_params] plbl = [r&#39;$ alpha = {0}$&#39;.format(p) for p in p_params] make_plots(axs[2], np.linspace(0.5, 5.5, 250), fpl, plbl, r&#39;$P(X | alpha)$&#39;, r&quot;Pareto Distribution&quot;) # Compute and plot Cauchy Distribution ca_params = [(0.5, 0.5), (1.5, 0.75), (2.5, 1.0)] fcal = [cauchy(*p) for p in ca_params] calbl = [r&#39;$X_0 = {0}$, $ gamma = {1}$&#39;.format(*p) for p in ca_params] make_plots(axs[3], np.linspace(-3.5, 6.5, 250), fcal, calbl, r&#39;$P(X | X_0, gamma)$&#39;, &quot;Cauchy Distribution&quot;) . . Random Sampling . One of the most powerful uses for a theoretical distribution is the creation of a sample of random values from the specified distribution. All distributions in the SciPy module have an rvs method that will sample a specified number of random variables from the parent distribution. The number sampled is specified by the size parameter. This process is demonstrated in the following two code blocks. . First, we create a Normal distribution, generate a sample of ten thousand random values, and finally compare a histogram of the random values to the theoretical distribution. Note how the agreement between the two is extremely strong. Second, we create an Exponential distribution and generate a new sample of ten thousand random values. This time we compare the histogram of the sampled random values to the theoretical distribution on a regular, linear plot, and also a plot where the y-axis has a logarithmic scaling. The second version clearly shows the strong agreement between the Exponential distribution and sampled random values. . . fig, axs = plt.subplots(figsize=(12, 8)) nd = norm(1.5, .75) x = np.linspace(-2.5, 5.5, 1000) axs.plot(x, nd.pdf(x), lw=2, alpha=0.75, color=g_colors[0], label=&#39;Model PDF&#39;) data = nd.rvs(10000) axs.hist(data, bins=20, normed=True, histtype=&#39;bar&#39;, lw=4, alpha=0.25, color=g_colors[1], label=&#39;Binned Random Data&#39;) axs.set_xlabel(&#39;X&#39;, fontsize=18) axs.set_xticks(np.arange(np.min(x), np.max(x) + 1)) axs.set_ylabel(&#39;P(X)&#39;, fontsize=18) axs.set_title(&quot;Random Draw from Normal PDF&quot;, fontsize=20) axs.legend(fontsize=16) sns.despine(offset = 5, trim=True) . # Two figures, one over the other fig, axs = plt.subplots(figsize=(8, 16), nrows=2, ncols=1) fig.subplots_adjust(hspace=.5) # Helper function that draws random data from exponential # and also computes the PDF, and plots the results def plot_exp(axs, lam = 1.0): ed = expon(lam) x = np.linspace(0, 10.5, 1000) axs.plot(x, ed.pdf(x), lw=2, alpha=0.75, color=g_colors[0], label=&#39;Model PDF&#39;) data = ed.rvs(10000) axs.hist(data, normed=True, histtype=&#39;bar&#39;, lw=4, alpha=0.25, color=g_colors[1], label=&#39;Binned Random Data&#39;) axs.set_xlabel(&#39;X&#39;, fontsize=18) axs.set_xticks(np.arange(np.min(x), np.max(x))) axs.set_ylabel(&#39;P(X)&#39;, fontsize=18) axs.set_title(&quot;Random Draw from Exponential PDF&quot;, fontsize=20) axs.legend(fontsize=16) sns.despine(offset = 5, trim=True) # We plot the first one normally plot_exp(axs[0]) # We plot the second one with logarithmic scaling on the y-axis axs[1].set_yscale(&#39;log&#39;) plot_exp(axs[1]) . . Alternative Distribution Forms . To this point, we have focused on the probability density functions for specific theoretical distributions. There are, however, other forms for these distribution functions that can be important in solving specific problems as they may simplify the resulting analysis. In this section, we will discuss four specific forms: . Cumulative distribution function (CDF) | Percent point function (PPF). | Survival function (SF). | Inverse survival function. | The cumulative distribution function specifies the probability that a real-valued random variable will have a value less than or equal to a specific value, $x$. Thus, for a continuous distribution, the CDF is the area under the PDF from minus infinity to $x$. The CDF is used frequently in hypothesis testing and in Monte Carlo integration, and is given by the following expression, where $f_X(x)$ is a function evaluated at $x$ of the real valued, random variable $X$. . $$ F_X(x) = P(X leq x) = int_{- infty}^x f(u) du$$ . One important result that follows from this definition is that the cumulative probability between two limits $(a, b]$ is given by the difference in the CDF evaluated at these two points: . $$ P(a &lt; X leq b) = F_X(b) - F_X(a)$$ . The percent point function of a real-valued, random variable $x$, alternatively known as the quantile function or inverse cumulative distribution function, specifies the value of random variable at which the probability of getting $x$ is less than or equal to the given probability. Alternatively, you can view this function as stating the value of $x$ for which the CDF has a specific value $p$. Thus in a simple one-dimensional example, given a probability, $p$, the PPF indicates the value $x$ at which the PDF has probability equal to or less than the target probability $p$. The PPF can be useful in determining quantiles, quartiles, and the median, and also in random number generation. . The survival function, $S(t)$, gives the probability that a real-valued random variable will exceed a specific value $x$. Often the variable is time, and the survival function indicates the probability an item, such as a mechanical device, a patient, or other item will last (or survive) beyond a given time. This function is given by $1 - CDF$. . $$ S(t) = P( T &gt; t) = int_t^{ infty} f(u) du = 1 - F(t)$$ . The inverse of this function, called the inverse survival function, gives the real-valued random variable at which the probability of surviving is less than or equal to the target probability $p$. . The following code cell displays these four functions, along with the probability density function (PDF) for a Normal distribution. In addition, the standard quartiles are displayed to provide an additional reference point in interpreting these other distribution forms. . . # Now Seaborn specific modifications sns.set_context(&quot;poster&quot;, font_scale=1.0) sns.set_style(&quot;white&quot;) # Now we create our figure and axes for the plot we will make. fig = plt.figure(figsize=(12, 18)) fig.subplots_adjust(wspace=0.5, hspace=0.5) # Define plot layout #fig, ax = plt.subplots() ax1 = plt.subplot2grid((3,2), (0,0), colspan=2) ax2 = plt.subplot2grid((3,2), (1,0)) ax3 = plt.subplot2grid((3,2), (1, 1)) ax4 = plt.subplot2grid((3,2), (2, 0)) ax5 = plt.subplot2grid((3,2), (2, 1)) mean = 1.0 sigma = 0.75 # Freeze normal distribution dist = norm(loc=mean, scale=sigma) # Define variate and probability arrays x = np.linspace(mean - 3. * sigma, mean + 3. * sigma, 1000) p = np.linspace(0, 1.0, 100) # Plot functional distributions ax1.plot(x, dist.pdf(x), lw=4, alpha=0.75, color=g_colors[2]) ax2.plot(x, dist.cdf(x), lw=2, alpha=0.75, color=g_colors[3]) ax3.plot(p, dist.ppf(p), lw=2, alpha=0.75, color=g_colors[4]) ax4.plot(x, dist.sf(x), lw=2, alpha=0.75, color=g_colors[3]) ax5.plot(p, dist.isf(p), lw=2, alpha=0.75, color=g_colors[4]) # Show &#39;special&#39; probabilities # First horizontal lines ax2.axhline(0.25, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax2.axhline(0.5, 0.25, 0.75, color=g_colors[0], lw=3, ls = &#39;-.&#39;) ax2.axhline(0.75, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax4.axhline(0.25, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax4.axhline(0.5, 0.25, 0.75, color=g_colors[0], lw=3, ls = &#39;-.&#39;) ax4.axhline(0.75, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) # Second vertical lines ax3.axvline(0.25, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax3.axvline(0.5, 0.25, 0.75, color=g_colors[0], lw=3, ls = &#39;-.&#39;) ax3.axvline(0.75, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax5.axvline(0.25, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax5.axvline(0.5, 0.25, 0.75, color=g_colors[0], lw=3, ls = &#39;-.&#39;) ax5.axvline(0.75, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) # Set the title ax1.set_title(&quot;Gaussian PDF&quot;, fontsize=20) ax1.set_xlabel(&#39;X&#39;, fontsize=18) ax1.set_xticks(np.arange(np.min(x), np.max(x) + 1)) ax1.set_ylabel(&#39;P(X)&#39;, fontsize=18) ax2.set_title(&quot;Cumulative Density Function&quot;) ax2.set_xlabel(&#39;X&#39;, fontsize=18) ax2.set_xticks(np.arange(np.min(x), np.max(x) + 1)) ax2.set_ylabel(&#39;Cumulative Probability&#39;, fontsize=18) ax3.set_title(&quot;Percent Point Function&quot;) ax3.set_xlabel(&#39;Cumulative Probability&#39;, fontsize=18) ax3.set_yticks(np.arange(np.min(x), np.max(x) + 1)) ax3.set_ylabel(&#39;X&#39;, fontsize=18) ax4.set_title(&quot;Survival Function&quot;) ax4.set_xlabel(&#39;X&#39;, fontsize=18) ax4.set_xticks(np.arange(np.min(x), np.max(x) + 1)) ax4.set_ylabel(&#39;Cumulative Probability&#39;, fontsize=18) ax5.set_title(&quot;Inverse Survival Function&quot;) ax5.set_xlabel(&#39;Cumulative Probability&#39;, fontsize=18) ax5.set_yticks(np.arange(np.min(x), np.max(x) + 1)) ax5.set_ylabel(&#39;X&#39;, fontsize=18) # Now Seaborn specific modifications sns.despine(offset=10, trim=True) . . Central Limit Theorem . The importance of the Normal (or Gaussian) distribution is hard to overstate. Not only do many data set in Nature display normality, but the Normal distribution can often be applied to the analysis of data that have been sampled repeatedly from a different distribution that is non-normal. . Assume we have been given a set of $n$ data points. Formally, as long as all of these data are . mutually independent, | drawn from a common distribution, and | the mean ($ mu$) and standard deviation ($ sigma$) of the data points are finite, | then the Central Limit Theorem, or (CLT) states that the sample average of these data points is Normally distributed with mean ($ mu$) and variance ($ sigma^2/n$): . $$P left(x = frac{1}{n} sum_i^n x_i right) rightarrow sqrt{ frac{n}{2 pi sigma^2}} exp{ left(- frac{n (x - mu)^2}{2 sigma^2} right)} $$The first two items in the previous list are often shortened to iid, which is an abbreviation for independent and identical distributed. This theorem essentially implies that sums of random quantities will be approximately distributed as a normal distribution, with the accuracy of the approximation determined by the number of points ($n$). In addition, the CLT often leads analysts to average out the noise and to assume a Normal model. While these conditions are often met, and thus the last two steps are valid, one should always exert caution and be sure that all conditions are met before jumping to strong assumptions. . The CLT is demonstrated in the following code cell, where we repeatedly simulate the process of flipping ten coins. By progressively increasing the number of samples that we accumulate, we can see that as $n$ increases our data set becomes increasingly well modeled by a Normal distribution. . . from scipy.stats import binom # Parameters for our distribution ph = 0.5 # Probability of heads trails = 10 # Number of trails (ten coins) # Convenience function to flip a coin x *times*, # and plot the results. def plot_coinspmf(p, trails, times, axes): # Build an array of coin flips flips = binom.rvs(trails, p, size=times) # Now histogram them, two bins, heads or tails results, _ = np.histogram(flips, bins=int(trails + 1)) # For simplicity turn into a Pandas Series. coins = pd.Series(results) / np.sum(results) x = np.arange(11) # Now plot the coin flip results ax = coins.plot.bar(ax=axes, color=g_colors[1], alpha = 0.75, fontsize=14, rot=0, xticks=x, yticks=np.linspace(0, 0.5, 11)) if times // trails &gt; 1: title = f&#39;{trails} coins: {times // trails} flips each&#39; else: title = f&#39;{trails} coins: {times // trails} flip each&#39; ax.set_title(title, fontsize=20) ax.set_ylabel(&quot;Probability&quot;, fontsize=18) # Now make a Gaussian and overplot. mu = trails * p sigma = np.sqrt(trails * p * (1 - p)) norm = 1./(sigma * np.sqrt(2 * np.pi)) y = norm * np.exp(-1. * (x - mu)**2/(2. * sigma**2)) ax.plot(x, y, linewidth=3, linestyle =&#39;-.&#39;, color=g_colors[0], alpha = 0.75) sns.despine(trim=&#39;True&#39;, left = False, right=True) # Create figure fig, axs = plt.subplots(figsize=(12, 12), nrows=2, ncols=2) adj = plt.subplots_adjust(hspace=0.25, wspace =0.25) # Perform coin flip simulations plot_coinspmf(ph, trails, 10, axs[0][0]) plot_coinspmf(ph, trails, 100, axs[0][1]) plot_coinspmf(ph, trails, 1000, axs[1][0]) plot_coinspmf(ph, trails, 10000, axs[1][1]) . . QQ Plot . Given a data set, one approach for gaining insight is to determine whether a specific distribution is a good match for one or more columns or features. For the Normal distribution, the standard technique for testing this assumption is the quantile-quantile plot, or QQ-plot. Effectively this plot sorts the data and compares the empirical distribution of the data to the theoretical distribution. If the empirical data lie on (or close to) a diagonal (or 45 degree ) line in a QQ-plot, the data can be effectively modeled by the target theoretical distribution (e.g., a Normal distribution). . To make this plot, we use the ProbPlot object from the Statsmodel module. As shown below, we can create a ProbPlot object by passing in normally distributed data, along with the data distribution to which we wish to compare the data (in this case a standard Normal distribution). We next create a qqplot objects to make the quantile-quantile plot, along with a qqline to display the diagonal line that aids in the interpretation of the match between our data and the selected distribution. . This technique is demonstrated in the next two code cells, where we first generate normally distributed random data and compare to a Normal distribution. The agreement is quite good (as expected), indicating that our data is modeled nicely by using a Normal distribution. In the second example, we generate log-normally distributed random data, which is not (again, as expected) well matched by a Normal distribution. Note, a QQ-Plot is not restricted to using a Normal distribution, but given the prevalence of Normal distributions in Nature, a QQ-Plot is often used as a test for Normality. . . # We use functions from the Statsmodel module import statsmodels.api as sm # Create figure fig, axs = plt.subplots(figsize=(8, 8)) # Create normal distributed data nd = norm(0, 1) # Setup Probability comparison plot pp = sm.ProbPlot(nd.rvs(100), dist=norm) # Create and show QQ-Plot qq = pp.qqplot(ax=axs, marker=&#39;D&#39;, markeredgecolor=g_colors[4], alpha=0.25) axs = qq.axes[0] # Show QQ Line (45 angle) sm.qqline(axs, line=&#39;45&#39;, fmt=&#39;k-.&#39;) # Decorate axs.set_title(&#39;QQ Plot: Normal Test&#39;) sns.despine(offset=5, trim=True) . # Create figure fig, axs = plt.subplots(figsize=(8, 8)) # Create lognormal distributed data lnd = lognorm(0.5, 0.5) # Setup Probability comparison plot pp = sm.ProbPlot(lnd.rvs(100), dist=norm) # Create and show QQ-Plot qq = pp.qqplot(ax=axs, marker=&#39;D&#39;, markeredgecolor=g_colors[4], alpha=0.25) axs = qq.axes[0] # Show QQ Line (45 angle) sm.qqline(axs, line=&#39;45&#39;, fmt=&#39;k--&#39;) # Decorate axs.set_title(&#39;QQ Plot: LogNormal Test&#39;) sns.despine(offset=5, trim=True) . . Fitting Distributions . On the other hand, if we have a reason to expect our data can be modeled by a specific theoretical distribution, we can directly fit the data by using the scipy module. Each distribution in the scipy module includes a fit method, which will compute the best fit parameter values for the specific distribution to the provided data. We demonstrate this in the following code cell where we generate normally distributed data, and next use the fit method on the Normal distribution to determine the best fit Normal distribution mean and standard deviation. Finally, we plot the binned normally distributed data along with the original theoretical Normal distribution and best fit Normal distribution. . Since the data generation is a random process, each time the code cell is run the results will be different. Thus, even though the process demonstrating this technique might seem circular, the agreement is usually quite good (or strong), which might be surprising. In practice, one would most likely generate a qq-plot to determine if a Normal distribution would provide a good match for a data set, after which the best fit distribution can be computed from the data. . The importance of this process is hard to overstate. Having a model for a data set can provide both physical insight into what generated the data, as well as the ability to generate new data that should appear indistinguishable from the original data set. This later technique can prove useful in situations where obtaining new data can be either prohibitively expensive or even impossible due to physical constraints. . . # Generate random data from normal distribution mean = -1.5 sigma = 1.25 model = norm(mean, sigma) data = model.rvs(size=1000) print(f&#39;Model Mean = {mean:6.2f}&#39;) print(f&#39;Model Std = {sigma:6.2f} n&#39;) # Fit a Normal distribution to the data fit_params = norm.fit(data) fit_dist = norm(loc=fit_params[0], scale=fit_params[1]) print(f&#39;Fit Mean = {fit_params[0]:6.3f}&#39;) print(f&#39;Fit Std = {fit_params[1]:6.3f} n&#39;) # Define variate x = np.linspace(mean - 3. * sigma, mean + 3. * sigma, 100) # Plot data and fitted model fig, axs = plt.subplots(figsize=(12, 8)) # Plot Fit axs.plot(x, fit_dist.pdf(x), lw=4, alpha=0.75, color=g_colors[0], label=&#39;Fit PDF&#39;) # Plot Model axs.plot(x, model.pdf(x), lw=2, ls=&#39;-.&#39;, alpha=0.75, color=g_colors[4], label=&#39;Model PDF&#39;) # Plot data axs.hist(data, bins=20, normed=True, histtype=&#39;bar&#39;, lw=4, alpha=0.25, color=g_colors[1], label=&#39;Binned Random Data&#39;) axs.set_xlabel(&#39;X&#39;, fontsize=18) axs.set_xticks(np.arange(np.min(x), np.max(x) + 1)) axs.set_ylabel(&#39;P(X)&#39;, fontsize=18) axs.set_title(&quot;Distribution fitting example&quot;, fontsize=20) axs.legend(fontsize=16) sns.despine(offset = 5, trim=True) . Model Mean = -1.50 Model Std = 1.25 Fit Mean = -1.465 Fit Std = 1.223 . . Student Exercise . In the empty Code cell below, write and execute code to gnerate data from a different distribution, such as a lognormal, and then fit the data with the new distribution. . . . Ancillary Information . The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional. . Statistics tutorial from Scipy | A somewhat dated blog article on simple statistical analysis in Python | A blog article on using distributions from Python | . &copy; 2017: Robert J. Brunner at the University of Illinois. . This notebook is released under the Creative Commons license CC BY-NC-SA 4.0. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder. .",
            "url": "https://tejakummarikuntla.github.io/notes/math/2020/01/25/Introduction-to-distributions.html",
            "relUrl": "/math/2020/01/25/Introduction-to-distributions.html",
            "date": " • Jan 25, 2020"
        }
        
    
  
    
        ,"post7": {
            "title": "Descriptive Statistics",
            "content": "Random Variables . A Random variable is a variable that takes on numerical values as a result of random expriments of mesuarement, associates a numerical value with each possible outcom .. R.V&#39;s must have numberical values. . Discrete Random Variable: has a finite number of values or an infinate sequence of values(0,2,3,...) AND the differnces between coutcomes are menaningful Die throw can have 1,2,3,4,6 and each is a meaningfully different. | . | Continuous Random Variable: ahs a nearly infinite numbers of outcomes that cannot be easiily counted AND the differences between teh outcomes are NOT meaningful With average income, the difference between $40,00 and $40,001 is not meaningful | . | . Discrete Probability Distribution . The probability distribution for a random variable X describes how assigned to each outcome for the random variable. Let 0=Heads and 1=Tails for a coin flip. so ouir discrete random variable x is described as: x = 0,1 . The Probability for each outcome is described by a discrete porbability funcion denoted by P(x) $$ sum P(x) = 1$$ sum of all RV Probabilities P(x) must equal 1 | . Uniform Probability Distribution . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import uniform . #Generating Uniform random numbers from scipy.stats import uniform # We can generate random variables/numbers # from uniform distribution from uniform distribution’s rvs function like uniform.rvs. # Here we are generating 1000 Random Variables (rvs) between 0 and 10. data_uniform = uniform.rvs(size=1000, loc=0, scale=10) . len(data_uniform) . 1000 . data_uniform[:20] . array([7.33092523, 6.91163852, 6.81076766, 6.10710074, 6.3161025 , 8.99572593, 5.78687982, 2.91428895, 5.54023576, 7.51997417, 7.85882577, 1.15105694, 6.13031743, 1.06657784, 1.61417764, 1.58547971, 6.65943099, 3.0200822 , 9.14553805, 7.58889625]) . ax = sns.distplot(data_uniform, bins=100, kde=True, color=&#39;skyblue&#39;, hist_kws={&quot;linewidth&quot;: 15,&#39;alpha&#39;:1}) ax.set(xlabel=&#39;Uniform &#39;, ylabel=&#39;Frequency&#39;) . [Text(0, 0.5, &#39;Frequency&#39;), Text(0.5, 0, &#39;Uniform &#39;)] . Probability Distributions . Standard Normal Distribuion or Standardization: . import numpy as np import pandas as pd from scipy import stats import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import StandardScaler %matplotlib inline . dset = pd.DataFrame([[1],[2],[2],[3],[3],[3],[4],[4],[5]]) . print(&quot;Mean: &quot;,dset.mean()[0]) print(&quot;Standard deviation&quot;, dset.std()[0]) . Mean: 3.0 Standard deviation 1.224744871391589 . density_plot(np.array(dset)) . # Applying Standardization. std_sc = StandardScaler() zscore_data = std_sc.fit_transform(dset) . /home/tejakummarikuntla/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler. return self.partial_fit(X, y) /home/tejakummarikuntla/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler. return self.fit(X, **fit_params).transform(X) . print(&quot;Mean: &quot;,zscore_data.mean()) print(&quot;Standard deviation&quot;, zscore_data.std()) . Mean: 0.0 Standard deviation 1.0 . def density_plot(ds): f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {&quot;height_ratios&quot;: (0.2, 1)}) mean = np.mean(ds) median = np.median(ds) mode = stats.mode(ds)[0] sns.boxplot(ds, ax=ax_box) ax_box.axvline(mean, color=&#39;r&#39;, linestyle=&#39;--&#39;) ax_box.axvline(median, color=&#39;g&#39;, linestyle=&#39;-&#39;) ax_box.axvline(mode, color=&#39;b&#39;, linestyle=&#39;-&#39;) sns.distplot(ds, hist=True, kde=True, ax=ax_hist, color = &#39;darkblue&#39;, hist_kws={&#39;edgecolor&#39;:&#39;black&#39;}, kde_kws={&#39;linewidth&#39;: 4}) ax_hist.axvline(mean, color=&#39;r&#39;, linestyle=&#39;--&#39;) ax_hist.axvline(median, color=&#39;g&#39;, linestyle=&#39;-&#39;) ax_hist.axvline(mode, color=&#39;b&#39;, linestyle=&#39;-&#39;) plt.legend({&#39;Mean&#39;:mean,&#39;Median&#39;:median,&#39;Mode&#39;:mode}) ax_box.set(xlabel=&#39;&#39;) plt.show() . density_plot(zscore_data) . type(zscore_data) . numpy.ndarray . np.median(zscore_data) . 0.0 . stats.mode(dset)[0] . array([[3]]) . l=[1,2,4] . dic = {1:&quot;a&quot;, 2:&quot;b&quot;, 3:&quot;4&quot;} . dic.items() . dict_items([(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;4&#39;)]) . for key,val in dic.items(): print(key) . 1 2 3 . lst = np.array([ [3, 2, 4], [6, 7, 8], [1, 4, 1]]) . two = np.array([ [1], [4], [5] ]) . lst*two . array([[ 3, 2, 4], [24, 28, 32], [ 5, 20, 5]]) . lst@two . array([[31], [74], [22]]) .",
            "url": "https://tejakummarikuntla.github.io/notes/math/data%20science/2020/01/24/Descriptive-Statistics-Intro.html",
            "relUrl": "/math/data%20science/2020/01/24/Descriptive-Statistics-Intro.html",
            "date": " • Jan 24, 2020"
        }
        
    
  
    
        ,"post8": {
            "title": "Introduction to Probability",
            "content": "Introduction to Probability . - reference . Probability is a concept that is generally familiar to most people. We use probabilities frequently, such as in weather forecasts where the probability of rain might be 60%, or in games of chance where the roll of a die might determine the winner. But what does probability really mean? Or, more importantly, how do we calculate and use probabilities effectively? . This notebook explores the concept of probability, including how to calculate probabilities from theoretical expectations and from the distribution of features in a particular data set. This will lead us through he world of combinations, to empirical probabilities and probability density functions, and finally to Bayes theorem. Before we get started, however, we first perform our standard Python imports. . . # Set up Notebook %matplotlib inline # Standard imports import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns # We do this to ignore several specific Pandas warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) sns.set(style=&quot;white&quot;) . Combinatorial Analysis . A basic problem in computing probabilities is determining the likelihood of a particular event. In some cases, this can be done simply by counting possible states, such as with flipping a coin. In others, however, a more formal approach can help derive insights more quickly. This approach is known as combinatorial analysis, where one determines the number of different states that are possible from the initial conditions. For example, given a single coin, there are only two possible states when flipping the coin: heads (H) or tails (T). With more coins, the space of possible events grows quickly, and if multiple coins are used, determining the possible end states becomes a much more complex problem. . For these more complex cases, there are two fundamental concepts that can be applied: permutations and combinations. The difference between these two concepts is that with permutations the order of the values is important, for example, when entering the values for a security code or a telephone number, while for combinations the order is not important, such as selecting lottery numbers or when purchasing items that are used to create a food item. We will walk through each of these cases in more detail. . Permutations . When we combine things together and the order does matter, we have a statistical permutation. Given a set of n values, a permutation is the number of different ways that the r values can be selected from the set. As a simple example, consider our set to be the first three numbers: $1, 2, 3$. Before proceeding, there are two approaches to this problem. . Permutation with replacement, where we select an item from the input set and replace it when we are done. | Permutation without replacement, where we select an item from the input set, and discard it when we are done. | For the first case, we select a number, and replace it back in the input set. So, every time we select a number, we have three choices. Thus, if we pick one number, we have three permutations, if we select two we have $3 * 3$, or $9$ permutations, and so on. In general, if we have n objects in our input set, we have n options each time we select another one; thus, if we select two objects, we have $n * n$, or $n^2$. This leads to the general formula for permutation with replacement: . $$ ^nP_r = n^r $$ . This is demonstrated in the following code cell, where we compute the permutations with replacement by using the product function to compute the possible permutations in this case. . . # Number of items in our input set n = 4 # number of items to select at one time r = 2 # Create our data data = np.arange(n) # Below is specific to this example print(&#39;Input data = {}&#39;.format(data)) print(f&#39;Possible permutations = {(n**r)}&#39;) from itertools import product # Compute and display actual permutations (with replacement) for cm in product(data, repeat=r): for c in cm: print(f&#39;{c} &#39;, end=&#39;&#39;) print() . Input data = [0 1 2 3] Possible permutations = 16 0 0 0 1 0 2 0 3 1 0 1 1 1 2 1 3 2 0 2 1 2 2 2 3 3 0 3 1 3 2 3 3 . . On the other hand, permutation without replacement provides fewer options with each new selection. In the previous example [1, 2, 3], we have three ways to select our first number, two ways to select the second (since one number has been removed from the input set), and only one way to select the final number. Note that we cannot select more than three numbers from this input set without replacement. We can express this mathematically as $ 3 times 2 times 1$, which is used so often that a shorthand notation has been developed called factorial, which we represent by using the exclamation mark ($!$). Thus, in our specific case, we have $ 3 times 2 times 1 = 3!$, or in general $n!$ for an input set that has $n$ objects. . If we were only interested in selecting one number from the input set, the answer is three. But what if our input set contained the numbers 1, 2, 3, and 4, and we wanted to know how many ways we could pick two numbers without replacement? In this case, we have four ways to select the first number, and three ways to select the second number, or $4 times 3$. Or, alternatively, we had to remove the last two multiplications, which we can do by writing out the full multiplication, as was the case for permutations with replacement, and dividing out by the number of ways that we aren&#39;t interested in counting, which in this case would be $2 times 1$. To write this in a simple math form, we can generalize this to select $r$ objects from a set of $n$ objects: . $$ ^nP_r = frac{n!}{(n - r)!} $$ . In the following code example, we first compute the number from our formula before using the built-in function permutations to generate and display the available permutations. . . # We reuse the data and values for n and r from above. # Display data print(&#39;Input data = {}&#39;.format(data)) from math import factorial # Compute our factorials and display theoretical permutation count nf = factorial(n) nrf = factorial(n - r) print(f&#39;Possible permutations = {nf/nrf}&#39;) from itertools import permutations # Compute and display actual permutations for pm in permutations(data, r): for p in pm: print(f&#39;{p} &#39;, end=&#39;&#39;) print() . Input data = [0 1 2 3] Possible permutations = 12.0 0 1 0 2 0 3 1 0 1 2 1 3 2 0 2 1 2 3 3 0 3 1 3 2 . . Student Exercise . Change the value of $r$ in the previous code cell (e.g., to 2 or 3) and see how the results change. Does the output make sense? Now change the value of n and r (keep them relatively small!). Do the new results make sense? . . Combinations . When we combine things together but the order doesn&#39;t matter, we have a statistical combination. Combinations are similar to permutations, described previously. However, since the order is not important in combinations, we have fewer options to count, thus the result is smaller. When we compute the number of options in a combination, we once again need to differentiate between computing a combination with replacement and without replacement. . First, we explore combinations without replacement. In this case, we can compute the number of permutations without replacement, which as shown earlier is given by . $$ ^nP_r = frac{n!}{(n - r)!} $$ . However, since order is not important, this number must be reduced. Specifically, if we are selecting three items, there are $3! = 6$ ways they can be ordered. Or, in general, if are selecting $r$ items from a set of $n$, we have $r!$ ways to order the selection. Thus, in combination without replacement, we must reduce the permutation count by $r!$. Since this particular combination is used so frequently, it has a special name, $n choose r$. . $$ ^nC_r = {n choose r} = frac{n!}{r!(n - r)!}$$ . Combination without replacement is demonstrated in the following code cell, where we compute the number of different combinations for our input data set. . . # Number of items in our input set n = 4 # number of items to select at one time r = 2 # Create our data data = np.arange(n) print(&#39;Input data = {}&#39;.format(data)) # Compute our factorials and display theoretical combination count nf = factorial(n) nrf = factorial(n - r) rf = factorial(r) print(f&#39;Possible combinations = {nf/(nrf * rf)}&#39;) from itertools import combinations # Compute and display actual combinations for cm in combinations(data, r): for c in cm: print(f&#39;{c} &#39;, end=&#39;&#39;) print() . Input data = [0 1 2 3] Possible combinations = 6.0 0 1 0 2 0 3 1 2 1 3 2 3 . . Combination with replacement is more complex, thus we simply present the formula: . $$ ^nC_r = {r + n - 1 choose r} = frac{(r + n - 1)!}{r!(n - 1)!}$$ . The formula is derived in different places and is demonstrated in the following code cell. . . # We reuse the data and values for n and r from above. print(&#39;Input data = {}&#39;.format(data)) # Compute our factorials and display theoretical combination count n1f = factorial(n - 1) rn1f = factorial(r + n - 1) rf = factorial(r) print(f&#39;Possible combinations = {rn1f/(n1f * rf)}&#39;) from itertools import combinations_with_replacement # Compute and display actual combinations (with replacement) for cm in combinations_with_replacement(data, r): for c in cm: print(f&#39;{c} &#39;, end=&#39;&#39;) print() . Input data = [0 1 2 3] Possible combinations = 10.0 0 0 0 1 0 2 0 3 1 1 1 2 1 3 2 2 2 3 3 3 . . Student Exercise . Change the values of $r$ in the previous code cell and see how the results change. Does the output make sense? Now change the value of n and r (keep them relatively small!). Do the new results make sense? . . Probability . One standard approach to estimating probability is to define the probability of an event by the frequency of its occurrence. Thus, for example, we expect that the probability a fair coin comes up heads is 0.5. Formally, this is computed as the number of successes divided by the number of attempts. In general, in the absence of outside information, we follow the principle of indifference and choose uniform probabilities for potential events. Thus we expect: . Coin: $P(H) = frac{1}{2}$ Dice: $P(1) = frac{1}{6}$ Card: $P(3) = frac{1}{13}$ . We can demonstrate several of these actions by performing simulations. . . Flipping Coin . First, we will demonstrate computing a probability from a sequence of events by simulating a coin flip. In the following code cell, we assume a fixed probability of flipping heads, and simulate flipping a heads ($H$) or tails ($T$) by using the choice function in the NumPy module to randomly select one of these two outcomes. We display the simulated flips, accumulate the number of heads, and display the ratio of heads to total flips as a probability derived from the frequency of occurrences. . . # Simulation probability of flipping heads ph = 0.5 # Number of coin flips to simulate num_flips = 25 # Simulate Coin flips def flip_coin(N, p=0.5): prob = [p, (1 - p)] return np.random.choice([&#39;H&#39;,&#39;T&#39;], size=N, p=prob) # Accumulate flips flips=flip_coin(num_flips, ph) # Count Heads num_heads = np.sum(flips == &#39;H&#39;) # Display results print(&quot;Flips:&quot;, &quot; &quot;.join(flips))t print(f&quot;Number of Heads: {num_heads}&quot;) print(f&#39;P(H) = {num_heads/num_flips} (Number of Heads/Total Flips)&#39;) . Flips: H H T H T T T T H T H T H H T T H T H H T T T H T Number of Heads: 11 P(H) = 0.44 (Number of Heads/Total Flips) . . Rolling Dice . Second, we can provide a more complex example by simulating the rolling of a six-sided dice. In this case, we must randomly select one of six options ($1$, $2$, $3$, $4$, $5$, $6$), which by default we assume all have equal probabilities ($= 1/ 6$). At the end, we accumulate the number of rolls that result in an ace (or $1$), and display the ratio of this sum to the total number of rolls as a probability derived from the frequency of occurrence. . . # Assume equal probability over all states prob_die = 6 * [(1/6)] # Number of dice rolls num_rolls = 25 # Simulate a dice roll def roll_die(N, prob = prob_die): return np.random.choice([&#39;1&#39;, &#39;2&#39;, &#39;3&#39;, &#39;4&#39;, &#39;5&#39;, &#39;6&#39;], size=N, p=prob) # Accumulate rolls rolls=roll_die(num_rolls) # Accumulate rolls of an &#39;Ace&#39; num_ones = np.sum(rolls == &#39;1&#39;) #Display results print(&quot;Rolls:&quot;, &quot; &quot;.join(rolls)) print(f&quot;Number of Ones: {num_ones}&quot;) print(f&#39;P(1) = {num_ones/num_rolls} (Number of Ones/Total Rolls)&#39;) . Rolls: 2 2 1 4 2 4 5 6 6 1 2 1 5 5 3 1 6 2 2 4 1 6 1 1 6 Number of Ones: 7 P(1) = 0.28 (Number of Ones/Total Rolls) . . Bernoulli Trial . If we conduct an experiment, such as flipping a coin or rolling a die, we can define a specific outcome as a success. For example, if the coin shows a head or we roll an ace (or a 1), we might consider those successful outcomes. An experiment of this type is known as a Bernoulli trial. If the probability of success is denoted by $p$, we have the probability of success as $P(1) = p$, and conversely the probability of failure is $P(0) = (1 - p)$. If we let $x$ be a random variable that is determined by this probability, then $X$ is known as a Bernoulli random variable. In addition, we can obtain the odds of a successful outcome as the ratio $p/(1 - p)$. . Binomial Distribution . If we extend the experiment to include more than one trial, we have a binomial experiment, which consists of a set number of statistically independent Bernoulli trials. Thus, flipping a coin multiple times is a binomial experiment, since each coin flip is independent. In general, a random variable that is determined from $n$ statistically independent Bernoulli trials, where each trial has probability $p$ of success, follows a Binomial distribution $B(n, p)$. In this case, the probability of exactly $r$ successes is given by . $$p(r) = {n choose r} p^r (1 - p)^{n - r}$$ . To demonstrate, we can calculate the probability of getting an odd roll of a die. Here the probability of success, $p$ is equal to 0.5, since we have three ways of getting an odd roll and six total options ($p = 3/6 = 1/2$). In addition, we have one trial ($n = 1$), and one success ($r = 1$). . $$P(X) = frac{1!}{1! 0!} left( frac{1}{2} right)^1 left( frac{1}{2} right)^0 = frac{1}{2} $$ . Or, we can calculate the probability of flipping five heads in ten coin tosses. For this example, the probability of getting heads for a fair coin is just 0.5, $n = 10$, and $r=5$, thus . $$P(X) = frac{10!}{5! (10 - 5)!} left( frac{1}{2} right)^5 left( frac{1}{2} right)^5 approx 0.246 $$ . This distribution can be applied when . the different trials (i.e., $n$) are statistically independent, | each trial can only end in one of two states, and | the probability of success (and failure) is constant for each trial. | . As a simple example, in the following code cell, we calculate the probability of flipping different numbers of heads from five fair coins, for which we assume the probability of getting a heads = 0.5. . . Important Information . An important issue that arises when performing data analytic tasks is reproducibility. The data analyst should obtain consistent results when performing the same analysis, even if the code, programming language, or analysis tool changes. When working with random variables, however, this can be made more difficult since each time a process is executed entirely new data are generated. . Computers, however, are entirely deterministic systems. Random data generated by traditional computers are not completely random. These processes often start with a seed, which impacts the random sequence that is generated. By reusing the same seed, one can minimize the impact of a random process within a computer program and enable code reproducibility. This can be done for many random number generators, such as those within NumPy, by explicitly setting a global seed, or, preferentially, always specifying the same random_state (or seed) to the specific random generator. . For example, if we are using the binomial distribution in the SciPy module, we can specify the seed globally, or we can specify a random state when we sample random variables from the distribution: . # Set global seed np.random.seed(42) # Set repeatable local random state dist = binom(n, p) data = dist.rvs(10, random_state=42) . . # Number of coins n = 5 # Simulated probability of heads ph = 0.5 from scipy.stats import binom x = np.arange(6) # Create frozen distribution rv = binom(n, ph) pmf = rv.pmf(x) for idx, pr in enumerate(pmf): print(f&#39;P({idx} Heads) = {pr:6.2f}&#39;) # Now plot the binomial distribution for different number of heads in 5 coin flips fig, ax = plt.subplots(figsize=(12, 6)) ax.plot(x, binom.pmf(x, n, ph), &#39;o&#39;, color=sns.xkcd_rgb[&quot;pale red&quot;], ms=10, label=&#39;Binomial PMF&#39;) #Plot frozen PMF ax.vlines(x, 0, pmf, colors=sns.xkcd_rgb[&quot;denim blue&quot;], lw=2, alpha=0.5, label=&#39;Frozen PMF&#39;) # Decorate plot ax.set_xlabel(&#39;Number of Heads&#39;, fontsize=16) ax.set_ylabel(r&#39;Probability of $N$ Heads&#39;, fontsize=16) ax.set_title(&#39;Binomial Distribution: 5 Coins&#39;, fontsize=16) ax.legend(fontsize=16) sns.despine(offset = 5, trim=True) . P(0 Heads) = 0.03 P(1 Heads) = 0.16 P(2 Heads) = 0.31 P(3 Heads) = 0.31 P(4 Heads) = 0.16 P(5 Heads) = 0.03 . . The binomial distribution forms the basis for much of probability theory. This distribution is both simple, and yet can be applied in many cases. Given a sample of $n$ events, which each have probability $p$ of succeeding, the mean value is simply $np$, and the variance is $np(p - 1)$. The binomial distribution can be applied to predicting the outcome of (multiple) coin tosses, the expected number of failures when manufacturing parts, or the expected number of fraudulent claims detected (assuming uniform probability of fraud occurring). . Interpreting Frequency Probabilities . Probabilities are often misinterpreted, and this can easily be demonstrated by simulating flips of a fair coin. Obviously, the probability of obtaining a heads is 0.5 for each coin flip. However, by simulating a number of coin flips, interesting patterns will eventually appear, such as a number of heads or tails appear in sequence. When viewing such a pattern, one generally feels that this pattern implies that the opposite result is overdue. However, each coin flip is independent of the others, and thus the probability is still equally likely of flipping either a heads or a tails. . We demonstrate this in the following code cell, where we generate a large number of coin flips. Peruse through the simulated flips and take notice of any patterns, yet at the end, the probability estimated from the frequency of an event being observed is still very close to the expected value. . . # Probabilty of flipping heads ph = 0.5 # How many times to flip a coin num_flips = 250 # Function to print out sequence of results from a coin flip def print_flips(flps, print_width = 50): # How many coin flip results we have displayed count = 0 # We limit to 100 total if flps.shape[0] &gt; 100: flips = flps[:-100] else: flips = flps[:] # Iterate through flips, displaying one at a time. for flip in flips: count += 1 print(f&#39;{flip}&#39;, end=&#39; &#39;) # We format to only display print_width flips on one line if (count % print_width) == 0: print() # Print Newline print() # Print newline # Function to generate num_flips coin flips def make_flips(num_flips, ph): # Flip coin num_flips number of times flips = flip_coin(num_flips, ph) # Display results print_flips(flips) # Accumulate heads num_heads = np.sum(flips == &#39;H&#39;) # Display results print(f&quot;Number of Heads: {num_heads}&quot;) print(f&#39;P(H) = {num_heads/num_flips} (Number of Heads/Total Flips)&#39;) make_flips(num_flips, ph) . H H T H T H T H T T H T H H H H T T H H T T T H T T H T T T T T H T H T H H T T H H H H H H T H H H H H H T T H H T H T T T H T H H T T T T H H T T T T H H T H T H H T H H H H T T H H H H T H H H T H T H T T H H T H H T H H H T T T T H T T H T H H H T T H T T T H H T T H T H T T H H T T T T T H H H Number of Heads: 122 P(H) = 0.488 (Number of Heads/Total Flips) . . Student Exercise . Change the probability of heads to a lower and higher value in the code snippet above (i.e., ph). How do the results change (you also might want to change the number of coin flips)? . . Long Term Frequency . When we flip a single, fair coin, the expectation is that fifty percent of the time a heads is the result. Thus, if we would expect five heads if we flip a single, fair coin ten times. Of course, when we actually perform this experiment (or in this notebook, simulate the result), this is not always the case. This is because a coin flip is fundamentally a random process, with each flip being independent of the previous coin flips. Thus, in reality, if we have flipped a coin ten times, and have gotten a tails each time, the eleventh flip still has a fifty percent chance of coming up tails (although at this point we might start questioning the fair coin hypothesis). . This should be clear after running the previous code cells. Each time they are run the result is different. However, if you simulate a large number of coin flips, the average number of heads will approach the theoretical expectation. This is an example of the Law of Large Numbers, which states that as we increase the number of times an experiment is performed, the average value of these experiments will approach the expected value. . This is demonstrated in the following code cell, where we simulate a very large number of coin flips. While the average will fluctuate due to random chance, over time, the average will come close the expected value of 0.5. . . # Simulated probability of flipping heads ph = 0.5 # Number of heads to compute total = 50000 ; size = 1000 # Sampling array to plot coin flips attempts = np.arange(size, total + size, size) # Simulate coin flips flips = flip_coin(total, ph) # Accumulate head frequencies results = [np.sum(flips[:idx] == &#39;H&#39;)/np.float(idx) for idx in attempts] # Make plot fig, ax = plt.subplots(figsize=(12, 8)) # Plot simulated flips ax.plot(attempts, results , &#39;o-&#39;, alpha=0.75, label=&#39;Simulation&#39;) # Plot theoretical expectation ax.axhline(ph, 0, 1, color=&#39;r&#39;, alpha=0.75, label=&#39;Theory&#39;) # Annotate plot ax.legend(fontsize=16) ax.set_title(&#39;Frequency P(H)&#39;, fontsize=20) ax.set_xlabel(&#39;Number of Flips&#39;, fontsize=18) ax.set_ylabel(&#39;P(H)&#39;, fontsize=18) # Compute y limits (how far above or below P(H) to display) yp = abs(max(results) - ph) yn = abs(min(results) - ph) y_delta = max(yp, yn) ax.set_ylim(ph - y_delta, ph + y_delta) sns.despine(trim=True) . . Student Exercise . Change the probability of heads to a lower and higher value in the code snippet above (i.e., ph). How does the plot change? . . Probability Density . When we have multiple outcomes, each with their own probability (which might all be equal), we can construct a functional representation of the possible end states. This function is called the Probability Density Function or PDF. A PDF can be constructed by normalizing each outcome by the total number of outcomes. Note that for discrete data, such as simulated coin flips, rolling a die, or selecting cards from a deck, an alternative name, Probability Mass Function or PMF is often used. This simple approach can often provide unique insights into a data set. For example, the PDF clearly indicates the mode, which is the point of highest probability in the PDF. Furthermore, the shape of the PDF can indicate whether a data set is skewed, or has long tails. A probability density function for a random variable $X$ is typically denoted by $f_X(x)$, and is related to the probability of an event occurring through an integration over the space of interest. . $$ P(a leq x leq b) = int_a^b f_X(x) dx $$ . The probability mass function is simply the discrete version of this result, which instead is a sum over the discrete probabilities. . $$ P(a leq x leq b) = sum_a^b f_X(x) $$ . In either case, the probability density (or mass) function is normalized to unit probability over the range of interest (for example from $[- infty, infty]$). . To demonstrate how to construct a probability mass function, we first simulate the rolling of a single die ten times in the following code cell. This code performs the simulation, assuming a fair die, and plots the accumulated outcomes. In the following code cell, we repeat this process, but for two, larger numbers of die rolls. In each of these two cases, we normalize the data so that a probability mass function is displayed. In addition, we show the expected probability for a fair, six-sided die. Notice how the simulated PMF approaches the true probability as the number of simulated die rolls increases. This is another manifestation of the Law of Large Numbers. . . # Define our die to have six sides with equal probability num_sides = 6 prob_die = num_sides * [1./num_sides] times = 10 # Number of times to flip the coin # Simulate numerical dice roll def roll_die(N, prob = prob_die, num_sides = 6): return np.random.choice(np.arange(1, num_sides + 1), size=N, p=prob) # Accumulate dice rolls rolls = roll_die(times, prob_die, num_sides) # Now histogram them, two bins, heads or tails results, bins = np.histogram(rolls, bins=num_sides) # For simplicity turn into a Pandas Series. rolls = pd.Series(results, index=np.arange(1, num_sides + 1)) # Draw expected probability ax.axhline(xmin=0.01, xmax=1.5, y=times/6, c=&#39;c&#39;, linestyle=&#39;-.&#39;) # Now plot the die roll results ax = rolls.plot.bar(fontsize=14, rot=&#39;horizontal&#39;) ax.set_title(f&#39;{times} Rolls&#39;, fontsize=24) ax.set_ylabel(&quot;Occurrences&quot;, fontsize=18) ax.set_xticklabels(np.arange(1, 7), fontsize = 18) # Clean up plot sns.despine(trim=&#39;True&#39;, bottom=True) . from numpy import random # Define our die to have six sides with equal probability num_sides = 6 prob_die = num_sides * [1./num_sides] # Convenience Function to roll a die x *times*, and plot the results. def plot_dice_pmf(ax, times, p, ns = num_sides, ): # Build an array of die rolls rolls = roll_die(times, p, ns) # Now histogram them. results, bins = np.histogram(rolls, bins=ns) # For simplicity turn into a Pandas Series. rolls = pd.Series(results, index=np.arange(1, ns + 1)) / np.sum(results) # Plot the expected results ax.axhline(xmin=0.1, xmax=0.9, y=1/ns, c=&#39;c&#39;, linestyle=&#39;-.&#39;) # Now plot the dice roll results ax = rolls.plot.bar(ax=ax, fontsize=14, rot=&#39;horizontal&#39;, yticks=[0, 0.1, 0.2, 0.3]) ax.set_title(f&#39;{times} Rolls&#39;, fontsize=24) ax.set_ylabel(&quot;Probability&quot;, fontsize=18) ax.set_xticklabels(np.arange(1, 7), fontsize = 18) sns.despine(trim=&#39;True&#39;, left = False, bottom=True) # Compare two dice rolls flips fig, axs = plt.subplots(figsize=(12, 6), nrows=1, ncols=2)#, sharey=True) adj = plt.subplots_adjust(hspace=1.5, wspace=0.5) # We will roll a die 100 and then 1000 times and compare results plot_dice_pmf(axs[0], 100, prob_die, num_sides) plot_dice_pmf(axs[1], 1000, prob_die, num_sides) . . Estimating Probability . To this point, we have discussed probabilities from a theoretical perspective. By inferring the likelihood of an event (such as rolling a die and getting an ace or flipping a coin and getting a heads), we were able to simulate all possible outcomes. In general, however, our knowledge of the world is limited by the data that we observe. Fortunately, many of the concepts that were previously introduced can be applied to these real-world cases by interpreting data distributions as probabilities. For example, we can translate a sample of restaurant receipts into the probability that a group of patrons will generate a total bill within a specific range. The next section introduces this concept more formally, by demonstrating how to construct a PDF from a data set. . Empirical PDF . Given a data set, we can compute a probability density function by simply binning the data and normalizing the bin counts to ensure unit probability over the entire set of bins. As previously stated, a PDF can often provide unique insights into a data set. A PDF can also be useful as we attempt to infer future behavior. Given a representative data set, an empirically constructed PDF can be used to model future expectations. For example, if one constructs a PDF of fraud occurring in a company&#39;s financial record as a function of the total amount involved, one can infer how likely fraud might be in a new audit. Of course, the accuracy of any prediction is strongly dependent as always on the quality of data being analyzed, so it is important to acquire a fair and representative sample. . We demonstrate this concept in the following code cell, where we create a probability mass function for the total_bill column in the tip data set. This distribution clearly shows that most bills are ten and twenty-five dollars, with a long tail to higher amounts. In addition, notice how the probability is low for any given bin. This is a result of our normalization, because the total probability for any given bin is the product of the bin width and this probability. Since we have ten bins that are each five dollars wide, the probability of a total_bill lying within one of the highest two bins is actually closer to 25 %. . . # Load the data tips = sns.load_dataset(&#39;tips&#39;) # Now we create our figure and axes for the plot we will make. fig, ax = plt.subplots() # Now we want to make a default histogram ax.hist(tips[&#39;total_bill&#39;], normed=True, bins=10, range=(5, 55), alpha = .5) # Set our axis limits, labels, and plot title ax.set_title(&quot;Probability Mass Function&quot;, fontsize=14) ax.set_xlim(-2,62) ax.set_xlabel(&quot;Total Bill ( $)&quot;, fontsize=14) ax.set_ylabel(&quot;Probability&quot;, fontsize=14) # Clean up final result sns.despine(offset=10, trim=True) . . CDF . The probability density (or mass) function provides a concise summary of the likelihood of finding a given value (such as the total_bill shown above). In some cases, however, we wish to explore the distribution in its entirety, in order to ask question such as What is the median price for a total bill? In these cases, an alternative form for the probability distribution is used, the Cumulative Density Function or CDF. This form of the probability distribution contains the cumulative probability up to a specific value, as opposed to the differential probability at a given value. . As a result, the CDF enables one to find the value at which the cumulative probability has reached a certain threshold, such as $50$ % for the median, or the quantile values. This is demonstrated in the following code cell, where we generate a cumulative histogram for the total_bill column in the tips data set. From the resulting figure, one can see that the median bill is around twenty dollars. . . tips = sns.load_dataset(&#39;tips&#39;) # Now we create our figure and axes for the plot we will make. fig, ax = plt.subplots() # Now we want to make a default histogram ax.hist(tips[&#39;total_bill&#39;], normed=True, bins=20, cumulative=True, alpha = .5) # Set our axis limits, labels, and plot title ax.set_title(&quot;Cumulative Mass Function&quot;, fontsize=14) ax.set_xlim(0, 50) ax.set_xlabel(&quot;Total Bill ( $)&quot;, fontsize=14) ax.set_ylabel(&quot;Probability&quot;, fontsize=14) # Clean up final result sns.despine(offset=10, trim=True) . . Student Exercise . In the empty Code cell below, write and execute code to make a new cumulative density plot (like the one above, but split out lunch and dinner for the total_bill column). Plot the two histograms in separate colors. . . . Comparing Probability Distributions . The probability density (or mass) function informs a viewer of the likelihood of a particular event occurring. For example, we can see from the total_bill PMF that the total_bill is more likely to be around twenty dollars than forty dollars. We can go beyond this, however, if we have two probability distributions that have been derived from the same data. For example, if we split the total_bill PMF by time into a lunch and dinner PMF, we can directly compare the likelihood for the total_bill at these two times by simply dividing the two PMFs. . We demonstrate this concept in the following code cell, where we use the NumPy histogram function to generate binned counts from the two data sets of interest. Next, we normalize each of these two histograms to convert them into probability mass functions. Finally, we plot the difference between these probability mass functions. The resulting plot clearly shows that lunch bills are generally smaller than dinner bills, since the difference is negative for lower values of total_bill, and generally positive at higher values. Note that the largest values for total_bill have smaller differences indicating that the result is more ambiguous (or in other words, the largest total_bill values can occur at either lunch or dinner). . . # Collect our two data sets as Numpy arrays data_tips = [] data_tips.append(tips.loc[tips.time == &#39;Lunch&#39;, [&#39;total_bill&#39;]].as_matrix()) data_tips.append(tips.loc[tips.time == &#39;Dinner&#39;, [&#39;total_bill&#39;]].as_matrix()) fig, ax = plt.subplots(figsize=(12, 8)) # Compute and normalize histograms of two data sets. h1, _ = np.histogram(data_tips[0], density=False, bins=25, range=(5, 55)) h1n = h1 / np.sum(h1) h2, be = np.histogram(data_tips[1], density=False, bins=25, range=(5, 55)) h2n = h2 / np.sum(h2) # Compute bin centers from the edges returned from np.histogram bins = be[:-1] + (be[1] - be[0])/2 # Plot the PMF difference ax.bar(bins, (h2n - h1n), alpha =0.75) ax.set_title(&quot;PMF (Dinner - Lunch)&quot;, fontsize=20) ax.set_xlim(0,52) ax.set_xlabel(&quot;Total Bill ( $)&quot;, fontsize=16) ax.axhline(0, 60, 0, linewidth=3, linestyle =&#39;-.&#39;, color=sns.xkcd_rgb[&quot;pale red&quot;], alpha = 0.75) ax.set_ylabel(&quot;Probability Difference&quot;, fontsize=16) sns.despine(offset = 5, trim=True) . . Joint Probability . The world is rarely one-dimensional. Often, we will want to understand how different features, or variables, are related or affect a particular result. In these cases, we will want to analyze the joint probability of two events occurring. Formally, the joint probability of $a$ and $b$ occurring is $P(a, b)$. This is demonstrated in the following code cells, where first compute a binned, two-dimensional representation of the total_bill and tip features from the tips data set. This joint representation is next visualized as the joint probability of a particular instance having specific values for both of these features simultaneously. . The construction of this two-dimensional histogram uses two new methods: . histogram2d, which computes the two-dimensional representation of the two one-dimensional arrays | meshgrid, which creates a grid of edges in this two-dimensional space to visualize the binned data | . . xb = np.arange(3, 51, 1) yb = np.arange(1, 11, 1) tdata, xedges, yedges = np.histogram2d(tips.total_bill, tips.tip, bins=[60, 15]) tdata /= np.sum(tdata) xe, ye = np.meshgrid(xedges, yedges) . . The previous code cell generated a two-dimensional, binned representation of our data. The following code cell first displays this two-dimensional representation, before adding custom annotations to highlight specific regions. While not frequently used, annotations, such as the circles, ext, and arrows used in the following figure, can help guide a reader to the important values. Adding these decorations to a figure is fairly straightforward in matplotlib, and essentially consists of creating the desired annotation (Circle, Text, etc.) and adding it to the figure in the desired location. After reading through this notebook, you should try changing these annotations, or adding new ones, to learn how to add, move, and change annotations on a matplotlib figure. . . from matplotlib.patches import Circle # Setup figure fig, ax = plt.subplots(figsize=(12, 6)) # We make our own heatmap, rather than use Seaborn&#39;s heatmap, since we # have a non-standard case here. Set equal aspect so we get square pixels. ax.pcolormesh(xe, ye, tdata.T) ax.set_aspect(&#39;equal&#39;) # Set labels and titles ax.set_xlabel(&#39;Total Bill&#39;, fontsize=18) ax.set_ylabel(&#39;Tip&#39;, fontsize=18) ax.set_title(&#39;Joint Probability&#39;, fontsize=20) # Set x limits, ticks, and tick labels ax.set_xticks([5, 10, 15, 20]) ax.set_xticklabels([&#39;$5&#39;, &#39;$10&#39;, &#39;$15&#39;, &#39;$20&#39;], fontsize=14) ax.set_xlim(4, 24) # Set y limits, ticks, and tick labels ax.set_yticks([2, 4, 6]) ax.set_yticklabels([&#39;$2&#39;, &#39;$4&#39;, &#39;$6&#39;], fontsize=14) ax.set_ylim(1, 6) # Annotate plot # First, we draw a circle around the rough pixel and # add annotative text and arrow ax.add_patch(Circle((10.65, 2), 0.5, color=sns.xkcd_rgb[&quot;pale red&quot;], fill=False, lw=3, alpha=0.5)) ax.annotate(r&#39;$P(x=10, y=2))$&#39;, xy=(10, 2.25), xytext=(4.5, 3.0), xycoords=&#39;data&#39;, fontsize=18, arrowprops=dict(facecolor=sns.xkcd_rgb[&quot;pale red&quot;], alpha = 0.5)) # Add second annotation ax.add_patch(Circle((15.35, 3), 0.5, color=sns.xkcd_rgb[&quot;pale red&quot;], fill=False, lw=3, alpha=0.5)) ax.annotate(r&#39;$P(x=15, y=3))$&#39;, xy=(15, 3.25), xytext=(9.5, 5.0), xycoords=&#39;data&#39;, fontsize=18, arrowprops=dict(facecolor=sns.xkcd_rgb[&quot;pale red&quot;], alpha = 0.5)) # Clean-up plot sns.despine(trim=False) # Display Joint Probability print(&#39;Joint Probability Examples n&#39;) print(&#39;P(Total Bill = $10, Tip = $2) = {:4.3f}&#39;.format(tdata[9, 1])) print(&#39;P(Total Bill = $15, Tip = $3) = {:4.3f}&#39;.format(tdata[14, 2])) . Joint Probability Examples P(Total Bill = $10, Tip = $2) = 0.033 P(Total Bill = $15, Tip = $3) = 0.008 . . Conditional Probability . We often need to calculate the probability of one variable given a second (or more) variable has a certain value or is within a certain range. This is known as conditional probability, since we are computing a probability of one variable conditioned on the value of a second variable. Formally, the conditional probability of event $a$ occurring, given the probability for a second event $b$, is written as $P(a | b)$. If two events are independent, then the conditional probability is equivalent to the individual probability of the event itself occurring. Thus, if $a$ and $b$ are independent, $P(a|b) = P(a)$. . We demonstrate conditional probability in the following two code cells. First, we once again compute and display the joint probability of total_bill and tip from the tips data set. But in this case, we also draw two sets of vertical lines. Next, we select data from the tips data set that lie within these regions, specifically, by selecting data from the total_bill column. Finally, we display histogram representations of the data from the tip column for these selected data. These histograms represent the conditional probability for the tip column, given the total_bill was within a specific range. Conditional probability is an important concept that forms the basis for Bayes theorem, which will be demonstrated later in this notebook. . . # Setup figure fig, ax = plt.subplots(figsize=(12, 6)) # We make our own heatmap, rather than use Seaborn&#39;s heatmap, since we # have a non-standard case here. Set equal aspect so we get square pixels. ax.pcolormesh(xe, ye, tdata.T) ax.set_aspect(&#39;equal&#39;) # Add first conditional region (draw parallel blue lines) low1 = 10.0 ; high1 = 12.0 ax.axvline(low1, 0, 10, c=&#39;b&#39;, alpha=0.25) ax.axvline(high1, 0, 10, c=&#39;b&#39;, alpha=0.25) # Add second conditional region (draw parallel red lines) low2 = 20.0 ; high2 = 22.0 ax.axvline(low2, 0, 10, c=&#39;r&#39;, alpha=0.25) ax.axvline(high2, 0, 10, c=&#39;r&#39;, alpha=0.25) # Label and show plot ax.set_xlabel(&#39;Total Bill&#39;, fontsize=18) ax.set_ylabel(&#39;Tip&#39;, fontsize=18) ax.set_title(&#39;Joint Probability&#39;, fontsize=20) # Clean-up plot sns.despine(trim=False) . # Generate two histograms for the conditional probability data_tips = [] data_tips.append(tips[(tips.total_bill &gt;= low2) &amp; (tips.total_bill &lt;= high2)].tip.as_matrix()) data_tips.append(tips[(tips.total_bill &gt;= low1) &amp; (tips.total_bill &lt;= high1)].tip.as_matrix()) # Define plot colors and titles clr = [sns.xkcd_rgb[&quot;pale red&quot;], sns.xkcd_rgb[&quot;denim blue&quot;]] # Define low and high ranges low = [low2, low1] high = [high2, high1] # Create figure fig, axs = plt.subplots(figsize=(10, 6), nrows=2, ncols=1, sharex=True) adj = plt.subplots_adjust(hspace=0.25) # Title plot axs[0].set_title(&#39;Conditional Probability: P(Tip | Total Bill)&#39;, ha=&#39;center&#39;, fontsize=22) # Make each subplot for idx, data in enumerate(data_tips): # Rugplot, with set rug height, thickness, and color sns.distplot(data, ax = axs[idx], bins=np.arange(1, 11, 0.25), color = clr[idx]) # No y-axis axs[idx].set_yticks([]) # Define axes limits and label axs[idx].set_xlim(0, 6) axs[idx].set_ylim(0, 1.5) axs[idx].set_xlabel(&#39;Tip ( $)&#39;, fontsize=14) #Annotate each plot with the total bill range axs[idx].annotate(&#39;Total Bill between n ${0} and ${1}&#39;.format(low[idx], high[idx]), xycoords=&#39;data&#39;, fontsize=16, xytext=(4.5, 0.6), xy=(4.5, 0.6)) # Clean up each plot sns.despine(ax=axs[idx], left=True, offset=2, trim=True) . . Marginal Probability . Given variables whose joint probability distribution is known, such as the relation demonstrated above between total_bill and tip, we can construct the marginal probability distribution by summing (or integrating for a continuous probability distribution) over the probability of one (or more) events in the joint probability distribution. Formally, given a joint probability $P(x, y)$ the marginal distribution $P(x)$ can be calculated . $$P(x) = sum_y P(x, y) = sum_y P(x | y) P(y)$$ . where we have related the joint probability $P(x, y)$ to the conditional probability $P(x | y) P(y)$. The term marginal is used to describe this probability distribution because the probability that $x$ occurs has been determined by marginalizing over the remaining variables, which are effectively discarded. . We can demonstrate the computation of a marginal probability distribution by summing over one of the axes in the two-dimensional histogram we created to display the joint probability of tip and total_bill. The following code cell demonstrates this, and also uses the edges (minus the far-right edge) determined by the NumPy histogram2d function, before plotting the one-dimensional marginal probability distributions as histograms (or specifically bars). . Note that this computation can also be understood visually by looking at the graph shown above of the joint distribution between tip and total_bill. The marginal distribution for total_bill would be found by collapsing the vertical axis of that graph, and the marginal distribution for tip would be found by collapsing the horizontal axis of that graph. . . # Create our marginal probability distributions by # summing the binned counts, recall tdata was normalized mp= [] mp.append(np.sum(tdata, axis=0)) # tips mp.append(np.sum(tdata, axis=1)) # total_bill # Build our edge array for plotting bars # (drop last one, which is far-right edge) edges = [yedges[:-1], xedges[:-1]] # Define plot colors and titles clr = [sns.xkcd_rgb[&quot;pale red&quot;], sns.xkcd_rgb[&quot;denim blue&quot;]] xlbls = [&#39;Tip&#39;, &#39;Total Bill&#39;] xmax = [12, 52] # Create figure fig, axs = plt.subplots(figsize=(10, 6), nrows=2, ncols=1, sharex=False) adj = plt.subplots_adjust(hspace=0.5) # Title plot axs[0].set_title(&#39;Marginal Probability Distributions&#39;, ha=&#39;center&#39;, fontsize=22) # Make each subplot for idx, data in enumerate(mp): # Rugplot, with set rug height, thickness, and color axs[idx].bar(edges[idx], mp[idx], width=0.5, color = clr[idx], alpha=0.5) # No y-axis axs[idx].set_yticks([]) # Define axes limits and label axs[idx].set_xlim(0, xmax[idx]) axs[idx].set_xlabel(f&#39;{xlbls[idx]} ( $)&#39;, fontsize=14) # Clean up each plot sns.despine(ax=axs[idx], left=True, offset=2, trim=True) . . Bayes Theorem . In a previous reading you learned about Bayes theorem, which relates conditional probabilities in a way that enables a hypothesis to be tested given an observed data set. This analysis starts with the fact that for two events $a$ and $b$, the conditional probabilities must be equal, thus $P(a | b) P(b) = P(b | a) P(a)$. If we express this as relation in terms of a Hypothesis $H$ and observed data $D$, the relation becomes Bayes theorem: . $$ P(H | D) = frac{P(D | H) P(H)}{P(D)}$$ . This reads, the probability of obtaining the hypothesis given the observed data is equal to the probability of the data given the hypothesis times the prior probability of observing the data normalized by the probability of the hypothesis. . In the Bayesian approach, we first construct a hypothesis, for example, that we are flipping a biased coin whose probability of obtaining heads is $P(H) = x$. Second, we encode our prior knowledge into a functional form, for example, that we have equally likely probability of getting either a heads or tails when flipping our coin. Finally, we compute our likelihood of obtaining a set of coin flips given the observed data. Rather than flipping a coin repeatedly, we will resort to simulations that generate a likelihood directly by using the binomial distribution. . We demonstrate this approach in the following code cell, where we first compute the prior probability of observing $h$ heads in $n$ coin flips for a fair coin with $P(H) = 0.5$. This value is constant, regardless of how many coin flips we might simulate since the probability is uniform. Next, we simulate coin flips by using the binomial distribution. Since we want to sample uniformly over the probability space from zero to one, we write our own binomial function (and we ignore the combinations since we will normalize the result to generate a probability). Finally, we combine the likelihood and prior to generate the posterior, which we plot. We repeat this process for increasingly large numbers of coin flips. As we do this, the likelihood swamps the prior and eventually converges strongly to the known biased value. This simple example demonstrates the importance of data in defining the posterior. . . # Bias value bc = 0.3 # We flip n coins and observe h heads, def binomial(data, n, h): # We can ignore the combination since it divides out in the normalization # Build up our probabilities p = [(x**h * (1 - x)**(n - h)) for x in data] # Normalize probabilities return (p / sum(p)) fig, ax = plt.subplots(figsize=(12, 8)) # Sample points x = np.arange(0.0, 1.0, 0.01) # Prior is fair coin, so we assume 10 flips, 5 heads. prior = binomial(x, 10, 5) ax.plot(x, prior, linewidth=3, color=&#39;g&#39;, linestyle=&#39;-&#39;, label=&#39;Prior&#39;) # Now simulate posterior for biased coin # First, 10 coin flips num_flips = 10 likelihood1 = binomial(x, num_flips, num_flips * bc) post1 = likelihood1 * prior post1 /= np.sum(post1) ax.plot(x, post1, linewidth=1, color=&#39;r&#39;, linestyle=&#39;:&#39;, label=f&#39;{{{num_flips}, {num_flips * bc:4.0f}}}&#39;) # Second, 100 coin flips num_flips = 100 likelihood2 = binomial(x, num_flips, num_flips * bc) post2 = likelihood2 * prior post2 /= np.sum(post2) ax.plot(x, post2, linewidth=1, color=&#39;b&#39;, linestyle=&#39;--&#39;, label=f&#39;{{{num_flips}, {num_flips * bc:4.0f}}}&#39;) # Third, 1000 coin flips num_flips = 1000 likelihood3 = binomial(x, num_flips, num_flips * bc) post3 = likelihood3 * prior post3 /= np.sum(post3) ax.plot(x, post3, linewidth=2, color=&#39;m&#39;, linestyle=&#39;-.&#39;, label=f&#39;{{{num_flips}, {num_flips * bc:4.0f}}}&#39;) # Decorate plot ax.set_xlabel(&quot;P(H)&quot;, fontsize=18) ax.set_ylabel(&quot;Posterior Probability&quot;, fontsize=18) ax.set_title(&quot;Simulation with {N tosses, H heads}&quot;, fontsize=20) ax.set_xlim(0, 1) ax.set_yticks([]) # Plot biased coin value ax.axvline(bc, 0, 1, c=&#39;k&#39;, linewidth=1, linestyle=&#39;-&#39;) ax.legend(loc=1, fontsize = 18) # Clean-up sns.despine(offset = 10, trim=True) . . Student Exercise . In the empty Code cell below, write and execute code to apply Bayes theorem for a loaded six-sided die. Assume the probability of rolling a three is 0.25, and that each of the remaining probabilities are uniform. Assume a uniform prior across the six numbers, and sample from the appropriate function to generate a likelihood to compute and display the prior. . . . Ancillary Information . The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional. . Statistics tutorial from Scipy | Probability chapter from the IPython Cookbook by Cyrille Rossant | Somewhat dated blog article on simple statistic analysis in Python | The probability chapter from the book Feynman Lectures on Physics by Richard Feynman | . &copy; 2017: Robert J. Brunner at the University of Illinois. . This notebook is released under the Creative Commons license CC BY-NC-SA 4.0. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder. .",
            "url": "https://tejakummarikuntla.github.io/notes/math/2020/01/23/Introduction-to-Probability.html",
            "relUrl": "/math/2020/01/23/Introduction-to-Probability.html",
            "date": " • Jan 23, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "Before Applying Log Transformation",
            "content": "Understanding and Applying Log Transformation . Log Transformation is used when events vary Drastically | . Log-scale informs on relative changes (multiplicative), while linear-scale informs on absolute changes (additive). When do you use each? When you care about relative changes, use the log-scale; when you care about absolute changes, use linear-scale. This is true for distributions, but also for any quantity or changes in quantities. . Note, I use the word &quot;care&quot; here very specifically and intentionally. Without a model or a goal, your question cannot be answered; the model or goal defines which scale is important. If you&#39;re trying to model something, and the mechanism acts via a relative change, log-scale is critical to capturing the behavior seen in your data. But if the underlying model&#39;s mechanism is additive, you&#39;ll want to use linear-scale. . ref:https://stats.stackexchange.com/questions/18844/when-and-why-should-you-take-the-log-of-a-distribution-of-numbers . | The log function maps the small range of numbers between (0, 1) to the entire range of negative numbers (–∞, 0). The function log10(x) maps the range of [1, 10] to [0, 1], [10, 100] to [1, 2], and so on. In other words, the log function compresses the range of large numbers and expands the range of small numbers. The larger x is, the slower log(x) increments. . | ref: Book :https://learning.oreilly.com/library/view/feature-engineering-for/9781491953235 | . import random import numpy as np import matplotlib.pyplot as plt from math import log import pandas as pd from sklearn import linear_model from sklearn.model_selection import cross_val_score . Using Log Transformation to convert exponentioal data to Linear . for i in range(1,20): exp.append(random.randint(1,20)) . srt_exp = sorted(exp) . np.array(srt_exp) . array([ 1, 1, 2, 2, 4, 6, 6, 7, 7, 8, 8, 8, 8, 9, 10, 10, 13, 14, 14, 14, 14, 15, 15, 15, 15, 15, 16, 17, 17, 17, 18, 19, 19, 20, 23, 23, 23, 23, 24, 24, 27, 28, 29, 31, 31, 32, 37, 38, 39, 39, 43, 48, 49, 52, 54, 58, 59, 60, 60, 60, 60, 61, 62, 63, 63, 65, 66, 66, 68, 68, 69, 70, 71, 77, 78, 78, 79, 80, 81, 87, 88, 89, 92, 93, 96, 96, 97]) . exp_10 = [(10**num) for num in srt_exp] #for num in srt_exp: # exp_10.append(10**num) . exp_10 . [10, 10, 100, 100, 10000, 10000, 10000, 10000, 100000, 100000, 100000, 100000, 1000000, 1000000, 10000000, 10000000, 100000000, 100000000, 100000000, 100000000, 1000000000, 1000000000, 10000000000, 10000000000, 10000000000, 10000000000, 100000000000, 100000000000, 1000000000000, 10000000000000, 100000000000000, 100000000000000, 100000000000000, 100000000000000, 100000000000000, 1000000000000000, 1000000000000000, 1000000000000000, 1000000000000000, 1000000000000000, 1000000000000000, 10000000000000000, 10000000000000000, 100000000000000000, 100000000000000000, 100000000000000000, 100000000000000000, 1000000000000000000, 10000000000000000000, 10000000000000000000, 10000000000000000000, 10000000000000000000, 100000000000000000000, 100000000000000000000000, 100000000000000000000000, 100000000000000000000000, 100000000000000000000000, 1000000000000000000000000, 1000000000000000000000000, 1000000000000000000000000000, 10000000000000000000000000000, 100000000000000000000000000000, 10000000000000000000000000000000, 10000000000000000000000000000000, 100000000000000000000000000000000, 10000000000000000000000000000000000000, 100000000000000000000000000000000000000, 1000000000000000000000000000000000000000, 1000000000000000000000000000000000000000, 10000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000, 10000000000000000000000000000000000000000000000000, 10000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000, 10000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000, 10000000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000000, 10000000000000000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000000000000000, 10000000000000000000000000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, 100000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, 1000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000, 10000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000] . plt.plot(exp_10) . [&lt;matplotlib.lines.Line2D at 0x1192e1828&gt;] . log_val = [log(x) for x in exp_10] . log_val_srt = sorted(log_val) . plt.plot(log_val_srt) . [&lt;matplotlib.lines.Line2D at 0x119609a90&gt;] . log values in intervel (0,1) are -ve . val_0_1 = [] for i in range(5): val_0_1.append(random.uniform(0,2)) . lg_val_0_1 = [log(x) for x in val_0_1] . plt.plot(sorted(lg_val_0_1)) . [&lt;matplotlib.lines.Line2D at 0x119b76390&gt;] . Plotting Log Curve on random data . rnd = [] for i in range(1,50): rnd.append(random.randint(1,50)) rnd = sorted(rnd) rnd . [2, 3, 5, 5, 6, 7, 8, 9, 9, 10, 13, 13, 17, 17, 18, 18, 19, 19, 21, 23, 23, 25, 26, 28, 29, 29, 30, 31, 31, 33, 34, 34, 36, 37, 38, 38, 39, 39, 39, 43, 44, 44, 46, 47, 47, 48, 49, 50, 50] . lg_rnd = [log(x) for x in rnd] plt.plot(lg_rnd) . [&lt;matplotlib.lines.Line2D at 0x119d68940&gt;] . Using log transformed word counts in the Online News Popularity dataset to predict article popularity . ref: https://www.oreilly.com/library/view/feature-engineering-for/9781491953235/ | . df = pd.read_csv(&#39;OnlineNewsPopularity.csv&#39;, delimiter=&#39;, &#39;) . /anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:1: ParserWarning: Falling back to the &#39;python&#39; engine because the &#39;c&#39; engine does not support regex separators (separators &gt; 1 char and different from &#39; s+&#39; are interpreted as regex); you can avoid this warning by specifying engine=&#39;python&#39;. &#34;&#34;&#34;Entry point for launching an IPython kernel. . df.head() . url timedelta n_tokens_title n_tokens_content n_unique_tokens n_non_stop_words n_non_stop_unique_tokens num_hrefs num_self_hrefs num_imgs ... min_positive_polarity max_positive_polarity avg_negative_polarity min_negative_polarity max_negative_polarity title_subjectivity title_sentiment_polarity abs_title_subjectivity abs_title_sentiment_polarity shares . 0 http://mashable.com/2013/01/07/amazon-instant-... | 731.0 | 12.0 | 219.0 | 0.663594 | 1.0 | 0.815385 | 4.0 | 2.0 | 1.0 | ... | 0.100000 | 0.7 | -0.350000 | -0.600 | -0.200000 | 0.500000 | -0.187500 | 0.000000 | 0.187500 | 593 | . 1 http://mashable.com/2013/01/07/ap-samsung-spon... | 731.0 | 9.0 | 255.0 | 0.604743 | 1.0 | 0.791946 | 3.0 | 1.0 | 1.0 | ... | 0.033333 | 0.7 | -0.118750 | -0.125 | -0.100000 | 0.000000 | 0.000000 | 0.500000 | 0.000000 | 711 | . 2 http://mashable.com/2013/01/07/apple-40-billio... | 731.0 | 9.0 | 211.0 | 0.575130 | 1.0 | 0.663866 | 3.0 | 1.0 | 1.0 | ... | 0.100000 | 1.0 | -0.466667 | -0.800 | -0.133333 | 0.000000 | 0.000000 | 0.500000 | 0.000000 | 1500 | . 3 http://mashable.com/2013/01/07/astronaut-notre... | 731.0 | 9.0 | 531.0 | 0.503788 | 1.0 | 0.665635 | 9.0 | 0.0 | 1.0 | ... | 0.136364 | 0.8 | -0.369697 | -0.600 | -0.166667 | 0.000000 | 0.000000 | 0.500000 | 0.000000 | 1200 | . 4 http://mashable.com/2013/01/07/att-u-verse-apps/ | 731.0 | 13.0 | 1072.0 | 0.415646 | 1.0 | 0.540890 | 19.0 | 19.0 | 20.0 | ... | 0.033333 | 1.0 | -0.220192 | -0.500 | -0.050000 | 0.454545 | 0.136364 | 0.045455 | 0.136364 | 505 | . 5 rows × 61 columns . df.n_tokens_content.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x122d6b630&gt; . df.n_tokens_content.plot(kind=&#39;kde&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11e0f76d8&gt; . Take the log transform of the &#39;n_tokens_content&#39; feature, which | represents the number of words (tokens) in a news article. | Note that we add 1 to the raw count to prevent the logarithm from | exploding into negative infinity in case the count is zero. | . df[&#39;log_n_tokens_content&#39;] = np.log10(df[&#39;n_tokens_content&#39;] + 1) . df.log_n_tokens_content.hist() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11fc9af98&gt; . df.log_n_tokens_content.plot(kind=&#39;kde&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x11fe9c668&gt; . fig, (ax1, ax2) = plt.subplots(2,1) df[&#39;n_tokens_content&#39;].hist(ax=ax1, bins=100) ax1.tick_params(labelsize=14) ax1.set_xlabel(&#39;Number of Words in Article&#39;, fontsize=14) ax1.set_ylabel(&#39;Number of Articles&#39;, fontsize=14) df[&#39;log_n_tokens_content&#39;].hist(ax=ax2, bins=100) ax2.tick_params(labelsize=14) ax2.set_xlabel(&#39;Log of Number of Words&#39;, fontsize=14) ax2.set_ylabel(&#39;Number of Articles&#39;, fontsize=14) . Text(0, 0.5, &#39;Number of Articles&#39;) . Train two linear regression models to predict the number of shares | of an article, one using the original feature and the other the | log transformed version. | . m_orig = linear_model.LinearRegression() scores_orig = cross_val_score(m_orig, df[[&#39;n_tokens_content&#39;]],df[&#39;shares&#39;], cv=10) m_log = linear_model.LinearRegression() scores_log = cross_val_score(m_log, df[[&#39;log_n_tokens_content&#39;]], df[&#39;shares&#39;], cv=10) print(&quot;R-squared score without log transform: %0.5f (+/- %0.5f)&quot; % (scores_orig.mean(), scores_orig.std() * 2)) print(&quot;R-squared score with log transform: %0.5f (+/- %0.5f)&quot; % (scores_log.mean(), scores_log.std() * 2)) . R-squared score without log transform: -0.00242 (+/- 0.00509) R-squared score with log transform: -0.00114 (+/- 0.00418) . fig2, (ax1, ax2) = plt.subplots(2,1) ax1.scatter(df[&#39;n_tokens_content&#39;], df[&#39;shares&#39;]) ax1.tick_params(labelsize=14) ax1.set_xlabel(&#39;Number of Words in Article&#39;, fontsize=14) ax1.set_ylabel(&#39;Number of Shares&#39;, fontsize=14) ax2.scatter(df[&#39;log_n_tokens_content&#39;], df[&#39;shares&#39;]) ax2.tick_params(labelsize=14) ax2.set_xlabel(&#39;Log of the Number of Words in Article&#39;, fontsize=14) ax2.set_ylabel(&#39;Number of Shares&#39;, fontsize=14) . Text(0, 0.5, &#39;Number of Shares&#39;) . Scatter plots of number of words (input) versus number of shares (target) in the Online News Popularity dataset—the top plot visualizes the original feature, and the bottom plot shows the scatter plot after log transformation . Why log(x), where x&lt;0 is not defined ? . why logarithm of -ve numbers is not defined ? . for i in range(10,-11,-1): print(&quot;2^{} = &quot;.format(i), 2**i,&quot; &quot;, &quot;log({}) = &quot;.format(2**i), log(2**i,2) ) print(&quot;2^-∞ = 0&quot;, &quot;log(0) = -∞&quot;) . 2^10 = 1024 log(1024) = 10.0 2^9 = 512 log(512) = 9.0 2^8 = 256 log(256) = 8.0 2^7 = 128 log(128) = 7.0 2^6 = 64 log(64) = 6.0 2^5 = 32 log(32) = 5.0 2^4 = 16 log(16) = 4.0 2^3 = 8 log(8) = 3.0 2^2 = 4 log(4) = 2.0 2^1 = 2 log(2) = 1.0 2^0 = 1 log(1) = 0.0 2^-1 = 0.5 log(0.5) = -1.0 2^-2 = 0.25 log(0.25) = -2.0 2^-3 = 0.125 log(0.125) = -3.0 2^-4 = 0.0625 log(0.0625) = -4.0 2^-5 = 0.03125 log(0.03125) = -5.0 2^-6 = 0.015625 log(0.015625) = -6.0 2^-7 = 0.0078125 log(0.0078125) = -7.0 2^-8 = 0.00390625 log(0.00390625) = -8.0 2^-9 = 0.001953125 log(0.001953125) = -9.0 2^-10 = 0.0009765625 log(0.0009765625) = -10.0 2^-∞ = 0 log(0) = -∞ . The values for 2^something is always +ve there&#39;s no way of occuring a -ve values, so as there&#39;s to chance of gettign a -ve value, Hence there&#39;s no way of calculating log(-ve) . Why log values between (0,1) is negative ? . From the above calculation, 2^0 starts with 1 and it goes on and it never stops as when you start cutting a cake in to two, three, four, five and so on... its goes up there is something more ahead to make in to pieces, so we say at infinity it&#39;s 0. 2^infinity it&#39;s 0 . Similarly, for the log values log(0) = -∞ and log(1) = 0. between (0,1) -&gt; log values will be (1,-∞) .",
            "url": "https://tejakummarikuntla.github.io/notes/math/2019/12/24/Log-Transformation.html",
            "relUrl": "/math/2019/12/24/Log-Transformation.html",
            "date": " • Dec 24, 2019"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Contact",
          "content": "You can find me in multiple social media platforms. . GitHub | LinkedIn | Instagram | Email | Twitter | . A few of my Writings at Blog | .",
          "url": "https://tejakummarikuntla.github.io/notes/contact/",
          "relUrl": "/contact/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tejakummarikuntla.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}