{
  
    
        "post0": {
            "title": "How Many Numbers Are Smaller Than the Current Number",
            "content": "Problem Statement . Given the array nums, for each nums[i] find out how many numbers in the array are smaller than it. That is, for each nums[i] you have to count the number of valid j&#39;s such that j != i and nums[j] &lt; nums[i]. Return the answer in an array.URL . Example 1: . Input: nums = [8,1,2,2,3] Output: [4,0,1,1,3] Explanation: For nums[0]=8 there exist four smaller numbers than it (1, 2, 2 and 3). For nums[1]=1 does not exist any smaller number than it. For nums[2]=2 there exist one smaller number than it (1). For nums[3]=2 there exist one smaller number than it (1). For nums[4]=3 there exist three smaller numbers than it (1, 2 and 2). . Example 2: . Input: nums = [6,5,4,8] Output: [2,1,0,3] . Example 3: . Input: nums = [7,7,7,7] Output: [0,0,0,0] . Constraints: . 2 &lt;= nums.length &lt;= 500 | 0 &lt;= nums[i] &lt;= 100 | . Approach 1: . Array traversal by count with Merge sort . #collapse-hide from typing import List class Solution: def smallerNumbersThanCurrent(self, nums: List[int]) -&gt; List[int]: input_list = nums.copy() if len(nums) == 0: return [] if len(nums) == 1: return [0] sorted_arr = self.merge_sort(nums) result = [] for i in range(len(input_list)): count = 0 for j in range(len(sorted_arr)): if input_list[i] == sorted_arr[j]: break else: count += 1 result.append(count) return result def merge_sort(self, arr): if(len(arr) &gt; 1): mid_val = len(arr)//2 left_list = arr[:mid_val] right_list = arr[mid_val:] self.merge_sort(left_list) self.merge_sort(right_list) i, j, k = 0, 0, 0 while(i &lt; len(left_list) and j &lt; len(right_list)): if left_list[i] &lt; right_list[j]: arr[k] = left_list[i] i += 1 else: arr[k] = right_list[j] j += 1 k += 1 while(i &lt; len(left_list)): arr[k] = left_list[i] i += 1 k += 1 while(j &lt; len(right_list)): arr[k] = right_list[j] j += 1 k += 1 return arr . . sol = Solution() sol.smallerNumbersThanCurrent([8,1,2,2,3]) . [4, 0, 1, 1, 3] . . Approach 2: . HashTable implementation with index and sorting . #collapse-hide class Solution: def smallerNumbersThanCurrent(self, nums): count = {} for i, v in enumerate(sorted(nums)): if v not in count: count[v] = i return [count[v] for v in nums] . . sol = Solution() sol.smallerNumbersThanCurrent([8,1,2,2,3]) . [4, 0, 1, 1, 3] . . Worst case performance in Time: $O(nlogn)$ . Approach 3: . considering the given constraints . 2 &lt;= nums.length &lt;= 500 | 0 &lt;= nums[i] &lt;= 100 | . . #collapse-hide class Solution: def smallerNumbersThanCurrent(self, nums): count = [0]*101 result = [0]*len(nums) for num in nums: count[num] += 1 for i in range(1, 100): count[i] += count[i-1] for ind, val in enumerate(nums): if val &gt; 0: result[ind] = count[val-1] return result . . sol = Solution() sol.smallerNumbersThanCurrent([5,0,10,0,10,6]) . [2, 0, 4, 0, 4, 3] . . Worst case performance in Time: $O(n)$ .",
            "url": "https://tejakummarikuntla.github.io/notes/problem%20solving/leetcode/2020/05/02/How-many-numbers-are-smaller-than-the-current-number.html",
            "relUrl": "/problem%20solving/leetcode/2020/05/02/How-many-numbers-are-smaller-than-the-current-number.html",
            "date": " • May 2, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Longest Common Prefix",
            "content": "Problem Statement . Write a function to find the longest common prefix string amongst an array of strings. . If there is no common prefix, return an empty string &quot;&quot;. [URL] . Example 1 . Input: [&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;] Output: &quot;fl&quot; . Example 2 . Input: [&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;] Output: &quot;&quot; Explanation: There is no common prefix among the input strings. . . Note: All given inputs are in lowercase letters a-z. . Approach 1: . Word based comparison approach . . #collapse-hide from typing import List class Solution: def longestCommonPrefix(self, strs: List[str]) -&gt; str: if len(strs) == 0: return &#39;&#39; if len(strs) == 1: return strs[0] i = 0 j = i+1 while(j &lt; len(strs)): strs[i] = Solution.lcp_of_two(self, strs[i], strs[j]) j += 1 return strs[i] def lcp_of_two(self, ele1, ele2): i, j = 0, 0 while (i &lt; len(ele1) and j &lt; len(ele2)): if (ele1[i] == ele2[j]): i += 1 j += 1 else: break lcp = ele1[:i] return lcp sol = Solution() print(sol.longestCommonPrefix([&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;])) . . fl . print(sol.longestCommonPrefix([&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;])) . fl . print(sol.longestCommonPrefix([&quot;dog&quot;,&quot;racecar&quot;,&quot;car&quot;])) . . . #collapse-hide class Solution(object): def commonPrefixUtil(self, str1, str2): len_str1 = len(str1) len_srt2 = len(str2) i = j = 0 while(i &lt; len_str1 and j &lt; len_srt2): if str1[i] != str2[j]: break i += 1 j += 1 return str1[:i] def longestCommonPrefix(self, strs): &quot;&quot;&quot; :type strs: List[str] :rtype: str &quot;&quot;&quot; if len(strs) &lt; 1: return &quot;&quot; prefix = strs[0] for i in range(1, len(strs)): prefix = self.commonPrefixUtil(prefix, strs[i]) return prefix . . sol = Solution() sol.longestCommonPrefix([&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]) . &#39;fl&#39; . . Approach 2 . Character based comparison approach . . #collapse-hide from typing import List class Solution: def longestCommonPrefix(self, strs): &quot;&quot;&quot; :type strs: List[str] :rtype: str &quot;&quot;&quot; if len(strs) == 0: return &#39;&#39; minLen = self.finMinimumLenString(strs) result = &quot;&quot; for i in range(minLen): current = strs[0][i] for j in range(1, len(strs)): if current != strs[j][i]: return result result += current return result def finMinimumLenString(self, strs): minLen = len(strs[0]) for i in range(1, len(strs)): if len(strs[i]) &lt; minLen: minLen = len(strs[i]) return minLen . . sol = Solution() sol.longestCommonPrefix([&quot;flower&quot;,&quot;flow&quot;,&quot;flight&quot;]) . &#39;fl&#39; . . Approach 3 . Character based comparison with Binary Tree implementation . 11%3 . 2 .",
            "url": "https://tejakummarikuntla.github.io/notes/problem%20solving/leetcode/2020/04/30/Longest-Common-Prefix.html",
            "relUrl": "/problem%20solving/leetcode/2020/04/30/Longest-Common-Prefix.html",
            "date": " • Apr 30, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "Comparison-based Lower Bounds for Sorting",
            "content": "The Big question . When we look back and figure out that, all the comprassion based sorting algorithms such as . Insertion Sort | Mergesort | Bubble sort | Quick sort | Selection sort etc., | . Best Case Perfomance Worst Case Perfomance . Insertion Sort | $O(n)$ comprisions, $O(1)$ swaps | $O(n^2)$ comprisions and $O(1)$ swaps | . Merge Sort | $O(n log n)$ | $O(n log n)$ | . Bubble Sort | $O(n)$ comparisons, ${ displaystyle O(1)}$ swaps | $O(n^{2})$ comparisons, ${ displaystyle O(n^{2})}$ swaps | . Quick Sort | $O(n log n)$ | $O(n^2)$ | . Selection Sort | $О(n^2)$ comparisons, $О(n)$ swaps | $O(n^2)$ comparisons, $О(n)$ swaps | . There is no comprasion based sorting algorithm with time complexity less than $O(nlogn)$ in worst case. So what would be the lower bound of the comparsion based sorting algorithms . Solving . Imagine we have three elements $a_1, a_2, a_3$ in which $a_i ne a_i$ and we are supposed to sort them in ascending order, The possible solutions would be . $a_1, a_2, a_3$ | $a_1, a_3, a_2$ | $a_2, a_1, a_3$ | $a_2, a_3, a_1$ | $a_3, a_1, a_2$ | $a_3, a_2, a_1$ | . The possible ascending order would be one among these 6. Which means the number of outcomes is $n!$.The Number of ways we can arrange n elements in $n!$ . . Note: 1. N elements give $N!$ arrangements . Let&#39;s assume an abstract approach of a comparison-based sorting algorithm using a Binary Decision tree, . . The possible arrangements of the three elements are the leaf nodes of the Binary Decision Tree. From this, we can say that the number of possible arrangements of n elements is equal to the number of leaf nodes in a binary decision tree. . Note: 2. # Leaf Nodes = # arrangements of n elements N! . Now, let&#39;s look into the hight of the tree. At each level of the tree we are making a decision, which means the height of the tree at a particular leaf will tell us the number of comparisons we made to reach that arrangement. . . Note: 3. hight of the tree = # comparisons we made . . From the Full binary tree above we can say that the number of leaf nodes of this tree is $2^{height}$ and there is no possibility that the leaf nodes can more than $2^{height}$ . Note: 3. Maximum Number of Leaf Nodes in a Binary tree = $2^{height}$ . Now, if we were given a binary decision tree for performing sorting which has $n!$[ref: Note2] leaf nodes, from the Note3, we can say that. . If h is the height of the Binary Decision tree that we are building for sorting n elements then, . $$2^{h} geq n! $$ . As the log is a monotonic function, we can apply log on both sides, . $$h ge log(n!)$$ . Now, we can calculate the h value which is the height, in return, we can figure out the comparison, from the asymptotic equations of growth of functions we know that, . $$log(n!) = theta(nlogn)$$ $$log(n!) = Omega(nlogn)$$ . Tip: Understand how $log(n!) = theta(nlogn)$ from this Video using the above equations, we can say . $$h = Omega(nlogn)$$ . Which means, the height of the binary decision tree for sorting n elements is lower bounded by $nlogn$, In which height is the number of comparisons we have to make. . From this we can say: . Note: #Comparision we have to make = $ Omega(nlogn)$ The Number of comparisions is lower bounded by $nlogn$. . . Important: From this, we can conclude that no matter of any comparison-based algorithms, we are bounded by the fact that the worst-case time complexity is lower bounded by $nlogn$. It means, we can not have a comparison-based algorithm which can perform better than $nlogn$ .",
            "url": "https://tejakummarikuntla.github.io/notes/algorithms/2020/04/28/Comparison-based-Lower-Bounds-for-Sorting.html",
            "relUrl": "/algorithms/2020/04/28/Comparison-based-Lower-Bounds-for-Sorting.html",
            "date": " • Apr 28, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Counting Sort",
            "content": "Introduction . If we stick to comparison-based sorting methods cannot do better than Ω(n log n), Comparison-based Lower Bounds for Sorting . It operates by counting the number of objects that have each distinct key values | Integer sorting algorithm: we assume the values to be integers | Using arithmetic on those counts to determine the positions of each key value in the output sequence. | It is only suitable for direct use in situations where the variation in keys is not significantly greater than the number of items | It can be used as a subroutine in radix sort | Because counting sort uses key values as indexes into an array, it is not a comparison based algorithm, so linearithmic running time can be reduced. | . Pseudo Code [CLRS] . CLRS has cooler implementation of counting sort with counters, no lists — but time bound is the same . . arr = [6, 3, 9, 10, 15, 6, 8, 12, 3, 6] . def return_max(arr): for i in range(len(arr)-1): if (arr[i] &gt; arr[i+1]): arr[i], arr[i+1] = arr[i+1], arr[i] return arr[-1] . return_max(arr) . 15 . Implementation of Counting Sort . #collapse-hide def counting_sort(arr): max_arr = return_max(arr) count_arr = [] res_arr = [] for i in range(max_arr+1): count_arr.append(0) for i in range(len(arr)): count_arr[arr[i]] += 1 i = 0 while(i &lt; len(count_arr)): if(count_arr[i] &gt; 0): res_arr.append(i) count_arr[i] -= 1 i = 0 i += 1 return res_arr . . counting_sort(arr) . [3, 3, 6, 6, 6, 8, 9, 10, 12, 15] . Implementation of Counting Sort without duplicates. . #collapse-hide def counting_sort_without_duplicates(arr): max_arr = return_max(arr) count_arr = [] res_arr = [] for i in range(max_arr+1): count_arr.append(0) for i in range(len(arr)): count_arr[arr[i]] += 1 for i in range(len(count_arr)): if count_arr[i] &gt; 0: res_arr.append(i) count_arr[i] -= 1 return res_arr . . counting_sort_without_duplicates(arr) . [3, 6, 8, 9, 10, 12, 15] . Algorithm Analysis . . . Time Complexity [Worst Case]: $O(n+k)$, where k is the range of the non-negative key values. . Space Complexcity [Worst Case]: $O(n+k)$ . In-Place: Counting sort is not an in-place algorithm as it makes use of external memory . Stable: Counting sort can be both Stable and non-stable, the above algorithm is stable . Crisp Summery: . Make assumptions about the data | Doesn&#39;t use comparisons | Counts the number of occurrences of each value | Only works with non-negative discrete values (can&#39;t work with floats, strings) | Values must be within a specific range. | O(n) can achieve this because we&#39;re making assumptions about the data we&#39;re sorting. | . Reference: . MIT 6.006 Lecture 7: Linear-Time Sorting PDF | MIT 6.006 Counting Sort PDF | MIT 6.006 Fall 2009 PDF | Learn Programming Academy WebPage | .",
            "url": "https://tejakummarikuntla.github.io/notes/algorithms/2020/04/27/Counting-Sort.html",
            "relUrl": "/algorithms/2020/04/27/Counting-Sort.html",
            "date": " • Apr 27, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "US Adult Income",
            "content": "Dataset of adult income &lt;/br&gt; DataSet Overveiw . Each row is labelled as either having a salary greater than &quot;&gt;50K&quot; or &quot;&lt;=50K&quot;. | This Data set is split into two CSV files, named adult-training.csv and adult-test.csv. &lt;/br&gt; To Build a binary classifier on the training dataset to predict the column income_bracket which has two possible values &quot;&gt;50K&quot; and &quot;&lt;=50K&quot; and evaluate the accuracy of the classifier with the test dataset. | categorical_columns = [workclass, education, marital_status, occupation, relationship, race, gender, native_country] | continuous_columns = [age, education_num, capital_gain, capital_loss, hours_per_week] | A set of reasonably clean records was extracted using the following conditions: ((AAGE&gt;16) &amp;&amp; (AGI&gt;100) &amp;&amp; (AFNLWGT&gt;1)&amp;&amp; (HRSWK&gt;0)) | . Prediction task is to determine whether a person makes over 50K a year. . Dataset Source: https://archive.ics.uci.edu/ml/datasets/census+income, http://mlr.cs.umass.edu/ml/machine-learning-databases/adult/ | . import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import numpy as np import warnings warnings.filterwarnings(&quot;ignore&quot;) %matplotlib inline train_file = &quot;adult-training.csv&quot; columns = [&#39;Age&#39;,&#39;Workclass&#39;,&#39;fnlgwt&#39;,&#39;Education&#39;,&#39;Education_num&#39;,&#39;Marital_Status&#39;, &#39;Occupation&#39;,&#39;Relationship&#39;,&#39;Race&#39;,&#39;Sex&#39;,&#39;Capital_Gain&#39;,&#39;Capital_Loss&#39;, &#39;Hours/Week&#39;,&#39;Native_country&#39;,&#39;Income&#39;] . #collapse-hide train = pd.read_csv(train_file, names=columns) train.head() . . Age Workclass fnlgwt Education Education_num Marital_Status Occupation Relationship Race Sex Capital_Gain Capital_Loss Hours/Week Native_country Income . 0 39 | State-gov | 77516 | Bachelors | 13 | Never-married | Adm-clerical | Not-in-family | White | Male | 2174 | 0 | 40 | United-States | &lt;=50K | . 1 50 | Self-emp-not-inc | 83311 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 13 | United-States | &lt;=50K | . 2 38 | Private | 215646 | HS-grad | 9 | Divorced | Handlers-cleaners | Not-in-family | White | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 3 53 | Private | 234721 | 11th | 7 | Married-civ-spouse | Handlers-cleaners | Husband | Black | Male | 0 | 0 | 40 | United-States | &lt;=50K | . 4 28 | Private | 338409 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Wife | Black | Female | 0 | 0 | 40 | Cuba | &lt;=50K | . train.info() . &lt;class &#39;pandas.core.frame.DataFrame&#39;&gt; RangeIndex: 32561 entries, 0 to 32560 Data columns (total 15 columns): # Column Non-Null Count Dtype -- -- 0 Age 32561 non-null int64 1 Workclass 32561 non-null object 2 fnlgwt 32561 non-null int64 3 Education 32561 non-null object 4 Education_num 32561 non-null int64 5 Marital_Status 32561 non-null object 6 Occupation 32561 non-null object 7 Relationship 32561 non-null object 8 Race 32561 non-null object 9 Sex 32561 non-null object 10 Capital_Gain 32561 non-null int64 11 Capital_Loss 32561 non-null int64 12 Hours/Week 32561 non-null int64 13 Native_country 32561 non-null object 14 Income 32561 non-null object dtypes: int64(6), object(9) memory usage: 3.7+ MB . train.shape . (32561, 15) . train.describe() . Age fnlgwt Education_num Capital_Gain Capital_Loss Hours/Week . count 32561.000000 | 3.256100e+04 | 32561.000000 | 32561.000000 | 32561.000000 | 32561.000000 | . mean 38.581647 | 1.897784e+05 | 10.080679 | 1077.648844 | 87.303830 | 40.437456 | . std 13.640433 | 1.055500e+05 | 2.572720 | 7385.292085 | 402.960219 | 12.347429 | . min 17.000000 | 1.228500e+04 | 1.000000 | 0.000000 | 0.000000 | 1.000000 | . 25% 28.000000 | 1.178270e+05 | 9.000000 | 0.000000 | 0.000000 | 40.000000 | . 50% 37.000000 | 1.783560e+05 | 10.000000 | 0.000000 | 0.000000 | 40.000000 | . 75% 48.000000 | 2.370510e+05 | 12.000000 | 0.000000 | 0.000000 | 45.000000 | . max 90.000000 | 1.484705e+06 | 16.000000 | 99999.000000 | 4356.000000 | 99.000000 | . # Replacing &#39;?&#39; with nan train.replace(&#39; ?&#39;, np.nan, inplace=True) . train.isnull().sum() . Age 0 Workclass 1836 fnlgwt 0 Education 0 Education_num 0 Marital_Status 0 Occupation 1843 Relationship 0 Race 0 Sex 0 Capital_Gain 0 Capital_Loss 0 Hours/Week 0 Native_country 583 Income 0 dtype: int64 . Missing Data: . Workclass(1836), Occupation(1843), Native_country(583) &lt;/br&gt; . Important: All the missing data belongs to Categorical data . train[&#39;Income&#39;].value_counts() . &lt;=50K 24720 &gt;50K 7841 Name: Income, dtype: int64 . sns.countplot(train[&#39;Income&#39;]) plt.title(&quot;Count of Income Category&quot;) plt.show() . . Warning: Dataset is Imbalenced with Majority class label &lt;=50k. - 75.91% data points labeled &lt;=50k . 24.08% data points labeled &lt;50k | . # Gender distribution sns.countplot(train[&#39;Sex&#39;]) plt.title(&quot;Count of Sex Category&quot;) plt.show() . sns.stripplot(x=&#39;Sex&#39;, y=&#39;Hours/Week&#39;, data=train,hue=&#39;Income&#39;,marker=&#39;X&#39;) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x19ac5a67ec8&gt; . # Workclass wclass_plot = sns.countplot(train[&#39;Workclass&#39;]) wclass_plot.set_xticklabels( wclass_plot.get_xticklabels(),rotation=50, ha=&quot;right&quot;) plt.title(&quot;Count Plot of Workclass&quot;) . Text(0.5, 1.0, &#39;Count Plot of Workclass&#39;) . Private class working people are overall High in count . train[&#39;Education&#39;].value_counts() . HS-grad 10501 Some-college 7291 Bachelors 5355 Masters 1723 Assoc-voc 1382 11th 1175 Assoc-acdm 1067 10th 933 7th-8th 646 Prof-school 576 9th 514 12th 433 Doctorate 413 5th-6th 333 1st-4th 168 Preschool 51 Name: Education, dtype: int64 . # Occupation occ_plot = sns.countplot(train[&#39;Occupation&#39;]) occ_plot.set_xticklabels(occ_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) plt.title(&quot;Count Plot of Occupation&quot;) . Text(0.5, 1.0, &#39;Count Plot of Occupation&#39;) . fig, axs = plt.subplots(ncols=2, nrows=4, figsize=(20, 20)) plt.subplots_adjust(hspace=0.68) fig.delaxes(axs[3][1]) fig.suptitle(&#39;Subplot of Various Categorical Variables&#39;) # Workclass wc_plot = sns.countplot(train[&#39;Workclass&#39;], ax=axs[0][0]) wc_plot.set_xticklabels(wc_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[0][0].title.set_text(&#39;Count Plot of Workclass&#39;) # Native country nc_plot = sns.countplot(train[&#39;Native_country&#39;], ax=axs[0][1]) nc_plot.set_xticklabels(nc_plot.get_xticklabels(), rotation=72, ha=&quot;right&quot;) axs[0][1].title.set_text(&#39;Count plot of Native_country&#39;) # Education ed_plot = sns.countplot(train[&#39;Education&#39;], ax=axs[1][0]) ed_plot.set_xticklabels(ed_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[1][0].title.set_text(&#39;Count Plot of Education&#39;) # Marital status ms_plot = sns.countplot(train[&#39;Marital_Status&#39;], ax=axs[1][1]) ms_plot.set_xticklabels(ms_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[1][1].title.set_text(&#39;Count Plot of Martial Status&#39;) # Relationship rel_plot = sns.countplot(train[&#39;Relationship&#39;], ax=axs[2][0]) rel_plot.set_xticklabels(rel_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[2][0].title.set_text(&#39;Count Plot of Relationship&#39;) # Race race_plot = sns.countplot(train[&#39;Race&#39;], ax=axs[2][1]) race_plot.set_xticklabels(race_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[2][1].title.set_text(&#39;Count Plot of Race&#39;) # Occupation occ_plot = sns.countplot(train[&#39;Occupation&#39;], ax=axs[3][0]) occ_plot.set_xticklabels(occ_plot.get_xticklabels(), rotation=40, ha=&quot;right&quot;) axs[3][0].title.set_text(&#39;Count Plot of Occupation&#39;) . . Note: #### Majority count aggrigation in each column: - Workclass: . - Private : 22696 . Native_country: United-States : 29170 | . | Education: Hs-grad : 10501 | . | Marital_Status: Married-civ-spouse : 14976 | . | Relationship Husband : 13193 | . | Race White : 27816 | . | Occupation: Prof-specialty : 4140 | . | . . Note: #### Minority count aggrigation in each column: - Workclass: . - Never-worked : 7 . Native_country: Holand-Netherlands : 1 | . | Education: Preschool : 51 | . | Marital_Status: Married-AF-spouse : 23 | . | Relationship Other-relative : 981 | . | Race: other : 271 | . | Occupation: Armed-Forces : 9 | . | . plt.figure(figsize=(20, 6)) sns.countplot(train[&#39;Marital_Status&#39;], hue=train[&#39;Income&#39;]) plt.title(&quot;Count Plot of Maritial Status with Hue Income&quot;) . Text(0.5, 1.0, &#39;Count Plot of Maritial Status with Hue Income&#39;) . Most of the Never Married people are under Income of &lt;=50k . plt.figure(figsize=(20, 6)) sns.countplot(train[&#39;Relationship&#39;], hue=train[&#39;Income&#39;]) plt.title(&quot;Count Plot of Relationship with Hue Income&quot;) . Text(0.5, 1.0, &#39;Count Plot of Relationship with Hue Income&#39;) . plt.figure(figsize=(20, 6)) sns.countplot(train[&#39;Age&#39;], hue=train[&#39;Income&#39;]) plt.title(&quot;Count Plot of Age with Hue Income&quot;) . Text(0.5, 1.0, &#39;Count Plot of Age with Hue Income&#39;) . sns.set_style(&quot;whitegrid&quot;) sns.pairplot(train, hue=&quot;Income&quot;, size=3) plt.show() . #collapse-hide # Age with Income sns.FacetGrid(train, hue=&quot;Income&quot;, size=6) .map(sns.distplot, &quot;Age&quot;) .add_legend(); plt.show(); # Education_num with Education_num sns.FacetGrid(train, hue=&quot;Income&quot;, size=6) .map(sns.distplot, &quot;Education_num&quot;) .add_legend(); plt.show(); # Education_num with Capital_Gain sns.FacetGrid(train, hue=&quot;Income&quot;, size=7) .map(sns.distplot, &quot;Capital_Gain&quot;) .add_legend(); plt.show(); # Education_num with Capital_Loss sns.FacetGrid(train, hue=&quot;Income&quot;, size=7) .map(sns.distplot, &quot;Capital_Loss&quot;) .add_legend(); plt.show(); # Education_num with Hours/Week sns.FacetGrid(train, hue=&quot;Income&quot;, size=7) .map(sns.distplot, &quot;Hours/Week&quot;) .add_legend(); plt.show(); . . [Report]Univariate Analysis . Dataset is Imbalenced with Majority class label &lt;=50k. . 75.91% data points labeled &lt;=50k | 24.08% data points labeled &lt;50k | . Missing Data: . Workclass(1836), Occupation(1843), Native_country(583) &lt;/br&gt; All belongs to Categorical data . Workclass Majority: Private Class, 22696 | . | Minority: Never-worked, 7 | Without-pay, 14 | Federal-gov, 960 | . | . | Native Country Majority: United-States, 29170 | . | Minority: Holand-Netherlands, 1 | Scotland, 12 | . | Missing Data: ?, 583 | . | . | Education Majority: HS-grad, 10501 | Some-college, 7291 | Bachelors, 5355 | . | Minority: Preschool, 51 | 1st-4th, 168 | 5th-6th, 333 | . | . | Martial Status Majority: Married-civ-spouse, 14976 | Never-married, 10683 | Divorced, 4443 | . | Minority: Married-AF-spouse, 23 | Married-spouse-absent, 418 | . | . | Relationship Majority: Husband, 13193 | Not-in-family, 8305 | . | Minority: Other-relative, 981 | Wife, 1568 | . | . | Race Majority: White, 27816 | Black, 3124 | . | Minority: Other, 271 | Amer-Indian-Eskimo, 311 | . | . | Occupation Majority: Prof-specialty, 4140 | Craft-repair, 4099 | Exec-managerial, 4066 | . | Minority: Armed-Forces, 9 | Priv-house-serv, 149 | Protective-serv, 649 | . | Missing Data: ?, 1843 | . | . | Majority count aggrigation in each column: . Workclass: Private : 22696 | . | Native_country: United-States : 29170 | . | Education: Hs-grad : 10501 | . | Marital_Status: Married-civ-spouse : 14976 | . | Relationship Husband : 13193 | . | Race White : 27816 | . | Occupation: Prof-specialty : 4140 | . | . Minority count aggrigation in each column: . Workclass: Never-worked : 7 | . | Native_country: Holand-Netherlands : 1 | . | Education: Preschool : 51 | . | Marital_Status: Married-AF-spouse : 23 | . | Relationship Other-relative : 981 | . | Race: other : 271 | . | Occupation: Armed-Forces : 9 | . | . train_df = pd.read_csv(&quot;adult-training.csv&quot;, names=columns) # Repalcing &#39;?&#39; to nan #train_df.replace(&#39; ?&#39;, np.nan, inplace=True) . Bivariate Analysis . Questions: . Which workclass people are earning the most? | Which level of educated people are earning the most? | Which martial category people are earning the most? | people form which occupation category are earning the most? | People form wich relation category are earning the most? | Which gender people are earning the most? | Which Race of people are earning the most? | People belongs to which Native country are earning the most? | . Income . changing Income into 0&#39;s and 1&#39;s . train_df[&#39;Income&#39;] = train[&#39;Income&#39;].apply(lambda x: 1 if x==&#39; &gt;50K&#39; else 0) . Workclass . Replaceing NaNs with 0s . train_df[&#39;Workclass&#39;].fillna(&#39; 0&#39;, inplace=True) . sns.factorplot(x=&quot;Workclass&quot;, y=&quot;Income&quot;, data=train_df, kind=&quot;bar&quot;, size = 6, palette = &quot;muted&quot;) plt.xticks(rotation=45); plt.title(&quot;Bar plot of Work Class VS Income&quot;) . Text(0.5, 1, &#39;Bar plot of Work Class VS Income&#39;) . People from Self-emp-inc are earning the most . Education . sns.factorplot(x=&quot;Education&quot;,y=&quot;Income&quot;,data=train_df,kind=&quot;bar&quot;, size = 7, palette = &quot;muted&quot;) plt.xticks(rotation=60); plt.title(&quot;Bar plot of Education VS Income&quot;) . Text(0.5, 1, &#39;Bar plot of Education VS Income&#39;) . All the Grade Education can be combined in to Primary as a single feature &lt;/br&gt; ref: https://www.kaggle.com/kost13/us-income-logistic-regression/comments . def primary(x): if x in [&#39; 1st-4th&#39;, &#39; 5th-6th&#39;, &#39; 7th-8th&#39;, &#39; 9th&#39;, &#39; 10th&#39;, &#39; 11th&#39;, &#39; 12th&#39;]: return &#39; Primary&#39; else: return x . train_df[&#39;Education&#39;] = train_df[&#39;Education&#39;].apply(primary) . sns.factorplot(x=&quot;Education&quot;, y=&quot;Income&quot;, data=train_df, kind=&quot;bar&quot;, size=7, palette=&quot;muted&quot;) plt.xticks(rotation=60); . Combinded [&#39; 1st-4th&#39;, &#39; 5th-6th&#39;, &#39; 7th-8th&#39;, &#39; 9th&#39;, &#39; 10th&#39;, &#39; 11th&#39;, &#39; 12th&#39;] to single feature Primary . Doctorates and Prof-school people has Hihger Income &gt;50k | . Education num . sns.factorplot(x=&quot;Education_num&quot;,y=&quot;Income&quot;,data=train_df,kind=&quot;bar&quot;, size = 6, palette = &quot;muted&quot;) plt.xticks(rotation=60); plt.title(&quot;Factorplot of Education VS Income&quot;) . Text(0.5, 1, &#39;Factorplot of Education VS Income&#39;) . Relation Higher the Education_num give better Income . Martial Status . sns.factorplot(x=&quot;Marital_Status&quot;,y=&quot;Income&quot;,data=train_df,kind=&quot;bar&quot;, size = 5, palette = &quot;muted&quot;) plt.xticks(rotation=60); print(train_df[&#39;Marital_Status&#39;].value_counts()) plt.title(&quot;Factor plot of Martial Status VS Income&quot;) . Married-civ-spouse 14976 Never-married 10683 Divorced 4443 Separated 1025 Widowed 993 Married-spouse-absent 418 Married-AF-spouse 23 Name: Marital_Status, dtype: int64 . Text(0.5, 1, &#39;Factor plot of Martial Status VS Income&#39;) . People belonging to Married-civ-spouse are earning the most. . Occupation . #filing NaNs in Occupation with 0 train_df[&#39;Occupation&#39;].replace(&#39; ?&#39;, &#39; 0&#39;, inplace=True) . train_df[&#39;Occupation&#39;].value_counts() . Prof-specialty 4140 Craft-repair 4099 Exec-managerial 4066 Adm-clerical 3770 Sales 3650 Other-service 3295 Machine-op-inspct 2002 0 1843 Transport-moving 1597 Handlers-cleaners 1370 Farming-fishing 994 Tech-support 928 Protective-serv 649 Priv-house-serv 149 Armed-Forces 9 Name: Occupation, dtype: int64 . sns.factorplot(x=&quot;Occupation&quot;,y=&quot;Income&quot;,data=train_df,kind=&quot;bar&quot;, size = 8, palette = &quot;muted&quot;) plt.xticks(rotation=60); plt.title(&quot;Factor plot of Occupation VS Income&quot;) . Text(0.5, 1, &#39;Factor plot of Occupation VS Income&#39;) . people belonging to Exec-managerial occupation are earning the most . Relationship . sns.factorplot(x=&quot;Relationship&quot;, y=&quot;Income&quot;, data=train_df, size=5, kind=&quot;bar&quot;, palette=&quot;muted&quot;) plt.xticks(rotation=60) plt.title(&quot;Factorplot of Relationship vs Income&quot;) print(train_df[&#39;Relationship&#39;].value_counts()) . Husband 13193 Not-in-family 8305 Own-child 5068 Unmarried 3446 Wife 1568 Other-relative 981 Name: Relationship, dtype: int64 . People belonging to wife category of relationship are earning the most . Race . sns.factorplot(x=&quot;Race&quot;, y=&quot;Income&quot;, data=train_df, size=5, kind=&quot;bar&quot;, palette=&quot;muted&quot;) plt.xticks(rotation=60) plt.title(&quot;Factorplot of Race VS Income&quot;) print(train_df[&#39;Race&#39;].value_counts()) . White 27816 Black 3124 Asian-Pac-Islander 1039 Amer-Indian-Eskimo 311 Other 271 Name: Race, dtype: int64 . People belonging to Asian-Pac-Islander are earning the most in Race . sex . sns.factorplot(x=&quot;Sex&quot;, y=&quot;Income&quot;, data=train_df,size=5,kind=&quot;bar&quot;, palette=&quot;muted&quot;) plt.xticks(rotation=60) plt.title(&quot;Factorplot of Sex VS Income&quot;) print(train_df[&#39;Sex&#39;].value_counts()) . Male 21790 Female 10771 Name: Sex, dtype: int64 . Male gender are earning the most . Native country . There Exist 583 Unknown values replacing with 0 . train_df[&#39;Native_country&#39;].replace(&#39; ?&#39;, &#39; 0&#39;, inplace=True) . #collapse-hide sns.factorplot(x=&quot;Native_country&quot;, y=&quot;Income&quot;, data=train_df,size=13,kind=&quot;bar&quot;, palette=&quot;muted&quot;) plt.xticks(rotation=80) print(train_df[&#39;Native_country&#39;].value_counts()) . . United-States 29170 Mexico 643 0 583 Philippines 198 Germany 137 Canada 121 Puerto-Rico 114 El-Salvador 106 India 100 Cuba 95 England 90 Jamaica 81 South 80 China 75 Italy 73 Dominican-Republic 70 Vietnam 67 Guatemala 64 Japan 62 Poland 60 Columbia 59 Taiwan 51 Haiti 44 Iran 43 Portugal 37 Nicaragua 34 Peru 31 France 29 Greece 29 Ecuador 28 Ireland 24 Hong 20 Cambodia 19 Trinadad&amp;Tobago 19 Thailand 18 Laos 18 Yugoslavia 16 Outlying-US(Guam-USVI-etc) 14 Hungary 13 Honduras 13 Scotland 12 Holand-Netherlands 1 Name: Native_country, dtype: int64 . train_df.columns . Index([&#39;Age&#39;, &#39;Workclass&#39;, &#39;fnlgwt&#39;, &#39;Education&#39;, &#39;Education_num&#39;, &#39;Marital_Status&#39;, &#39;Occupation&#39;, &#39;Relationship&#39;, &#39;Race&#39;, &#39;Sex&#39;, &#39;Capital_Gain&#39;, &#39;Capital_Loss&#39;, &#39;Hours/Week&#39;, &#39;Native_country&#39;, &#39;Income&#39;], dtype=&#39;object&#39;) . train_df[&#39;Native_country&#39;].value_counts() . United-States 29170 Mexico 643 0 583 Philippines 198 Germany 137 Canada 121 Puerto-Rico 114 El-Salvador 106 India 100 Cuba 95 England 90 Jamaica 81 South 80 China 75 Italy 73 Dominican-Republic 70 Vietnam 67 Guatemala 64 Japan 62 Poland 60 Columbia 59 Taiwan 51 Haiti 44 Iran 43 Portugal 37 Nicaragua 34 Peru 31 France 29 Greece 29 Ecuador 28 Ireland 24 Hong 20 Cambodia 19 Trinadad&amp;Tobago 19 Thailand 18 Laos 18 Yugoslavia 16 Outlying-US(Guam-USVI-etc) 14 Hungary 13 Honduras 13 Scotland 12 Holand-Netherlands 1 Name: Native_country, dtype: int64 . People from Iran are earning the most . colormap = plt.cm.magma plt.figure(figsize=(16,16)) plt.title(&#39;Pearson Correlation of Features&#39;, y=1.05, size=15) sns.heatmap(train_df.corr(),linewidths=0.1,vmax=1.0, square=True, cmap=colormap, linecolor=&#39;white&#39;, annot=True) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fbe9e5fe080&gt; . [Bivariate][Report] Answers with perfomed bivariate analysis . Which workclass people are earning the most? Self-emp-inc | . | Which level of educated people are earning the most? Doctorates and Prof-school | . | Which martial category people are earning the most? Married-civ-spouse | . | people form which occupation category are earning the most? Exec-managerial | . | People form wich relation category are earning the most? Wife | . | Which gender people are earning the most? Men | . | Which Race of people are earning the most? Asian-Pac-Islander | . | People belongs to which Native country are earning the most? Iran | . | . Mulitvariate Analysis, pivoting . Questions: . Specific Counts of each in different workclass belongs to various education on Income basis | Specific Counts of each in different workclass belongs to various education on Gender basis | . train_mult_index = train_df.set_index(keys = [&#39;Income&#39;,&#39;Education&#39;,&#39;Native_country&#39;]).sort_index() . train_mult_index.tail() . Age Workclass fnlgwt Education_num Marital_Status Occupation Relationship Race Sex Capital_Gain Capital_Loss Hours/Week . Income Education Native_country . 1 Some-college United-States 30 | Self-emp-not-inc | 176185 | 10 | Married-spouse-absent | Craft-repair | Own-child | White | Male | 0 | 0 | 60 | . United-States 53 | Private | 304504 | 10 | Married-civ-spouse | Transport-moving | Husband | White | Male | 0 | 1887 | 45 | . United-States 46 | Private | 42251 | 10 | Married-civ-spouse | Sales | Husband | White | Male | 0 | 0 | 45 | . United-States 46 | Private | 364548 | 10 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 48 | . Yugoslavia 36 | Self-emp-inc | 337778 | 10 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 60 | . train_mult_index.loc[(1, &quot; Primary&quot;, &quot; United-States&quot;),].count()[0] . 202 . # People having Income &gt;50K with Primary Education in United-Sates: 202 . train_mult_index.stack().to_frame() . 0 . Income Education Native_country . 0 Assoc-acdm ? Age 42 | . Workclass Self-emp-not-inc | . fnlgwt 183765 | . Education_num 12 | . Marital_Status Married-civ-spouse | . ... ... ... ... ... | . 1 Some-college Yugoslavia Race White | . Sex Male | . Capital_Gain 0 | . Capital_Loss 0 | . Hours/Week 60 | . 390732 rows × 1 columns . #collapse-hide train_df . . Age Workclass fnlgwt Education Education_num Marital_Status Occupation Relationship Race Sex Capital_Gain Capital_Loss Hours/Week Native_country Income . 0 39 | State-gov | 77516 | Bachelors | 13 | Never-married | Adm-clerical | Not-in-family | White | Male | 2174 | 0 | 40 | United-States | 0 | . 1 50 | Self-emp-not-inc | 83311 | Bachelors | 13 | Married-civ-spouse | Exec-managerial | Husband | White | Male | 0 | 0 | 13 | United-States | 0 | . 2 38 | Private | 215646 | HS-grad | 9 | Divorced | Handlers-cleaners | Not-in-family | White | Male | 0 | 0 | 40 | United-States | 0 | . 3 53 | Private | 234721 | Primary | 7 | Married-civ-spouse | Handlers-cleaners | Husband | Black | Male | 0 | 0 | 40 | United-States | 0 | . 4 28 | Private | 338409 | Bachelors | 13 | Married-civ-spouse | Prof-specialty | Wife | Black | Female | 0 | 0 | 40 | Cuba | 0 | . ... ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | . 32556 27 | Private | 257302 | Assoc-acdm | 12 | Married-civ-spouse | Tech-support | Wife | White | Female | 0 | 0 | 38 | United-States | 0 | . 32557 40 | Private | 154374 | HS-grad | 9 | Married-civ-spouse | Machine-op-inspct | Husband | White | Male | 0 | 0 | 40 | United-States | 1 | . 32558 58 | Private | 151910 | HS-grad | 9 | Widowed | Adm-clerical | Unmarried | White | Female | 0 | 0 | 40 | United-States | 0 | . 32559 22 | Private | 201490 | HS-grad | 9 | Never-married | Adm-clerical | Own-child | White | Male | 0 | 0 | 20 | United-States | 0 | . 32560 52 | Self-emp-inc | 287927 | HS-grad | 9 | Married-civ-spouse | Exec-managerial | Wife | White | Female | 15024 | 0 | 40 | United-States | 1 | . 32561 rows × 15 columns . iec_data = train_df.loc[:,(&quot;Income&quot;, &quot;Education&quot;, &quot;Workclass&quot;)] . iec_data . Income Education Workclass . 0 0 | Bachelors | State-gov | . 1 0 | Bachelors | Self-emp-not-inc | . 2 0 | HS-grad | Private | . 3 0 | Primary | Private | . 4 0 | Bachelors | Private | . ... ... | ... | ... | . 32556 0 | Assoc-acdm | Private | . 32557 1 | HS-grad | Private | . 32558 0 | HS-grad | Private | . 32559 0 | HS-grad | Private | . 32560 1 | HS-grad | Self-emp-inc | . 32561 rows × 3 columns . iec_data.pivot_table(values=&#39;Income&#39;, index=&#39;Education&#39;, aggfunc=&#39;count&#39;, margins_name=&#39;Income&#39;) . Income . Education . Assoc-acdm 1067 | . Assoc-voc 1382 | . Bachelors 5355 | . Doctorate 413 | . HS-grad 10501 | . Masters 1723 | . Preschool 51 | . Primary 4202 | . Prof-school 576 | . Some-college 7291 | . iec_data[iec_data.Income == 1].pivot_table(values=&#39;Income&#39;, index=&#39;Education&#39;, aggfunc=&#39;count&#39;, margins_name=&#39;Income&#39;) . Income . Education . Assoc-acdm 265 | . Assoc-voc 361 | . Bachelors 2221 | . Doctorate 306 | . HS-grad 1675 | . Masters 959 | . Primary 244 | . Prof-school 423 | . Some-college 1387 | . #collapse-hide iec_data_pivot = iec_data[iec_data.Income == 1].pivot_table(values=&#39;Income&#39;, index=&#39;Education&#39;, aggfunc=&#39;count&#39;, margins_name=&#39;Income&#39;) plt.figure(figsize=(16, 8)) sns.heatmap(iec_data_pivot, annot=True, fmt=&#39;.1f&#39;, cbar_kws= {&#39;label&#39;:&#39;Income range in categories&#39;}, cmap=&#39;coolwarm&#39;) plt.title(&#39;Incomes of various educated categories in Income wise&#39;) . . Text(0.5, 1, &#39;Incomes of various educated categories in Income wise&#39;) . #collapse-hide train_df[train_df.Income == 1].pivot_table(values=&#39;Income&#39;, index=[&#39;Native_country&#39;, &#39;Education&#39;], aggfunc=&#39;count&#39;) . . Income . Native_country Education . 0 Assoc-acdm 3 | . Assoc-voc 4 | . Bachelors 52 | . Doctorate 15 | . HS-grad 13 | . Masters 23 | . Primary 10 | . Prof-school 9 | . Some-college 17 | . Cambodia Bachelors 2 | . HS-grad 2 | . Primary 1 | . Some-college 2 | . Canada Assoc-acdm 1 | . Assoc-voc 3 | . Bachelors 9 | . Doctorate 4 | . HS-grad 8 | . Masters 3 | . Primary 1 | . Prof-school 1 | . Some-college 9 | . China Bachelors 8 | . Doctorate 5 | . HS-grad 3 | . Masters 4 | . Columbia Doctorate 1 | . Prof-school 1 | . Cuba Bachelors 4 | . Doctorate 1 | . ... ... ... | . South Prof-school 1 | . Some-college 3 | . Taiwan Bachelors 3 | . Doctorate 7 | . HS-grad 1 | . Masters 6 | . Prof-school 3 | . Thailand Assoc-acdm 1 | . Doctorate 1 | . Some-college 1 | . Trinadad&amp;Tobago HS-grad 1 | . Primary 1 | . United-States Assoc-acdm 247 | . Assoc-voc 336 | . Bachelors 2016 | . Doctorate 249 | . HS-grad 1583 | . Masters 866 | . Primary 202 | . Prof-school 374 | . Some-college 1298 | . Vietnam Bachelors 1 | . Doctorate 1 | . HS-grad 1 | . Primary 2 | . Yugoslavia Assoc-acdm 1 | . Bachelors 2 | . HS-grad 1 | . Primary 1 | . Some-college 1 | . 191 rows × 1 columns . gen_in_df = train_df.where(train_df.Income == 1).pivot_table(values=[&#39;Income&#39;], index=&#39;Education&#39;, columns=&#39;Workclass&#39;, aggfunc=&#39;count&#39;) . #collapse-hide gen_in_df.sort_index() . . Income . Workclass ? Federal-gov Local-gov Private Self-emp-inc Self-emp-not-inc State-gov . Education . Assoc-acdm 6 | 19 | 28 | 170 | 18 | 18 | 6 | . Assoc-voc 13 | 15 | 25 | 256 | 19 | 21 | 12 | . Bachelors 45 | 95 | 162 | 1495 | 171 | 163 | 90 | . Doctorate 11 | 15 | 17 | 132 | 29 | 31 | 71 | . HS-grad 46 | 73 | 90 | 1119 | 119 | 179 | 49 | . Masters 18 | 47 | 173 | 534 | 57 | 59 | 71 | . Primary 9 | 2 | 10 | 163 | 15 | 40 | 5 | . Prof-school 8 | 23 | 19 | 171 | 78 | 106 | 18 | . Some-college 35 | 82 | 93 | 923 | 116 | 107 | 31 | . plt.figure(figsize=(16, 8)) sns.heatmap(gen_in_df.sort_index(), annot=True, fmt=&#39;.1f&#39;, cbar_kws= {&#39;label&#39;:&#39;Income range in categories&#39;}, cmap=&#39;coolwarm&#39;) plt.title(&#39;Incomes of various educated categories in Income wise&#39;) . Text(0.5, 1, &#39;Incomes of various educated categories in Income wise&#39;) . Bachelors of Education field in Private Worclass are 1495.0 Income count . gen_sex_df = train_df.where(train_df.Income == 1).pivot_table(values=[&#39;Sex&#39;], index=&#39;Education&#39;, columns=&#39;Workclass&#39;, aggfunc=&#39;count&#39;) . gen_sex_df . Sex . Workclass ? Federal-gov Local-gov Private Self-emp-inc Self-emp-not-inc State-gov . Education . Assoc-acdm 6 | 19 | 28 | 170 | 18 | 18 | 6 | . Assoc-voc 13 | 15 | 25 | 256 | 19 | 21 | 12 | . Bachelors 45 | 95 | 162 | 1495 | 171 | 163 | 90 | . Doctorate 11 | 15 | 17 | 132 | 29 | 31 | 71 | . HS-grad 46 | 73 | 90 | 1119 | 119 | 179 | 49 | . Masters 18 | 47 | 173 | 534 | 57 | 59 | 71 | . Primary 9 | 2 | 10 | 163 | 15 | 40 | 5 | . Prof-school 8 | 23 | 19 | 171 | 78 | 106 | 18 | . Some-college 35 | 82 | 93 | 923 | 116 | 107 | 31 | . #collapse-hide plt.figure(figsize=(16, 8)) sns.heatmap(gen_sex_df, annot=True, fmt=&#39;.1f&#39;, cbar_kws= {&#39;label&#39;:&#39;Income range in categories&#39;}, cmap=&#39;coolwarm&#39;) plt.title(&#39;Incomes of various educated categories in Gender wise&#39;) . . Text(0.5, 1, &#39;Incomes of various educated categories in Gender wise&#39;) . Bachelors of Education field in Private Worclass are in marjority of Gender count basis . train_df.Sex.value_counts() . Male 21790 Female 10771 Name: Sex, dtype: int64 . gen_in_df.index.names . FrozenList([&#39;Education&#39;]) . gen_in_df.loc[:,&#39;Sex&#39;] . Workclass ? Federal-gov Local-gov Private Self-emp-inc Self-emp-not-inc State-gov . Education . Assoc-acdm 6 | 19 | 28 | 170 | 18 | 18 | 6 | . Assoc-voc 13 | 15 | 25 | 256 | 19 | 21 | 12 | . Bachelors 45 | 95 | 162 | 1495 | 171 | 163 | 90 | . Doctorate 11 | 15 | 17 | 132 | 29 | 31 | 71 | . HS-grad 46 | 73 | 90 | 1119 | 119 | 179 | 49 | . Masters 18 | 47 | 173 | 534 | 57 | 59 | 71 | . Primary 9 | 2 | 10 | 163 | 15 | 40 | 5 | . Prof-school 8 | 23 | 19 | 171 | 78 | 106 | 18 | . Some-college 35 | 82 | 93 | 923 | 116 | 107 | 31 | . [Report] Multivaiate Analysis . Specific Counts of each in different workclass belongs to various education on Income basis Bachelors of Education field in Private Worclass are 1495.0 Income count | . | Specific Counts of each in different workclass belongs to various education on Gender basis Bachelors of Education field in Private Worclass are in marjority of Gender count basis | . | .",
            "url": "https://tejakummarikuntla.github.io/notes/eda/2020/04/23/EDA-US-Salary-Dataset.html",
            "relUrl": "/eda/2020/04/23/EDA-US-Salary-Dataset.html",
            "date": " • Apr 23, 2020"
        }
        
    
  
    
        ,"post5": {
            "title": "Introduction to Probability Distributions",
            "content": "Introduction to Distributions . - ref: https://www.coursera.org/lecture/data-analytics-accountancy-1/introduction-to-descriptive-statistics-WDaaH . Given a data set, an individual feature, or groups of features, can be converted into an empirical probability density function. An empirical density function can provide important insight into a data set, such as measures of a typical value, the variability around this typical value, and the shape of the data distribution. Often, however, we wish to understand some parent data set, from which our empirical data are sampled. In these cases, we often resort to theoretical distributions, which can provide invaluable guidance based on either theoretical expectations or previous applications. Theoretical distributions also can be used as generative models to create new data samples for subsequent analysis. In addition, theoretical distributions often are based on physical insights that can provide deeper insight into either our data or the process by which the data are acquired. . Theoretical Distributions . If you wish to delve deeply into probability theory, a good place to start is the Wikipedia discussion on probability theory. For this course, however, we can assume, without loss of generality, that theoretical distributions come in two varieties: . discrete distributions, which are restricted to integer values, and | continuous distributions, which are restricted to real values. | While we could roll our own Python code for these distributions, there are several standard implementations for the standard theoretical distributions. We will use the best supported version that is developed and maintained by the SciPy community within the [scipy.stats][ss] module. To use any of the available distributions, we will import them directly. For example, to use the uniform distribution (talked about in the next section), we simply import the distribution directly. . from scipy.stats import uniform . Given a theoretical distribution, we can create and use samples of random variables or sample the probability density function. Once imported, there are two ways to work with a distribution: . Create the distribution once, by creating the distribution with the desired parameters. Afterwards, this frozen distribution’s (the name refers to the fact the distribution&#39;s parameters are fixed, or frozen solid) functions are called as needed on the already created distribution. | Create the distribution each time it is needed (for instance to create random samples) by specifying the distribution’s parameters every time. | We will demonstrate both approaches in this notebook. However, of these two techniques, the easiest is the first technique where the distribution is created once from fixed parameters. This also provides a potential performance boost since the distribution is effectively only created once, and subsequently reused as needed. Before we proceed with a discussion on how to use theoretical distribution functions in Python, we perform our standard imports. . . %matplotlib inline # Standard imports import numpy as np import scipy as sp import pandas as pd import scipy.stats as stats import matplotlib.pyplot as plt import seaborn as sns # We do this to ignore several specific Pandas warnings import warnings warnings.filterwarnings(&quot;ignore&quot;) sns.set(style=&quot;white&quot;) # Global color definition g_colors = [sns.xkcd_rgb[&#39;pale red&#39;], sns.xkcd_rgb[&#39;denim blue&#39;], sns.xkcd_rgb[&#39;bluegrey&#39;], sns.xkcd_rgb[&#39;faded green&#39;], sns.xkcd_rgb[&#39;dusty purple&#39;]] . . Uniform . The simplest theoretical distribution is the Uniform distribution, which has probability uniformly distributed between two end points. A Uniform distribution can be either discrete, being defined only for integer values, or continuous, being defined over all real values. In the latter case, we have a probability density function $f(x)$, which for $a leq x leq b$ is defined: . $$f(x) = frac{1}{(b - a)} $$ . with probability zero everywhere else. . To create a discrete uniform distribution, we use the randint function from the SciPy module. We first create our distribution by passing in the two end points, after which we can apply different functions to perform specific operations. The result of this operation is the creation of a frozen distribution, which allows different statistical functions to be easily applied to the created distribution without specifying the distribution parameters repeatedly (e.g., the end points of the Uniform distribution). . The primary function we apply to a frozen distribution is the rvs function, which generates a sample of random values from a distribution. For example, to generate a set of ten integers from a discrete Uniform distribution between five and ten: . udrv = randint(5, 10) ddata = udrv.rvs(1000) . Note that since this is a random process, a different result will be produced each time a set of random variables is drawn from this distribution. Thus, one possible output from this code would be for the discrete data variable ddata to be array([5, 6, 9, 7, 5, 8, 5, 9, 6, 9]), which demonstrates that the first value, 5, is included in the output, but the second value, 10, is not included. . For a continuous Uniform distribution, the process is similar, but we instead use the uniform function from the SciPy module. For example, to draw ten, real valued random numbers from a Uniform distribution, we create a uniform object before sampling from this frozen distribution to obtain random data: . ucrv = uniform(5, 10) cdata = ucrv.rvs(10) . Once again, the continuous data cdata is randomly drawn, but is now composed of real values, for example: . array([ 13.36813407, 8.40738799, 10.52660647, 9.83945343, 10.5271254 , 13.54307454, 11.77493877, 11.3713698 , 8.36527195, 7.58982897]) . The SciPy distribution objects provide a rich interface that can be used to obtain statistics, including the mean, standard deviation, or median, as well as summary distributions. One of the most important is the probability mass (or PMF for discrete distributions) or density (or PDF for continuous distributions) function. Once a distribution object has been created, we can sample the PMF (or PDF) by simply calling the pmf (or pdf) function with different values for the random variable of interest. For example, if we wish to sample the PDF of the uniform distribution between zero and one, we can sample the pdf function by using a linearly spaced set of values for x. . # Create distribution ucrv = uniform(0, 1) # Create sample points x = np.linspace(0, 1) # Construct PDF probs = ucrv.pdf(x) . The following code cell demonstrates this approach for both the discrete and continuous uniform distributions, where the probability is sampled over the range from zero to ten. . . from scipy.stats import randint, uniform # Set region of interest low = 1 ; high = 10 # Define frozen distributions # For discrete the format is a distribution [low, high) udrv = randint(low, high) # For continuous, the format is distribution [low, (high -low)) ucrv = uniform(low, high - low) # Function to plot discrete distribution def plot_pmf(axs, x, low, high, dist, froz): # Plot distribution PMF axs.plot(x, dist.pmf(x, low, high), &#39;o&#39;, ms=10, color=g_colors[0], label=f&#39;Discrete PMF&#39;) # Plot frozen PMF axs.vlines(x, 0, froz.pmf(x), lw=2, alpha=0.5, colors=g_colors[1], label=&#39;Frozen PMF&#39;) # Function to plot continuous distribution def plot_pdf(axs, x, low, high, dist, froz): # Plot distribution PMF axs.plot(x, dist.pdf(x, low, high - low), lw=7, alpha = 0.5, color=g_colors[0], label=f&#39;Contiuous PMF&#39;) # Plot frozen PMF axs.plot(x, froz.pdf(x), lw=2, alpha=0.75, color=g_colors[1], label=&#39;Frozen PMF&#39;) # Plot binned counts data = froz.rvs(1000) axs.hist(data, normed=True, histtype=&#39;step&#39;, lw=4, alpha=0.25, color=g_colors[2], label=&#39;Binned Counts&#39;) # Function to make overall plot, leaves distribution plotting # to one of the two helper plot functions def make_plot(axs, low, high, dist, froz, kind): # Decide whch helper function to call if kind == &#39;Discrete&#39;: # Sampling range x = np.arange(low, high) plot_pmf(axs, x, low, high, dist, froz) elif kind ==&#39;Continuous&#39;: # Sampling range x = np.linspace(low, high) plot_pdf(axs, x, low, high, dist, froz) # We scale the y-axis to allow room for the legend axs.set_ylim(0, 1.75 * 1.0/(high - low)) # Decorate plot axs.set_xticks(np.arange(low, high + 1)) axs.set_xlabel(&#39;X&#39;, fontsize=16) axs.set_ylabel(r&#39;P(X)&#39;, fontsize=16) axs.set_title(f&#39;{kind} Uniform Distribution: [{low}, {high})&#39;, fontsize=16) axs.legend(loc=1, fontsize=14) sns.despine(ax=axs, offset = 5, trim=True) # Now plot the binomial distribution for different # numbers of heads in 5 coin flips fig, axs = plt.subplots(figsize=(15, 6), nrows=1, ncols=2, sharey=True) # Call our plotting routine for the different distributions make_plot(axs[0], low, high, randint, udrv, &#39;Discrete&#39;) make_plot(axs[1], low, high, uniform, ucrv, &#39;Continuous&#39;) . . Poisson Distribution . One of the most useful discrete distribution functions is the Poisson distribution, which expresses the probability that a given number of events will occur in a fixed interval of time or space. For example, the Poisson distribution is used to model the number goals that might be scored in a soccer match, the number of floods that occur on a particular river, or the number of geographically diverse group of people that visit an online discussion forum. Formally, the following requirements must be met for the Poisson distribution to be valid: . If $X$ represents the number of events that can occur in the defined interval, then $X$ can only be either zero, or a positive integer. | The events are independent, thus one event occurring does not impact any of the other events. | The rate at which events occurs is constant. | Two events can not occur simultaneously. | The probability an event occurs within a small interval is proportional to the length of the interval or the actual probability is determined by a Binomial distribution where the number of trials is much larger than the number of successes. | When these conditions are true, given a positive, real-valued rate parameter $ mu$, we have the probability of observing $X$ events in a Poisson process: . $$ P(X) = frac{e^{- mu} mu^X}{X!} $$ . The Poisson distribution has mean and variance equal to the rate parameter, $ mu$. The Poisson distribution can be derived as a special case of the Binomial distribution where the number of trials goes to infinity, while the number of successes remains fixed, a result known as the Law of Rare Events. . To work with this distribution in Python, we will create the poisson distribution by specifying the rate parameter to create a frozen distribution: . # Create frozen Poisson Discrete Distribution (pdd) mu = 2.5 pdd = poisson(mu) . The following code cell demonstrates how to use the Poisson distribution within a Python script, where we create and visually compare three different distributions, for three different rate parameters. For simplicity, we do not create frozen distributions in this example, and instead create and sample the probability mass function for each distribution as needed. . . from scipy.stats import poisson # Number of points to sample for each distribution num_pts = 100 # Poisson distribution parameter definition mu = [1., 3., 5.] # Local label definition labels = [&#39;$ mu = 1$&#39;, &#39;$ mu = 3$&#39;, &#39;$ mu = 5$&#39;] # Sampling points x = np.arange(2.5 * mu[-1]) fig, ax = plt.subplots(figsize=(10, 6)) # Plot each distribution PMF, offset them by delta to give clarity delta = 0.25 plt.bar(x - 1.25 * delta, poisson.pmf(x, mu[0]), width=delta, color=g_colors[2], label = labels[0]) plt.bar(x, poisson.pmf(x, mu[1]), width=delta, color=g_colors[3], label = labels[1]) plt.bar(x + 1.25 * delta, poisson.pmf(x, mu[2]), width=delta, color=g_colors[4], label = labels[2]) # Decorate plot plt.xticks(x) plt.xlabel(r&quot;$X$&quot;, fontsize=18) plt.ylabel(r&quot;$P(X| mu)$&quot;, fontsize=18) plt.title(&quot;Poisson PMF&quot;, fontsize=20) plt.legend(fontsize=16) sns.despine(trim=True, offset = 5) . . Other Discrete Distributions . In addition to the Binomial, Uniform, and Poisson distributions, there are a number of other discrete theoretical distributions that prove useful on occasion, including the . Boltzmann, | Geometric, | Hypergeometric, and | Zipf distributions. | . While we will not review these (or other discrete) distributions in this notebook, you should be aware of their existence in case they prove useful in a future analytics task. . . Normal Distribution . Perhaps the most common distribution is the Normal (or Gaussian) distribution, which occurs frequently in nature and is also frequently seen as a result of the Central Limit Theorem, which will be discussed later in this notebook. The Normal distribution is often referred to as the bell curve since it resembles a bell. This distribution can be used to represent the heights of a group of people errors in measurements, health measurements such as blood pressure, grades on a test, or the measurements of items manufactured by the same machine. . Given a mean value, $ mu$, and standard deviation, $ sigma$, the probability density function for a Normal distribution is given by . $$ P(X) = frac{1}{ sqrt{2 pi sigma}} exp^{ frac{-(X - mu)^2}{2 sigma^2}} $$ . Note that the median, mode, and mean values all coincide for a Normal distribution, while the standard deviation measures how wide the distribution is around the central value. Given the ubiquity of the Normal distribution, you likely should memorize the percent of all data in a Normal distribution enclosed by $X$ standard deviations (i.e., $ mu pm X sigma$): . $X sigma$ Percent of data enclosed Interpretation . 1 | 68% | Data likely to be contained with this interval | . 2 | 95% | Data very likely to be contained with this interval | . 3 | 99.7% | Data almost certainly to be contained with this interval | . An alternative form, known as the standard normal distribution, has zero mean and unit standard deviation, thus the standard normal distribution has the following probability density function: . $$ P(X) = frac{1}{ sqrt{2 pi}} exp^{ frac{-X^2}{2}} $$ . A general Normal distribution can be transformed into a standard normal distribution by . subtracting the mean value, and | dividing by the standard deviation. | This process is sometimes encoded by the standard score or z-score, . $$ z = frac{x - mu}{ sigma} $$ . which has the benefit of (1) allowing different Normal distributions to be easily compared since they are all on the same scale, and (2) simplifying the interpretation of the result since the z-score implicitly is a measure of the number of standard deviations a value is from the mean of the distribution. . In Python, we create a Normal distribution by using the norm object in the SciPy module, which takes a mean value and a standard deviation. For example, to create a frozen Normal distribution with $ mu = 2.5$ and $ sigma = 1.2$: . from scipy.stats import norm ncd = norm(2.5, 1.2) . In the following code cell, we create and contrast three different Normal distributions. In this case, we do not use frozen distributions since we create and use the different distributions in one line of code. . . from scipy.stats import norm # Number of points to sample for each distribution num_pts = 100 # Gaussian distribution parameter definition mu = [1., 2., 3.] sig = [0.25, 0.5, 1.0] # Local label definition labels = [r&#39;$ mu = 1.0$, $ sigma = 0.25$&#39;, r&#39;$ mu = 2.0$, $ sigma = 0.5$&#39;, r&#39;$ mu = 3.0$, $ sigma = 1.0$&#39;] # Sampling points x = np.linspace(-0.5, 6.5, 250) fig, ax = plt.subplots(figsize=(10, 6)) # Create and Plot Normal PDFs for idx in range(len(mu)): ax.plot(x, norm.pdf(x, mu[idx], sig[idx]), color=g_colors[2 + idx], lw=3, label = labels[idx]) # Decorate plot ax.set_xlabel(r&quot;$X$&quot;, fontsize=18) ax.set_ylabel(r&quot;$P(X| mu, sigma)$&quot;, fontsize=18) ax.set_title(&quot;Gaussian PMF&quot;, fontsize=20) ax.legend(fontsize=16) sns.despine(trim=True, offset = 5) . . Other Continuous Distributions . While the Normal distribution is the most popular continuous distribution, there are many other continuous distributions that arise frequently in data analytics. Some of these have long histories in classical statistical analysis, such as the t-distribution and the F-distribution, which are typically used with small samples. Others arise from the analysis of diverse populations, such as the Exponential or Pareto distributions. In the next section, we explore eight specific distributions in more detail. The SciPy reference documentation provides a complete list of the distributions supported by the SciPy module. . Student&#39;s t . The [t]-distribution is used in many statistical tests, perhaps most prominently in assessing the statistical significance of the difference between two sample means. This distribution, which is similar to the Normal distribution but with heavier tails, was popularized by testing the quality of ingredients for making Guinness beer. The formal probability density function is rather complex, but in Python we can create and use a t-distribution by using the t object in the SciPy module. When creating this object, we specify the degrees of freedom via the df parameter. . from scipy.stats import t td = t(df=3) . In SciPy, this distribution is only defined for $ df &gt; 0$. To shift the distribution, the loc and scale parameters can be specified. . $ chi^2$ . A $ chi^2$ distribution is a widely used distribution in inferential statistics and when determining the quality of a model fit. Formally, the sum of $k$ independent, standard normal variables ($Z_i$) is distributed according to the $ chi^2$ distribution with $k$ degrees of freedom. The probability density function for a $ chi^2$ distribution is given by . $$ P(x) = frac{x^{(k/2 - 1)}e^{-x/2}}{2^{k/2} Gamma left( frac{k}{2} right)} $$for $x &gt; 0$, and $P(x) = 0$ otherwise. $ Gamma left( frac{k}{2} right)$ is the Gamma function. . In Python, we generate and use a $ chi^2$ distribution by using the chi2 object in the SciPy module. The $k$ parameter is specified by using the df, or degrees of freedom, parameter when creating the object. . from scipy.stats import chi2 cd = chi2(df=3) . F . The F-distribution arises in inferential statistics, particularly in the analysis of variance (or ANOVA). The F-distribution is generated by the ratio of two random variables that are described by $ chi^2$ distributions with degrees of freedom $d_1$ and $d_2$, respectively. Formally, the probability density function is rather complex, but in Python we can create and use an F-distribution by using the f object from the SciPy module. This object takes the two degrees of freedom as input parameters, dfn and dfd, for the degree of freedom in the numerator or denominator, respectively. . from scipy.stats import f fd = f(dfn=2, dfd=3) . In SciPy, this distribution is only defined for $ x &gt; 0$. To shift the distribution, the loc and scale parameters can be specified. . LogNormal Distribution . A LogNormal distribution is used to model random variables whose logarithm is distributed normally. This distribution can be used to model the size of living tissues, the length of comments posted on Internet forums, extreme values of rainfall, repair times for many systems, the allocation of wealth for approximately the lower 99% of the population, and the LogNormal distribution underlies the financial Black-Scholes model. . Formally, the probability density function for a LogNormal distribution is given by $$ P(X) = frac{1}{X sqrt{2 pi sigma^2}} exp left[- frac{1}{2} left( frac{ log(X/ mu)}{ sigma} right)^2 right] $$ . In Python, we use the lognorm object within the SciPy module to generate and use LogNormal distributions. To specify the mean ($ mu$) and standard deviation ($ sigma$) of this distribution, you must pass the standard deviation in using the s parameter and the exponential of the mean in via the scale parameter. . from scipy.stats import lognorm lnd = lognorm(scale = np.exp(1.25), s = 0.75) . In SciPy, this distribution is only defined for $ x &gt; 0$ and $s &gt; 0$. To shift the distribution, the loc and scale parameters can be specified. . . Note: We use a special trick in the following code cells called tuple unpacking. Simply put, we create arrays of parameters for the different distributions, and pass the entire tuple into the function as shown in the first code block below. The function will unpack the elements in the tuple and in order assign them to required (or optional) parameters. The only reason we do this is to shorten the code, if it is easier for you to follow or understand, simply unpack these operations as demonstrated in the second code block. . ln_params = [(0.5, 0.0, np.exp(0.75)), (.75, 0.0, np.exp(0.5)), (1.0, 0.0, np.exp(0.25))] flnl = [lognorm(*params) for params in ln_params] . flnl = [] flnl.append(lognorm(0.5, 0.0, np.exp(0.75))) flnl.append(lognorm(.75, 0.0, np.exp(0.5))) flnl.append(lognorm(1.0, 0.0, np.exp(0.25))) . . from scipy.stats import t, f, chi2, lognorm def make_plots(axs, x, fd_list, lbl, ylbl, title): for idx in range(len(fd_list)): axs.plot(x, fd_list[idx].pdf(x), color=g_colors[2 + idx], label=lbl[idx]) # Decorate plot axs.set_xticks(np.arange(np.min(x), np.max(x))) axs.set_xlabel(r&quot;$X$&quot;, fontsize=18) axs.set_ylabel(ylbl, fontsize=18) axs.set_title(title, fontsize=20) axs.legend(fontsize=16) sns.despine(offset = 5, trim=True) # Now we create our figure and axes for the plot we will make. fig, axs = plt.subplots(figsize=(10, 20), nrows=4, ncols=1) fig.subplots_adjust(hspace=.75) # Compute and plot Student&#39;s t-Distribution t_params = [(2, 0.5, 0.5), (3, 1.5, 0.75), (4, 2.5, 1.0)] ftl = [t(*params) for params in t_params] tlbl = [r&#39;$df = {0}$, $ mu = {1}$, $ sigma = {2}$&#39;.format(*p) for p in t_params] make_plots(axs[0], np.linspace(-3, 8, 250), ftl, tlbl, r&#39;$P(X | df, mu, sigma)$&#39;, &quot;Student&#39;s t-Distribution&quot;) # Compute and plot f-Distribution f_params = [(2, 2), (5, 1), (10, 12)] ffl = [f(*params) for params in f_params] flbl = [r&#39;$df1 =$ {0}, $df2 =$ {1}&#39;.format(*p) for p in f_params] make_plots(axs[1], np.linspace(0, 8, 250), ffl, flbl, r&#39;$P(X | df1, df2)$&#39;, &quot;f-Distribution&quot;) # Compute and plot ChiSqaured Distribution c_params = [2, 3, 5] fcl = [chi2(params) for params in c_params] clbl = [r&#39;$k = {0}$&#39;.format(p) for p in c_params] make_plots(axs[2], np.linspace(0, 13, 250), fcl, clbl, r&#39;$P(X | k)$&#39;, r&quot;$ chi^2$-Distribution&quot;) # Compute and plot LogNormal Distribution ln_params = [(0.5, 0.0, np.exp(0.75)), (.75, 0.0, np.exp(0.5)), (1.0, 0.0, np.exp(0.25))] flnl = [lognorm(*params) for params in ln_params] lnlbl = [f&#39;$ mu = {np.log(p[2]):4.2f}$, $ sigma = {p[0]:4.2f}$&#39; for p in ln_params] make_plots(axs[3], np.linspace(-0.5, 6.5, 250), flnl, lnlbl, r&#39;$P(X | mu, sigma)$&#39;, &quot;LogNormal Distribution&quot;) . . Power-Law . The Power-law distribution is seen frequently in situations where there are many small items and fewer large items, or where physical conditions are determined by polynomial relations (like mass and height). A classic example is incomes for a large group of individuals, as there are many people with small incomes and very few with large incomes. Other examples include populations in cities, masses of different species, or frequencies of word counts. In addition, certain critical events, such as landslides, can be modeled by using a Power-law distribution. Finally, other distributions (including the Pareto or Zipf distributions) can be considered special cases of the general Power-law distribution. . As a general form, we can consider a minimum cutoff ($x_{min}$), and a scaling exponent ($ alpha$). . $$ P(x) = frac{ alpha - 1}{x_{min}} left( frac{x}{x_{min}} right)^{- alpha} $$ . To use a Power-law distribution in Python, we use the powerlaw object within the SciPy module. The SciPy powerlaw distribution takes a single, real valued argument, a, which must be greater than zero. . from scipy.stats import powerlaw pld = powerlaw(a=1.25) . By default, in SciPy this distribution is defined over the range $0 leq x leq 1$. Additional parameters (loc and scale) can be specified to extend the range over which the distribution is defined. . Exponential . An Exponential distribution describes the time between events that occur continuously and at a constant rate (i.e., a Poisson process). The Exponential distribution can be used to model decay of radioactive particles, the maximum rainfall in a set period of time, the time between telephone calls, and the time until a default in certain credit risk modeling. . Note, even if a process generates events at a constant rate, the Exponential distribution can be applied over a time that the rate is nearly constant, for example, the time during which calls are generally made. One interesting result of the Exponential distribution is that the time to the next event (such as a phone call) is independent of the time since the last call. This property is formally known as being memoryless. . Given a rate parameter: $ lambda$, we have the following probability density function for the Exponential distribution: . $$ P(x) = lambda e^{- lambda x} $$if $x geq 0$ and zero otherwise. For this PDF, an Exponential distribution has mean value $ mu = lambda^{-1}$ and variance $ sigma^2 = lambda^{-2}$. . In Python, we use the expon object within the SciPy module to generate and use Exponential distributions. To set the rate parameter for the expon object, you set the scale parameter to the inverse of the rate parameter: . from scipy.stats import expon ed = expon(scale=1.25) . In SciPy, this distribution is defined only for $x geq 0$. . Pareto . A Pareto distribution is a special case of the Power-law distribution, originally developed to describe the allocation of wealth, where 20% of the population was inferred to control 80% of the wealth. Thus, the Pareto distribution informally refers to distributions that follow the 80-20 rule (or a similar pre-defined allocation) with scale parameter $ alpha$. This distribution can be used to model the sizes of human settlements, values in oil fields, sizes of sand particles, or the standardized price returns on individual stocks. . Formally, for any $x &gt; x_m$, the probability density function for a Pareto distribution is given by $$ P(X &gt; x) = frac{ alpha x_m^{ alpha}}{x^{ alpha + 1}} $$ . with mean value . $$ mu = frac{ alpha x_m}{ alpha - 1}$$ . and variance . $$ sigma^2 = frac{ alpha x_m^2}{( alpha - 1)^2( alpha - 2)}$$ . In Python, we use the pareto object within the SciPy module to generate and use Pareto distributions. To specify a scale parameter when creating this distribution, set the b parameter to the desired scale value. . from scipy.stats import pareto bd = pareto(b=1.25) . In SciPy, this distribution is only defined for $ x geq 1$. To shift the distribution, the loc and scale parameters can be specified. . Cauchy . The Cauchy distribution appears when solving different physical conditions or mathematical expressions, and also results from the ratio of two independent normally distributed random variables. The Cauchy distribution is the canonical pathological distribution in that both the mean and variance for a Cauchy distribution are undefined. Formally, the probability density function for a Cauchy distribution is given by . $$ P(x) = frac{1}{ pi gamma} left[ frac{ gamma^2}{(x - x_0)^2 + gamma^2} right] $$In Python, we generate and use a Cauchy distribution by using the cauchy object in the SciPy module. By default, $ gamma$ and $x_0$ are assumed to be one. . from scipy.stats import cauchy cd = cauchy(scale=1.25) . The distribution can be shifted by specifying the loc and scale parameters. . . from scipy.stats import powerlaw, expon, pareto, cauchy # Now we create our figure and axes for the plot we will make. fig, axs = plt.subplots(figsize=(10, 20), nrows=4, ncols=1) fig.subplots_adjust(hspace=.75) # Compute and plot Scipy PowerLaw Distribution pl_params = [0.001, 0.01, 2] fpll = [powerlaw(p, 0.5, 5.5) for p in pl_params] pllbl = [r&#39;$ alpha = {0}$&#39;.format(p) for p in pl_params] # Scale y-axis to be logarithmic for clarity axs[0].set_yscale(&#39;log&#39;) make_plots(axs[0], np.linspace(-0.5, 8.5, 250), fpll, pllbl, r&#39;$P(X | alpha)$&#39;, &quot;PowerLaw Distribution&quot;) # Compute and plot Exponential Distribution e_params = [1, 2, 3] fel = [expon(scale=p) for p in e_params] elbl = [r&#39;$ lambda =$ {0}&#39;.format(p) for p in e_params] make_plots(axs[1], np.linspace(-0.5, 7.5, 250), fel, elbl, r&#39;$P(X | lambda)$&#39;, &quot;Exponential Distribution&quot;) # Compute and plot Pareto Distribution p_params = [3, 2, 1] fpl = [pareto(p) for p in p_params] plbl = [r&#39;$ alpha = {0}$&#39;.format(p) for p in p_params] make_plots(axs[2], np.linspace(0.5, 5.5, 250), fpl, plbl, r&#39;$P(X | alpha)$&#39;, r&quot;Pareto Distribution&quot;) # Compute and plot Cauchy Distribution ca_params = [(0.5, 0.5), (1.5, 0.75), (2.5, 1.0)] fcal = [cauchy(*p) for p in ca_params] calbl = [r&#39;$X_0 = {0}$, $ gamma = {1}$&#39;.format(*p) for p in ca_params] make_plots(axs[3], np.linspace(-3.5, 6.5, 250), fcal, calbl, r&#39;$P(X | X_0, gamma)$&#39;, &quot;Cauchy Distribution&quot;) . . Random Sampling . One of the most powerful uses for a theoretical distribution is the creation of a sample of random values from the specified distribution. All distributions in the SciPy module have an rvs method that will sample a specified number of random variables from the parent distribution. The number sampled is specified by the size parameter. This process is demonstrated in the following two code blocks. . First, we create a Normal distribution, generate a sample of ten thousand random values, and finally compare a histogram of the random values to the theoretical distribution. Note how the agreement between the two is extremely strong. Second, we create an Exponential distribution and generate a new sample of ten thousand random values. This time we compare the histogram of the sampled random values to the theoretical distribution on a regular, linear plot, and also a plot where the y-axis has a logarithmic scaling. The second version clearly shows the strong agreement between the Exponential distribution and sampled random values. . . fig, axs = plt.subplots(figsize=(12, 8)) nd = norm(1.5, .75) x = np.linspace(-2.5, 5.5, 1000) axs.plot(x, nd.pdf(x), lw=2, alpha=0.75, color=g_colors[0], label=&#39;Model PDF&#39;) data = nd.rvs(10000) axs.hist(data, bins=20, normed=True, histtype=&#39;bar&#39;, lw=4, alpha=0.25, color=g_colors[1], label=&#39;Binned Random Data&#39;) axs.set_xlabel(&#39;X&#39;, fontsize=18) axs.set_xticks(np.arange(np.min(x), np.max(x) + 1)) axs.set_ylabel(&#39;P(X)&#39;, fontsize=18) axs.set_title(&quot;Random Draw from Normal PDF&quot;, fontsize=20) axs.legend(fontsize=16) sns.despine(offset = 5, trim=True) . # Two figures, one over the other fig, axs = plt.subplots(figsize=(8, 16), nrows=2, ncols=1) fig.subplots_adjust(hspace=.5) # Helper function that draws random data from exponential # and also computes the PDF, and plots the results def plot_exp(axs, lam = 1.0): ed = expon(lam) x = np.linspace(0, 10.5, 1000) axs.plot(x, ed.pdf(x), lw=2, alpha=0.75, color=g_colors[0], label=&#39;Model PDF&#39;) data = ed.rvs(10000) axs.hist(data, normed=True, histtype=&#39;bar&#39;, lw=4, alpha=0.25, color=g_colors[1], label=&#39;Binned Random Data&#39;) axs.set_xlabel(&#39;X&#39;, fontsize=18) axs.set_xticks(np.arange(np.min(x), np.max(x))) axs.set_ylabel(&#39;P(X)&#39;, fontsize=18) axs.set_title(&quot;Random Draw from Exponential PDF&quot;, fontsize=20) axs.legend(fontsize=16) sns.despine(offset = 5, trim=True) # We plot the first one normally plot_exp(axs[0]) # We plot the second one with logarithmic scaling on the y-axis axs[1].set_yscale(&#39;log&#39;) plot_exp(axs[1]) . . Alternative Distribution Forms . To this point, we have focused on the probability density functions for specific theoretical distributions. There are, however, other forms for these distribution functions that can be important in solving specific problems as they may simplify the resulting analysis. In this section, we will discuss four specific forms: . Cumulative distribution function (CDF) | Percent point function (PPF). | Survival function (SF). | Inverse survival function. | The cumulative distribution function specifies the probability that a real-valued random variable will have a value less than or equal to a specific value, $x$. Thus, for a continuous distribution, the CDF is the area under the PDF from minus infinity to $x$. The CDF is used frequently in hypothesis testing and in Monte Carlo integration, and is given by the following expression, where $f_X(x)$ is a function evaluated at $x$ of the real valued, random variable $X$. . $$ F_X(x) = P(X leq x) = int_{- infty}^x f(u) du$$ . One important result that follows from this definition is that the cumulative probability between two limits $(a, b]$ is given by the difference in the CDF evaluated at these two points: . $$ P(a &lt; X leq b) = F_X(b) - F_X(a)$$ . The percent point function of a real-valued, random variable $x$, alternatively known as the quantile function or inverse cumulative distribution function, specifies the value of random variable at which the probability of getting $x$ is less than or equal to the given probability. Alternatively, you can view this function as stating the value of $x$ for which the CDF has a specific value $p$. Thus in a simple one-dimensional example, given a probability, $p$, the PPF indicates the value $x$ at which the PDF has probability equal to or less than the target probability $p$. The PPF can be useful in determining quantiles, quartiles, and the median, and also in random number generation. . The survival function, $S(t)$, gives the probability that a real-valued random variable will exceed a specific value $x$. Often the variable is time, and the survival function indicates the probability an item, such as a mechanical device, a patient, or other item will last (or survive) beyond a given time. This function is given by $1 - CDF$. . $$ S(t) = P( T &gt; t) = int_t^{ infty} f(u) du = 1 - F(t)$$ . The inverse of this function, called the inverse survival function, gives the real-valued random variable at which the probability of surviving is less than or equal to the target probability $p$. . The following code cell displays these four functions, along with the probability density function (PDF) for a Normal distribution. In addition, the standard quartiles are displayed to provide an additional reference point in interpreting these other distribution forms. . . # Now Seaborn specific modifications sns.set_context(&quot;poster&quot;, font_scale=1.0) sns.set_style(&quot;white&quot;) # Now we create our figure and axes for the plot we will make. fig = plt.figure(figsize=(12, 18)) fig.subplots_adjust(wspace=0.5, hspace=0.5) # Define plot layout #fig, ax = plt.subplots() ax1 = plt.subplot2grid((3,2), (0,0), colspan=2) ax2 = plt.subplot2grid((3,2), (1,0)) ax3 = plt.subplot2grid((3,2), (1, 1)) ax4 = plt.subplot2grid((3,2), (2, 0)) ax5 = plt.subplot2grid((3,2), (2, 1)) mean = 1.0 sigma = 0.75 # Freeze normal distribution dist = norm(loc=mean, scale=sigma) # Define variate and probability arrays x = np.linspace(mean - 3. * sigma, mean + 3. * sigma, 1000) p = np.linspace(0, 1.0, 100) # Plot functional distributions ax1.plot(x, dist.pdf(x), lw=4, alpha=0.75, color=g_colors[2]) ax2.plot(x, dist.cdf(x), lw=2, alpha=0.75, color=g_colors[3]) ax3.plot(p, dist.ppf(p), lw=2, alpha=0.75, color=g_colors[4]) ax4.plot(x, dist.sf(x), lw=2, alpha=0.75, color=g_colors[3]) ax5.plot(p, dist.isf(p), lw=2, alpha=0.75, color=g_colors[4]) # Show &#39;special&#39; probabilities # First horizontal lines ax2.axhline(0.25, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax2.axhline(0.5, 0.25, 0.75, color=g_colors[0], lw=3, ls = &#39;-.&#39;) ax2.axhline(0.75, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax4.axhline(0.25, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax4.axhline(0.5, 0.25, 0.75, color=g_colors[0], lw=3, ls = &#39;-.&#39;) ax4.axhline(0.75, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) # Second vertical lines ax3.axvline(0.25, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax3.axvline(0.5, 0.25, 0.75, color=g_colors[0], lw=3, ls = &#39;-.&#39;) ax3.axvline(0.75, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax5.axvline(0.25, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) ax5.axvline(0.5, 0.25, 0.75, color=g_colors[0], lw=3, ls = &#39;-.&#39;) ax5.axvline(0.75, 0.25, 0.75, color=g_colors[0], lw=1, ls = &#39;-.&#39;) # Set the title ax1.set_title(&quot;Gaussian PDF&quot;, fontsize=20) ax1.set_xlabel(&#39;X&#39;, fontsize=18) ax1.set_xticks(np.arange(np.min(x), np.max(x) + 1)) ax1.set_ylabel(&#39;P(X)&#39;, fontsize=18) ax2.set_title(&quot;Cumulative Density Function&quot;) ax2.set_xlabel(&#39;X&#39;, fontsize=18) ax2.set_xticks(np.arange(np.min(x), np.max(x) + 1)) ax2.set_ylabel(&#39;Cumulative Probability&#39;, fontsize=18) ax3.set_title(&quot;Percent Point Function&quot;) ax3.set_xlabel(&#39;Cumulative Probability&#39;, fontsize=18) ax3.set_yticks(np.arange(np.min(x), np.max(x) + 1)) ax3.set_ylabel(&#39;X&#39;, fontsize=18) ax4.set_title(&quot;Survival Function&quot;) ax4.set_xlabel(&#39;X&#39;, fontsize=18) ax4.set_xticks(np.arange(np.min(x), np.max(x) + 1)) ax4.set_ylabel(&#39;Cumulative Probability&#39;, fontsize=18) ax5.set_title(&quot;Inverse Survival Function&quot;) ax5.set_xlabel(&#39;Cumulative Probability&#39;, fontsize=18) ax5.set_yticks(np.arange(np.min(x), np.max(x) + 1)) ax5.set_ylabel(&#39;X&#39;, fontsize=18) # Now Seaborn specific modifications sns.despine(offset=10, trim=True) . . Central Limit Theorem . The importance of the Normal (or Gaussian) distribution is hard to overstate. Not only do many data set in Nature display normality, but the Normal distribution can often be applied to the analysis of data that have been sampled repeatedly from a different distribution that is non-normal. . Assume we have been given a set of $n$ data points. Formally, as long as all of these data are . mutually independent, | drawn from a common distribution, and | the mean ($ mu$) and standard deviation ($ sigma$) of the data points are finite, | then the Central Limit Theorem, or (CLT) states that the sample average of these data points is Normally distributed with mean ($ mu$) and variance ($ sigma^2/n$): . $$P left(x = frac{1}{n} sum_i^n x_i right) rightarrow sqrt{ frac{n}{2 pi sigma^2}} exp{ left(- frac{n (x - mu)^2}{2 sigma^2} right)} $$The first two items in the previous list are often shortened to iid, which is an abbreviation for independent and identical distributed. This theorem essentially implies that sums of random quantities will be approximately distributed as a normal distribution, with the accuracy of the approximation determined by the number of points ($n$). In addition, the CLT often leads analysts to average out the noise and to assume a Normal model. While these conditions are often met, and thus the last two steps are valid, one should always exert caution and be sure that all conditions are met before jumping to strong assumptions. . The CLT is demonstrated in the following code cell, where we repeatedly simulate the process of flipping ten coins. By progressively increasing the number of samples that we accumulate, we can see that as $n$ increases our data set becomes increasingly well modeled by a Normal distribution. . . from scipy.stats import binom # Parameters for our distribution ph = 0.5 # Probability of heads trails = 10 # Number of trails (ten coins) # Convenience function to flip a coin x *times*, # and plot the results. def plot_coinspmf(p, trails, times, axes): # Build an array of coin flips flips = binom.rvs(trails, p, size=times) # Now histogram them, two bins, heads or tails results, _ = np.histogram(flips, bins=int(trails + 1)) # For simplicity turn into a Pandas Series. coins = pd.Series(results) / np.sum(results) x = np.arange(11) # Now plot the coin flip results ax = coins.plot.bar(ax=axes, color=g_colors[1], alpha = 0.75, fontsize=14, rot=0, xticks=x, yticks=np.linspace(0, 0.5, 11)) if times // trails &gt; 1: title = f&#39;{trails} coins: {times // trails} flips each&#39; else: title = f&#39;{trails} coins: {times // trails} flip each&#39; ax.set_title(title, fontsize=20) ax.set_ylabel(&quot;Probability&quot;, fontsize=18) # Now make a Gaussian and overplot. mu = trails * p sigma = np.sqrt(trails * p * (1 - p)) norm = 1./(sigma * np.sqrt(2 * np.pi)) y = norm * np.exp(-1. * (x - mu)**2/(2. * sigma**2)) ax.plot(x, y, linewidth=3, linestyle =&#39;-.&#39;, color=g_colors[0], alpha = 0.75) sns.despine(trim=&#39;True&#39;, left = False, right=True) # Create figure fig, axs = plt.subplots(figsize=(12, 12), nrows=2, ncols=2) adj = plt.subplots_adjust(hspace=0.25, wspace =0.25) # Perform coin flip simulations plot_coinspmf(ph, trails, 10, axs[0][0]) plot_coinspmf(ph, trails, 100, axs[0][1]) plot_coinspmf(ph, trails, 1000, axs[1][0]) plot_coinspmf(ph, trails, 10000, axs[1][1]) . . QQ Plot . Given a data set, one approach for gaining insight is to determine whether a specific distribution is a good match for one or more columns or features. For the Normal distribution, the standard technique for testing this assumption is the quantile-quantile plot, or QQ-plot. Effectively this plot sorts the data and compares the empirical distribution of the data to the theoretical distribution. If the empirical data lie on (or close to) a diagonal (or 45 degree ) line in a QQ-plot, the data can be effectively modeled by the target theoretical distribution (e.g., a Normal distribution). . To make this plot, we use the ProbPlot object from the Statsmodel module. As shown below, we can create a ProbPlot object by passing in normally distributed data, along with the data distribution to which we wish to compare the data (in this case a standard Normal distribution). We next create a qqplot objects to make the quantile-quantile plot, along with a qqline to display the diagonal line that aids in the interpretation of the match between our data and the selected distribution. . This technique is demonstrated in the next two code cells, where we first generate normally distributed random data and compare to a Normal distribution. The agreement is quite good (as expected), indicating that our data is modeled nicely by using a Normal distribution. In the second example, we generate log-normally distributed random data, which is not (again, as expected) well matched by a Normal distribution. Note, a QQ-Plot is not restricted to using a Normal distribution, but given the prevalence of Normal distributions in Nature, a QQ-Plot is often used as a test for Normality. . . # We use functions from the Statsmodel module import statsmodels.api as sm # Create figure fig, axs = plt.subplots(figsize=(8, 8)) # Create normal distributed data nd = norm(0, 1) # Setup Probability comparison plot pp = sm.ProbPlot(nd.rvs(100), dist=norm) # Create and show QQ-Plot qq = pp.qqplot(ax=axs, marker=&#39;D&#39;, markeredgecolor=g_colors[4], alpha=0.25) axs = qq.axes[0] # Show QQ Line (45 angle) sm.qqline(axs, line=&#39;45&#39;, fmt=&#39;k-.&#39;) # Decorate axs.set_title(&#39;QQ Plot: Normal Test&#39;) sns.despine(offset=5, trim=True) . # Create figure fig, axs = plt.subplots(figsize=(8, 8)) # Create lognormal distributed data lnd = lognorm(0.5, 0.5) # Setup Probability comparison plot pp = sm.ProbPlot(lnd.rvs(100), dist=norm) # Create and show QQ-Plot qq = pp.qqplot(ax=axs, marker=&#39;D&#39;, markeredgecolor=g_colors[4], alpha=0.25) axs = qq.axes[0] # Show QQ Line (45 angle) sm.qqline(axs, line=&#39;45&#39;, fmt=&#39;k--&#39;) # Decorate axs.set_title(&#39;QQ Plot: LogNormal Test&#39;) sns.despine(offset=5, trim=True) . . Fitting Distributions . On the other hand, if we have a reason to expect our data can be modeled by a specific theoretical distribution, we can directly fit the data by using the scipy module. Each distribution in the scipy module includes a fit method, which will compute the best fit parameter values for the specific distribution to the provided data. We demonstrate this in the following code cell where we generate normally distributed data, and next use the fit method on the Normal distribution to determine the best fit Normal distribution mean and standard deviation. Finally, we plot the binned normally distributed data along with the original theoretical Normal distribution and best fit Normal distribution. . Since the data generation is a random process, each time the code cell is run the results will be different. Thus, even though the process demonstrating this technique might seem circular, the agreement is usually quite good (or strong), which might be surprising. In practice, one would most likely generate a qq-plot to determine if a Normal distribution would provide a good match for a data set, after which the best fit distribution can be computed from the data. . The importance of this process is hard to overstate. Having a model for a data set can provide both physical insight into what generated the data, as well as the ability to generate new data that should appear indistinguishable from the original data set. This later technique can prove useful in situations where obtaining new data can be either prohibitively expensive or even impossible due to physical constraints. . . # Generate random data from normal distribution mean = -1.5 sigma = 1.25 model = norm(mean, sigma) data = model.rvs(size=1000) print(f&#39;Model Mean = {mean:6.2f}&#39;) print(f&#39;Model Std = {sigma:6.2f} n&#39;) # Fit a Normal distribution to the data fit_params = norm.fit(data) fit_dist = norm(loc=fit_params[0], scale=fit_params[1]) print(f&#39;Fit Mean = {fit_params[0]:6.3f}&#39;) print(f&#39;Fit Std = {fit_params[1]:6.3f} n&#39;) # Define variate x = np.linspace(mean - 3. * sigma, mean + 3. * sigma, 100) # Plot data and fitted model fig, axs = plt.subplots(figsize=(12, 8)) # Plot Fit axs.plot(x, fit_dist.pdf(x), lw=4, alpha=0.75, color=g_colors[0], label=&#39;Fit PDF&#39;) # Plot Model axs.plot(x, model.pdf(x), lw=2, ls=&#39;-.&#39;, alpha=0.75, color=g_colors[4], label=&#39;Model PDF&#39;) # Plot data axs.hist(data, bins=20, normed=True, histtype=&#39;bar&#39;, lw=4, alpha=0.25, color=g_colors[1], label=&#39;Binned Random Data&#39;) axs.set_xlabel(&#39;X&#39;, fontsize=18) axs.set_xticks(np.arange(np.min(x), np.max(x) + 1)) axs.set_ylabel(&#39;P(X)&#39;, fontsize=18) axs.set_title(&quot;Distribution fitting example&quot;, fontsize=20) axs.legend(fontsize=16) sns.despine(offset = 5, trim=True) . Model Mean = -1.50 Model Std = 1.25 Fit Mean = -1.465 Fit Std = 1.223 . . Student Exercise . In the empty Code cell below, write and execute code to gnerate data from a different distribution, such as a lognormal, and then fit the data with the new distribution. . . . Ancillary Information . The following links are to additional documentation that you might find helpful in learning this material. Reading these web-accessible documents is completely optional. . Statistics tutorial from Scipy | A somewhat dated blog article on simple statistical analysis in Python | A blog article on using distributions from Python | . &copy; 2017: Robert J. Brunner at the University of Illinois. . This notebook is released under the Creative Commons license CC BY-NC-SA 4.0. Any reproduction, adaptation, distribution, dissemination or making available of this notebook for commercial use is not allowed unless authorized in writing by the copyright holder. .",
            "url": "https://tejakummarikuntla.github.io/notes/math/2020/01/25/Introduction-to-distributions.html",
            "relUrl": "/math/2020/01/25/Introduction-to-distributions.html",
            "date": " • Jan 25, 2020"
        }
        
    
  
    
        ,"post6": {
            "title": "Descriptive Statistics",
            "content": "Random Variables . A Random variable is a variable that takes on numerical values as a result of random expriments of mesuarement, associates a numerical value with each possible outcom .. R.V&#39;s must have numberical values. . Discrete Random Variable: has a finite number of values or an infinate sequence of values(0,2,3,...) AND the differnces between coutcomes are menaningful Die throw can have 1,2,3,4,6 and each is a meaningfully different. | . | Continuous Random Variable: ahs a nearly infinite numbers of outcomes that cannot be easiily counted AND the differences between teh outcomes are NOT meaningful With average income, the difference between $40,00 and $40,001 is not meaningful | . | . Discrete Probability Distribution . The probability distribution for a random variable X describes how assigned to each outcome for the random variable. Let 0=Heads and 1=Tails for a coin flip. so ouir discrete random variable x is described as: x = 0,1 . The Probability for each outcome is described by a discrete porbability funcion denoted by P(x) $$ sum P(x) = 1$$ sum of all RV Probabilities P(x) must equal 1 | . Uniform Probability Distribution . import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns from scipy.stats import uniform . #Generating Uniform random numbers from scipy.stats import uniform # We can generate random variables/numbers # from uniform distribution from uniform distribution’s rvs function like uniform.rvs. # Here we are generating 1000 Random Variables (rvs) between 0 and 10. data_uniform = uniform.rvs(size=1000, loc=0, scale=10) . len(data_uniform) . 1000 . data_uniform[:20] . array([7.33092523, 6.91163852, 6.81076766, 6.10710074, 6.3161025 , 8.99572593, 5.78687982, 2.91428895, 5.54023576, 7.51997417, 7.85882577, 1.15105694, 6.13031743, 1.06657784, 1.61417764, 1.58547971, 6.65943099, 3.0200822 , 9.14553805, 7.58889625]) . ax = sns.distplot(data_uniform, bins=100, kde=True, color=&#39;skyblue&#39;, hist_kws={&quot;linewidth&quot;: 15,&#39;alpha&#39;:1}) ax.set(xlabel=&#39;Uniform &#39;, ylabel=&#39;Frequency&#39;) . [Text(0, 0.5, &#39;Frequency&#39;), Text(0.5, 0, &#39;Uniform &#39;)] . Probability Distributions . Standard Normal Distribuion or Standardization: . import numpy as np import pandas as pd from scipy import stats import matplotlib.pyplot as plt import seaborn as sns from sklearn.preprocessing import StandardScaler %matplotlib inline . dset = pd.DataFrame([[1],[2],[2],[3],[3],[3],[4],[4],[5]]) . print(&quot;Mean: &quot;,dset.mean()[0]) print(&quot;Standard deviation&quot;, dset.std()[0]) . Mean: 3.0 Standard deviation 1.224744871391589 . density_plot(np.array(dset)) . # Applying Standardization. std_sc = StandardScaler() zscore_data = std_sc.fit_transform(dset) . /home/tejakummarikuntla/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py:645: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler. return self.partial_fit(X, y) /home/tejakummarikuntla/anaconda3/lib/python3.7/site-packages/sklearn/base.py:464: DataConversionWarning: Data with input dtype int64 were all converted to float64 by StandardScaler. return self.fit(X, **fit_params).transform(X) . print(&quot;Mean: &quot;,zscore_data.mean()) print(&quot;Standard deviation&quot;, zscore_data.std()) . Mean: 0.0 Standard deviation 1.0 . def density_plot(ds): f, (ax_box, ax_hist) = plt.subplots(2, sharex=True, gridspec_kw= {&quot;height_ratios&quot;: (0.2, 1)}) mean = np.mean(ds) median = np.median(ds) mode = stats.mode(ds)[0] sns.boxplot(ds, ax=ax_box) ax_box.axvline(mean, color=&#39;r&#39;, linestyle=&#39;--&#39;) ax_box.axvline(median, color=&#39;g&#39;, linestyle=&#39;-&#39;) ax_box.axvline(mode, color=&#39;b&#39;, linestyle=&#39;-&#39;) sns.distplot(ds, hist=True, kde=True, ax=ax_hist, color = &#39;darkblue&#39;, hist_kws={&#39;edgecolor&#39;:&#39;black&#39;}, kde_kws={&#39;linewidth&#39;: 4}) ax_hist.axvline(mean, color=&#39;r&#39;, linestyle=&#39;--&#39;) ax_hist.axvline(median, color=&#39;g&#39;, linestyle=&#39;-&#39;) ax_hist.axvline(mode, color=&#39;b&#39;, linestyle=&#39;-&#39;) plt.legend({&#39;Mean&#39;:mean,&#39;Median&#39;:median,&#39;Mode&#39;:mode}) ax_box.set(xlabel=&#39;&#39;) plt.show() . density_plot(zscore_data) . type(zscore_data) . numpy.ndarray . np.median(zscore_data) . 0.0 . stats.mode(dset)[0] . array([[3]]) . l=[1,2,4] . dic = {1:&quot;a&quot;, 2:&quot;b&quot;, 3:&quot;4&quot;} . dic.items() . dict_items([(1, &#39;a&#39;), (2, &#39;b&#39;), (3, &#39;4&#39;)]) . for key,val in dic.items(): print(key) . 1 2 3 . lst = np.array([ [3, 2, 4], [6, 7, 8], [1, 4, 1]]) . two = np.array([ [1], [4], [5] ]) . lst*two . array([[ 3, 2, 4], [24, 28, 32], [ 5, 20, 5]]) . lst@two . array([[31], [74], [22]]) .",
            "url": "https://tejakummarikuntla.github.io/notes/math/data%20science/2020/01/24/Descriptive-Statistics-Intro.html",
            "relUrl": "/math/data%20science/2020/01/24/Descriptive-Statistics-Intro.html",
            "date": " • Jan 24, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Contact",
          "content": "You can find me in multiple social media platforms. . GitHub | LinkedIn | Instagram | Email | Twitter | . A few of my Writings at Blog | .",
          "url": "https://tejakummarikuntla.github.io/notes/contact/",
          "relUrl": "/contact/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://tejakummarikuntla.github.io/notes/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}